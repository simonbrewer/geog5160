---
title: "GEOG 5160/6160 Lab 10 Deep learning with Keras"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
```

## Introduction

In this lab, we'll look at the use of the Keras package for deep learning. This document will introduce the building blocks of Keras models and use them to create basic backpropagation type neural networks. There are two additional lab documents that then go on to show how to build recurrent neural networks and convolutional neural networks using Keras. These are optional and are simply available in case you would like to explore this method a little further. Both of the examples are taken from Deep Learning with Python by F. Chollet, and Deep Learning with R by Chollet and Allaire, and I highly recommend either (or both!) of these books. 

The only new data file you will need for the lab is *boston6k.csv*, which should be available from Canvas with this document. Download this to your `datafiles` folder (extract any zip files). Make a new folder for today's class called `lab10`. 

### R users

Start RStudio and set the working directory to this directory (This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder). 

The **mlr3** developers are currently working on an interface to Keras models, which would allow you to use the same approach that we have used in previous labs to build models. Unfortunately, this is still in early development, so for this lab, we'll use the R **keras** package. 

## Installing Keras

To utilize keras and/or tensorflow in R, you must have a working installation of Anaconda on your PC. Anaconda for individual users can be found and downloaded at: https://www.anaconda.com/products/individual

Once Anaconda is installed, keras and the TensorFlow backend can be installed in R from Github as follows. Keras is designed to run on either a CPU or GPU, but this installation guide uses the pathway that recruits CPU support only.

```{r eval = FALSE}
install.packages('keras')
```

Once this step is complete, use the `install_keras()` function to install keras.

```{r eval = FALSE}
library(keras)
install_keras()
```

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notebook for this lab (*GEOG_5160_6160_lab10_Deep_learning_with_keras.ipynb*). Make a new folder for the lab (`lab10`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). 

Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

...Keras installation instructions...

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 

## Keras workflow

Building a deep learning model through Keras requires a series of steps:

1. Create training data as tensors. This should include the input features and the target as separate tensors. For the simple models we are looking at in this lab, this is fairly straightforward, but for the more complex models, careful attention is required to the size and shape of these tensors.
2. Create the network architecture. This consists of the set of layers that link the inputs to the target(s)
3. Define the loss function, the optimizer and the performance metrics to be used to test the progress of the training
4. Train the model using the training data, with part of the training data left out as validation data

### Defining the network architecture

Keras has two methods to create the network. The simplest is to use the `keras_model_sequential()` function. This takes as input the definition of the hidden layers, as well as any parameters that are used to modify these during training. 

For example, to create a simple, two layer model with a single output, one hidden layer with 10 nodes, you would run the following command. Note that the first layer takes the argument `input_shape` which describes the expected shape of the tensor holding the features, and that we specify a ReLU activation function for the output. 

```{r eval=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 10, input_shape = c(20)) %>%
  layer_dense(units = 1, activation = "relu")
```

The other method to define the architecture uses the Keras API. This is a much more flexible approach based on graph theory and can be used to create networks that are much more complex. 

## Example 1: binary classification

The first example we'll look is a binary classification task. This is based on a set of movie reviews taken from the IMDB. There are a total of 50,000 reviews, split 50-50 into a training and testing set. This dataset comes with the Keras package, so we don't need to worry about processing it, but simply load it. This will download the data to your computer, so make sure you are connected to the internet when you run this.

```{r}
library(keras)

imdb <- dataset_imdb(num_words = 10000)
```

The `num_words` argument limits the data to the top 10,000 words, and ignores words that occur less frequently. If you take a quick look at the structure of the object that has been created, you should see that this has already been split into the training and testing set for us. 

```{r}
str(imdb, max.level = 2)
```

Let's take a quick look at the first review:

```{r}
imdb$train$x[[1]]
```

The review has already been encoded to an integer representation. Each number represents the presence of a single word in a review (for example 19 == `film`). 

The authors provide a function that allows you to decode these reviews (i.e. back transform them to the original text). First, create an index containing the words and their integer representations:

```{r}
word_index <- dataset_imdb_word_index()
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
```

Now use this to transform the above review 
```{r}
decoded_review <- sapply(imdb$train$x[[1]], function(index) {                
  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else "?"
})
decoded_review
```

We can also take a look at the outcome label. This is a binary value, where 0 indicates a negative review and 1 indicates a positive review. 

```{r}
imdb$train$y[[1]]
```


We'll now split the data into training and test sets:

```{r}
train_data <- imdb$train$x
train_labels <- imdb$train$y
test_data <- imdb$test$x
test_labels <- imdb$test$y
```

The variables `train_data` and `test_data` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:

Although the data have been processed and organized by review, we can't use these reviews directly in a model, as it will based any weights on the integer assigned to each word, rather than simply the presence or absence of that word. In addition, each review is of a different length, which makes it tricky to assign these values to input nodes in a neural network. We can get a better organization by one-hot encoding the reviews. This will produce, for each review, a vector of length 10,000, where each entry will be a binary (0/1) value indicating whether or not that particular word was present in the review. As we need to do this for both the training and testing sets, we'll create a function to do this for us:

```{r}
## One hot encode data
vectorize_sequences <- function(sequences, dimension = 10000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}
```

This first creates a matrix with 10,000 columns (representing each word) and the same number of rows as the set of reviews we are processing. It then iterates through each review, and if a given word is present in that review, then the value in the matrix is set to 1, otherwise it is left as a 0. 

Now we can apply this to both the training and testing reviews:

```{r}
x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```

If you want to see what the first training review looks like following the one-hot encoding, simply type `x_train[1,]`. These represent the input *tensors*. For this model, these are rank 2 tensors and simply represented in R as 2D arrays. 

We'll also convert the labels from integer values to floating point (numeric):

```{r}
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

Before we start to build the model, we'll create a validation dataset by splitting the training data in two parts. Validation datasets are important in deep learning as the complex networks can quickly and easily overfit to the training data. Validation data are used during the training process. Following each adjustment of the weights, the network predicts the labels for the validation set, and a validation error is calculated. A decreasing validation error suggests that the model is training well to the data, but when it starts to increase, this indicates overfitting: prediction for new data becomes worse. Here, we'll take the first 10,000 reviews and use them for validation:

```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]
y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

### Setting up the network

Now we'll set up the network architecture. The Keras API uses a set of building block functions to set up this network. The `keras_model_sequential()` function defines a blank template for a sequential neural network. The `layer_dense()` function adds a single hidden layer to this. A *dense* layer is a a standard neural network hidden layer that is fully (or densely) connected to all inputs and all outputs. 

In the following code, we define a simple network with two hidden layers. Each hidden layer has 16 nodes (`units`) and uses a ReLU activation function. The second layer is then connected to an output later with a single unit as we are predicting a single outcome (good vs. bad reviews). Note that 

- we use a sigmoid activation function for the output layer, as this forces the output to be in 0-1 range. 
- we define the number of features in the `input_shape` in the first hidden layer. This will define the number of connections needed between the input (10,000 words) and the nodes of the first layer
- we use R's pipe operator (`%>%`) to link the successive layers together

```{r warning=FALSE}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

You may see a warning about the type of hardware detected. You can safely ignore this - the model will still run. 

Let's take a look at the architecture this has created:

```{r}
summary(model)
```

This table shows for each layer:

- Its name (this can be set in the `layer_dense` function)
- The number of outputs (shape)
- The number of parameters to be estimated. For the first this is the number of inputs (10,000) multiplied by the number of nodes (16) + a bias for each node (16). 
- The total number of parameters in the model, which comes out to a little over 160,000. This large number of parameters is why these models require large amounts of data to train (and why they can fit complex problems)

Next, we need to configure the model by specifying the optimizer to be used, the loss function and any error metrics we want to calculate. We'll use standard backpropagation for the optimizer, binary cross entropy for the loss function and we'll measure the model's accuracy. Cross entropy is a measure of how different two distributions are, and this is well suited to a binary classification exercise, where we want to discriminate between 0's and 1's,

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

### Training the model

Now we have everything in place, we can train the model. Here we specify:

- the set of features for training (`partial_x_train`)
- the set of labels for training (`partial_y_train`)
- the number of iterations to train the model for (`epochs`)
- the `batch_size`. 
- the validation features and labels

The batch size is quite an useful parameter to make your network more efficient. This is the number of samples to pass through the network before updating weights. This allows you to trade off the speed of the calculation against the convergence. Using smaller batches uses less memory but may take longer to converge, as the weights are being updated using only a fraction of the data. Larger batches will converge quickly, by may cause memory issues, and may more easily result in overfitting. 

```{r eval=FALSE}
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r eval=TRUE, echo=FALSE}
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val),
  verbose = 0
)
```

If you are running this in RStudio, Keras will plot the evolution of the loss function and the accuracy during the training. You can also visualize this from the `history` object that stores the results of the training:

```{r}
plot(history)
```

The red lines show the loss and accuracy for the training dataset, with a maximum accuracy close to 1. The blue lines show the same for the validation dataset, and show clear signs of overfitting. There is a peak in accuracy (and a minimum in the loss function) at around 5 epochs. We'll refit our model with a smaller number of epochs to the full training set, and then go on to test it.

```{r eval=FALSE}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(x_train, 
              y_train, 
              epochs = 5, 
              batch_size = 512)
```


```{r eval=TRUE, echo=FALSE}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(x_train, 
              y_train, 
              epochs = 5, 
              batch_size = 512, 
              verbose = 0)
```

We can test the model by using the `evaluate` function and the original test (not validation!) dataset:

```{r}
results <- model %>% evaluate(x_test, y_test)
results
```

We get an accuracy of about `r round(results[2]*100)`, which is not bad for a first attempt. The next steps would be to modify the network architecture to see if this could be improved on, e.g. by adding more layers or more nodes in the layers. We'll finish here by making a quick prediction using our trained network, but feel free to try and change your network. 

We'll take the first 10 reviews from the test data and use the `predict()` function to get a predicted probability of this being a good review:
```{r}
model %>% predict(x_test[1:10,])
```

This shows a fair range, with some reviews being predicted as very close to 0 or 1, but others where there is greater uncertainty. One thing to do would be to take the function that back transforms the reviews, and look at an example that has been predicted close to 0 or 1 to see what was actually written.

## Example 2: regression

For the second example, we'll demonstrate a regression example, as well as a $k$-fold cross-validation. We'll use data on house prices and neighborhood characteristics from Boston, in the file *boston6k.csv*. Start by reading this data in, and take a quick look at the values:

```{r}
boston <- read.csv("../datafiles/boston6k.csv")
head(boston)
```
### Data processing

Next, we'll split out the features (we'll use a subset of the available information) and the labels. The 'labels` we are going to model are the corrected median house values in $000s (from the 1970's - that's why it looks so cheap). 

```{r}
library(dplyr)
X <- boston %>%
  select(CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT) %>%
  as.matrix()

y <- boston$CMEDV
```

Next, we'll make up a training and testing dataset, by adding 80% of the samples randomly to training and leaving the rest for testing. 

```{r}
train_set = sample(nrow(X), 0.8 * nrow(X))
test_set = setdiff(seq_len(nrow(X)), train_set)

X_train <- X[train_set, ]
X_test <- X[-train_set, ]

y_train <- y[train_set]
y_test <- y[-train_set]
```

The features are on quite different scales, so we'll standardize them to $z$-scores. To do this, we a) calculate the column-wise mean and standard deviation from the training set, b) use R's built in `scale()` function to transform the values. 

```{r}
mean <- apply(X_train, 2, mean)
std <- apply(X_train, 2, sd)
X_train <- scale(X_train, center = mean, scale = std)
X_test <- scale(X_test, center = mean, scale = std)
```

### Setting up the network

We'll build a model that again has two hidden layers and a single output node. We'll use use the ReLU activation function and slightly more nodes in each hidden layer than in the previous example. One difference here is that we'll create a function (`build_model()`) that contains all the code to set up a Keras neural network. This just contains the two sets of instructions to build a sequential model and compile it together with loss and performance metrics (we're using the mean square error for loss and the mean absolute error for performance:

```{r}
build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = dim(X_train)[[2]]) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)

  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics = c("mae")
  )
}
```

Now we can create a new, untrained version of our network by calling this function:

```{r}
model <- build_model()
summary(model)
```

### Cross-validation

We'll now set up a cross-validation strategy by hand. We'll use a four fold strategy, and we first define a set of indices that show which sample is in which fold. This is done by randomly shuffling the sample row number and then splitting them into 4 groups using the `cut()` function:

```{r}
## k-fold x-validation
k <- 4
indices <- sample(1:nrow(X_train))
folds <- cut(indices, breaks = k, labels = FALSE)
```

Now we'll run the cross-validation. In the following code, we:

- Define the number of epochs to run for (250)
- Create a blank R object to store training information
- Run a loop 4 times. In each iteration:
    - Create a vector of indices for the validation set in this loop
    - Use this vector to extract the features and labels for the validation set (`X_val`, `y_val`)
    - Use these indices to reverse select the training feature and labels (`X_partial_train`, `y_partial_train`)
    - Build a new, untrained network using the function we created earlier
    - Train the model for the set number of epochs, using a batch size of 16
    - Store the validated MAE in `all_mae_histories`
    
Copy this over, check the code and run it. It will take a couple of minutes to iterate across all the training

```{r}
num_epochs <- 250
all_mae_histories <- NULL
for (i in 1:k) {
  cat("processing fold #", i, "\n")
  
  val_indices <- which(folds == i, arr.ind = TRUE)
  X_val <- X_train[val_indices,]
  
  y_val <- y_train[val_indices]
  
  X_partial_train <- X_train[-val_indices,]
  y_partial_train <- y_train[-val_indices]
  
  model <- build_model()
  
  history <- model %>% fit(
    X_partial_train, y_partial_train,
    validation_data = list(X_val, y_val),
    epochs = num_epochs, batch_size = 16, verbose = 0
  )
  mae_history <- history$metrics$val_mae
  all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

We can now average across the 4 folds, and plot the evolution of the MAE

```{r}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_histories)),
  validation_mae = apply(all_mae_histories, 2, mean)
)
library(ggplot2)
ggplot(average_mae_history, aes(x = epoch, y = validation_mae)) + 
  geom_line() 
```

The large drop in MAE over the first few epochs makes it quite difficult to see if and when the model reached an optimum. We'll remake this plot without the first 10 values:

```{r}
average_mae_history %>%
  filter(epoch >= 10) %>%
  ggplot(aes(x = epoch, y = validation_mae)) + 
  geom_line() 
```

The plot suggests that, while there is no visible increase in the validation MAE, there is little improvement beyond about 125 epochs. Let's use this to train a final model on the full training dataset, and evaluate with the test set:

```{r eval=FALSE}
model <- build_model()
model %>% fit(X_train, y_train,
              epochs = 125, batch_size = 16)
result <- model %>% evaluate(X_test, y_test)
result
```

```{r eval=TRUE, echo=FALSE}
model <- build_model()
model %>% fit(X_train, y_train,
              epochs = 100, batch_size = 16, verbose=0)
result <- model %>% evaluate(X_test, y_test)
result
```

As before, there are several modifications you could try to this basic network to attempt to improve on this. 

## Exercise

For the exercise we will once again use the data from the *Sonar.csv* file to model types of object (rocks 'R' or mines 'M') using the values of a set of frequency bands. The goal this time is to construct a neural network using Keras. You will need to do the following:

- Import the data and divide it into a training and test set
- Design a network
- Train the network for a number of epochs with a validation set (use the `binary_crossentropy` loss function)
- Identify the optimum number of epochs
- Run the model with the full training set
- Evaluate the model on the test set and report the *accuracy* of your final model

All the code to do this is in the two examples of this lab. You submitted answers should contain:

- A description of your network, including the summary table (from the `model.summary()` function)
- A figure showing the evolution of the loss function and accuracy
- The accuracy of your final model


## Appendix

### Boston housing dataset: *boston6k.csv*
| Column header | Variable |
| --- | --- |
| ID | Sequential ID |
| TOWN | Town names |
| TOWNNO | Town ID |
| TRACT | Tract ID numbers |
| LON | Longitude in decimal degrees |
| LAT | Latitude in decimal degrees |
| MEDV | Median values of owner-occupied |
|  | housing (USD 1000) |
| CMEDV | Corrected median values of owner-occupied |
|  | housing (USD 1000) |
| CRIM | Per capita crime |
| ZN | Proportion of residential land zoned |
| | for lots over 25000 sq. ft |
| INDUS | Proportions of non-retail business acres per town |
| CHAS | 1 if tract borders Charles River; 0 otherwise |
| NOX | Nitric oxides concentration (parts per 10 million) |
| RM | Average numbers of rooms per dwelling |
| AGE | Proportions of owner-occupied units built prior to 1940 |
| DIS | Weighted distances to five Boston employment centers |
| RAD | Index of accessibility to radial highways per town |
| TAX | Property-tax rate per USD 10,000 per town |
| PTRATIO | Pupil-teacher ratios per town |
| B | Proportion African-American |
| LSTAT | Percent lower status population |
