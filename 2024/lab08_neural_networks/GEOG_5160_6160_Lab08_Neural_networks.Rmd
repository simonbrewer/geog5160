---
title: "GEOG 5160/6160 Lab 08 Neural networks"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
#lgr::suspend_logging()
```

## Introduction

In this lab, we'll go over two neural network examples. These are simple one or two hidden layer networks, and the main purpose of the exercise is to demonstrate how to build the networks and what options are available to modify these. We'll also walk through tuning these simple networks. 

The two data files you will need are *cereal.csv* and *credit_data.csv*, which should be available from Canvas with this document. Download these to your `datafiles` folder (extract any zip files). Make a new folder for today's class called `lab08`. 


### R users

Start RStudio and set the working directory to this directory (This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder). 

You will need the following packages for today's lab, so make sure to install anything that is missing before proceeding.

- **dplyr**
- **mlr3verse**

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notebook for this lab (*GEOG_5160_6160_lab08.ipynb*). Make a new folder for the lab (`lab08`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). 

You will need to make sure the following packages are installed on your computer (in addition to the packages we have used in previous labs). 

- **xarray**: functions for working with regular arrays (`conda install xarray`)
- **xgboost**: extreme gradient boosting (`conda install py-xgboost`)

Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 

## Neural network regression

We'll start with an example of using a neural network for a regression task. The data are taken from a Kaggle competition and are based on a set of 77 breakfast cereals. A description of the data is given in the appendix. The last field in the file (`rating`) is the outcome variable that we will build the network for. Start by loading the packages we'll need for today:

```{r message=FALSE}
library(dplyr)
library(mlr3verse)
```

Next, read in the file:

```{r}
# Read the Data
data <- read.csv("../datafiles/cereals.csv")
head(data)
```

Next, we'll select a subset of the numeric features for modeling, as well as the outcome variable:

```{r}
mydat <- data %>% 
  select(rating, calories, protein, fat, sodium, fiber)
```

To get an overview of the data, we can plot the correlation matrix (the pair-wise correlations between variables). This requires the **GGally** add-on package and can be skipped if you do not have this installed:

```{r}
library(GGally)
ggcorr(mydat, 
       label = TRUE, nbreaks = 6,
       palette = "PuOr")
```

And a histogram of the outcome variable (`rating`):

```{r}
hist(mydat$rating, xlab = 'Rating', main = '')
```

### Neural network in **mlr3**

Regression neural networks are provided in **mlr3** from the **nnet** package (`regr.nnet`). 

As the network uses weighted sums of the input features, it's important that none of these are on very different scales. The easiest way to avoid this is to scale all variables to approximately the same range. The scaling we use here is a min-max transformation or *normalization* (i.e. each variable is converted to a 0-1 range). This transformation is given by the following equation. 

\[
x_i'=\frac{x_i-min(x)}{max(x)-min(x)}
\]

We do this in three steps: 
- First calculate the maximum then minimum value for each variable using the `apply()` function. 
- Then we use these to scale the data - effectively setting the smallest value of each variable to 0, the highest value to 1
- Lastly, we make a new dataset that combines these scaled values with the output (`rating`)

```{r}
feature_names <- c("calories", "protein", "fat", "sodium", "fiber")
maxs = apply(mydat[, feature_names], 2, max)
mins = apply(mydat[, feature_names], 2, min)
scaled_features = as.data.frame(scale(mydat[, feature_names], 
                                      center = mins, scale = maxs - mins))
scaled_data <- data.frame(rating = mydat$rating, scaled_features)
```



Now we'll go through the usual steps of defining our task, performance metric and resampling strategy first. Note that we use the scaled data to create the task, and that we create an outer resampler using k-fold cross-validation, to allow tuning later. 

```{r}
cereal_task <- TaskRegr$new("cereal", backend = scaled_data,
                            target = "rating")
```

```{r}
msr_rmse <- msr("regr.rmse")
rsmp_outer <- rsmp("cv", folds = 5)
```

Before building the neural network, we'll test a simple linear model to give us a baseline performance score, and to allow us to judge whether a neural network represents an improvement.

```{r}
lrn_lm <- lrn("regr.lm")
```

Now run the cross-validation:

```{r echo=FALSE}
lgr::get_logger("bbotk")$set_threshold("warn")
```

```{r}
rr_lm <- resample(cereal_task, lrn_lm, rsmp_outer)
```

```{r}
rr_lm$aggregate(msr_rmse)
```

For reference, the range of the outcome (`rating`) is about 10 to 100. 

Now we'll set up a neural network learner. Note that we can specify here the number of nodes in the hidden layer. We'll start with 3 nodes to give us a simple model. Note that we specify this directly in the setup of the learner. 

```{r eval=FALSE}
lrn_nn <- lrn("regr.nnet", size = 3)
```

```{r echo=FALSE}
lrn_nn <- lrn("regr.nnet", size = 3, maxit = 100, trace = FALSE)
```

And again, run the resampler:

```{r}
rr_nn <- resample(cereal_task, lrn_nn, rsmp_outer)
rr_nn$aggregate(msr_rmse)
```

We get a RMSE value of `r round(rr_nn$aggregate(msr_rmse), 2)` which is slightly worse than the original linear model. Let's see if increasing the number of hidden nodes to 50 and the number of learning iterations will improve this:

```{r eval=FALSE}
lrn_nn <- lrn("regr.nnet", size = 50, maxit = 500)
rr_nn <- resample(cereal_task, lrn_nn, rsmp_outer)
rr_nn$aggregate(msr_rmse)
```

```{r echo=FALSE}
lrn_nn <- lrn("regr.nnet", size = 50, maxit = 500, trace = FALSE)
rr_nn <- resample(cereal_task, lrn_nn, rsmp_outer)
rr_nn$aggregate(msr_rmse)
```

This has made the model too complex, as the RMSE has increased substantially. 

### Tuning neural networks

Let's now tune the model to see if we can choose the optimal value for this parameter.  First, let's check the available parameters for `regr.nnet`:

```{r echo = TRUE}
lrn_nn$param_set
```

In addition to the number of neurons (`size`), we'll also tune the number of iterations (`maxit`) and the learning decay rate (`decay`). Next, load the **paradox** library, and define the parameter space that we will explore:

```{r}
library(paradox)

tune_ps = ParamSet$new(list(
  ParamInt$new("size", lower = 1, upper = 10),
  ParamInt$new("maxit", lower = 50, upper = 500),
  ParamDbl$new("decay", lower = 0, upper = 1e-4)
))
tune_ps
```

We'll use a random search strategy to select values of the parameters to test:

```{r}
tuner = tnr("random_search")
```

And we'll define a stopping condition. We'll run this for 20 iterations here (20 parameter values). This is a relatively low number to explore a three dimensional parameter space, in practice you would want to increase this (or use a different search strategy).

```{r}
evals = trm("evals", n_evals = 20)
```

Lastly, we'll set up an inner cross-validation strategy for the tuning process. We'll use the same k-fold strategy as above, but with $k=3$:
```{r}
rsmp_inner <- rsmp("cv", folds = 3)
```

With all that in place, let's create an `AutoTuner` that will bring all of this together. 

```{r}
at_nn = AutoTuner$new(tuner,
                      learner = lrn_nn, 
                      resampling = rsmp_inner,
                      measure = msr_rmse, 
                      search_space = tune_ps,
                      terminator = evals)
```

And finally(!), run the `resample` function with this. This will run for a little while as it is building 3 * 5 * 20 models. 

```{r eval = FALSE}
rr_nn <- resample(cereal_task, 
                  at_nn,
                  rsmp_outer,
                  store_models = TRUE)
```

```{r echo = FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
rr_nn <- resample(cereal_task, 
                  at_nn,
                  rsmp_outer,
                  store_models = TRUE)
```

We can extract the results of the tuning process (the inner cross validation) as follows:

```{r}
extract_inner_tuning_results(rr_nn)[,
                                    list(maxit, decay, size, regr.rmse)]
```

From these results, it would appear that the first set of parameters gives the best result. However, when we extract the RMSE for the test set as follows, this has the highest test RMSE, suggesting that these parameters overfit the model. 

```{r}
rr_nn$score(msr_rmse)[,
                      list(regr.rmse)]
```

The results of the second outer fold result in the lowest overall test error, and we'll now go ahead and build a full model with these values. 

```{r}
lrn_nn <- lrn("regr.nnet", 
              size = extract_inner_tuning_results(rr_nn)[2]$size,
              maxit = extract_inner_tuning_results(rr_nn)[2]$maxit,
              decay = extract_inner_tuning_results(rr_nn)[2]$decay)
```

```{r}
cereal_nn <- lrn_nn$train(cereal_task)
```

[Optional section] The final model we obtained is in a format that is quite hard to visualize. You can show the network by first downloading the following function:

```{r}
devtools::source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
```

Then rebuilding the model directly with `nnet` and plotting. The width of the lines reflects the weights and the color reflects the sign (black = positive; grey = negative)

```{r message=FALSE, warning=FALSE}
library(nnet)
cereal_nn2 <- nnet(rating ~ ., 
                   scaled_data,
                   size = extract_inner_tuning_results(rr_nn)[2]$size,
                   maxit = extract_inner_tuning_results(rr_nn)[2]$maxit,
                   decay = extract_inner_tuning_results(rr_nn)[2]$decay)
plot(cereal_nn2)
```

## Neural network classification

Next, we'll build a neural network for a classification task. We'll use a new dataset, containing credit rankings for over 4000 people (see appendix for a description of the fields). The goal will be to predict `Status`, a binary outcome with two levels: `good` and `bad`. We'll start again by reading the data:

```{r}
credit_data <- read.csv("../datafiles/credit_data.csv")
str(credit_data)
```

As we have several categorical variables, we need to make sure that R recognizes these as factors. The following line of code checks each column in the `credit_data` data frame, and if it contains character data, it then converts it to a factor. Note this is similar to the approach in the previous lab, where we convert individual variables to factors:

```{r}
credit_data <- credit_data %>% 
  mutate_if(is.character,as.factor) %>%
  mutate_if(is.integer,as.numeric) 
```

If you run the `summary()` function, you should see that there are several missing values:

```{r}
summary(credit_data)
```

As machine learning algorithms can't use missing data to train, we need to decide what to do with these. For this lab, we'll just exclude them, which results in the loss of about 400 observations, but in the next lab, we'll explore methods to impute values and use these. 

```{r}
dim(credit_data)
credit_data <- credit_data %>%
  na.omit() 
dim(credit_data)
```

Now let's first set up a classification task with the credit dataset:

```{r}
credit_task <- TaskClassif$new("credit", backend = credit_data,
                               target = "Status")
```

As before, we'll set up a resampler and performance metric:

```{r}
rsmp_outer <- rsmp("cv", folds = 5)
msr_auc <- msr('classif.auc')
```

And finally design our neural network. This will have a single hidden layer with 10 nodes:

```{r eval=TRUE}
lrn_nn <- lrn("classif.nnet", size = 10, decay = 1e-5,
              maxit = 500, trace = FALSE)
rr_nn <- resample(credit_task, lrn_nn, rsmp_outer)
rr_nn$aggregate(msr_auc)
```

And let's try with a higher number of nodes and a greater number of iterations:

```{r eval=TRUE}
lrn_nn <- lrn("classif.nnet", size = 25, decay = 1e-5, 
              maxit = 1000, trace = FALSE)
rr_nn <- resample(credit_task, lrn_nn, rsmp_outer)
rr_nn$aggregate(msr_auc)
```

## Exercise

For the exercise, the goal will be to tune the Credit score neural network from the previous section. Using code from the cereal example, set up a nested cross-validation strategy to tune the following parameters:

- Size of the hidden layer
- Maximum number of iterations
- Decay function

You are free to use any cross-validation method for the inner and outer loops, any relevant performance metric (try a few if you are not sure), any definition of the parameter space and the tuning method (e.g. grid vs random). 

Your answer should consist of the following

- A description of how you set up the experiment (e.g. number of folds, tuning method, limits on parameters)
- The values of the parameters from the tuning, along with the *inner* cross validation results
- The outer cross-validation results. Note which results give the best performance on the testing data

You should also provide your full R code. 

## Appendix

### Cereal data set

From https://www.kaggle.com/crawford/80-cereals

|    | Column name | Feature                | 
|----|-------------|------------------------|
| 1  | `Name`      | Name of cereal          |
| 2  | `mfr`       | Manufacturer of cereal  |
|    |             | A = American Home Food Products |
|    |             | G = General Mills |
|    |             | K = Kelloggs |
|    |             | N = Nabisco |
|    |             | P = Post |
|    |             | Q = Quaker Oats |
|    |             | R = Ralston Purina |
| 3  | `type`      | cold or hot |
| 4  | `calories`  | calories per serving |
| 5  | `protein`   | grams of protein          |
| 6  | `fat`       | grams of fat         |
| 7  | `sodium`    | milligrams of sodium   |
| 8  | `fiber`     | grams of dietary fiber           |
| 9  | `carbo`     | grams of complex carbohydrates   |
| 10 | `sugars`    | grams of sugars       |
| 11 | `potass`    | milligrams of potassium       |
| 12 | `vitamins`  | vitamins and minerals - 0, 25, or 100,          |
|    |             | indicating the typical percentage of FDA recommended         |
| 13 | `shelf`     | display shelf (1, 2, or 3, counting from the floor) |
| 14 | `weight`    | weight in ounces of one serving         |
| 15 | `cups`      | number of cups in one serving          |
| 16 | `rating`    | a rating of the cereals          |

### Credit data set 

From https://github.com/gastonstat/CreditScoring

|    | Column name | Feature                | 
|----|-------------|------------------------|
| 1  | `Status`    | credit status          |
| 2  | `Seniority` | job seniority (years)  |
| 3  | `Home`      | type of home ownership |
| 4  | `Time`      | time of requested loan |
| 5  | `Age`       | client's age           |
| 6  | `Marital`   | marital status         |
| 7  | `Records`   | existence of records   |
| 8  | `Job`       | type of job            |
| 9  | `Expenses`  | amount of expenses     |
| 10 | `Income`    | amount of income       |
| 11 | `Assets`    | amount of assets       |
| 12 | `Debt`      | amount of debt         |
| 13 | `Amount`    | loan amount requested  |
| 14 | `Price`     | price of good          |

[poID]: https://mlr3pipelines.mlr-org.com
[filID]: https://mlr3filters.mlr-org.com/index.html
