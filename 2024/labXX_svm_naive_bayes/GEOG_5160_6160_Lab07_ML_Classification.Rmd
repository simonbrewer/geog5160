---
title: "GEOG 5160/6160 Lab 07 Classification methods"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```


## Introduction

In today's lab we will look at three machine learning algorithms used for classification: logistic regression, Naive Bayes and Support Vector Machines (SVM). We will also take a brief look at using **mlr3** to tune the SVM algorithm. Rather than using lots of different datasets, we will stick with the California housing dataset used last week, as use this to try to model and predict districts with high value housing. This also allows us to treat this as a model comparison exercise, where we look to see which of the three methods has the highest predictive skill with these data. 

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notenook for this lab (*GEOG_5160_6160_lab07.ipynb*). Make a new folder for the lab (`lab07`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 

## Data processing

As before, we will start by loading and cleaning the dataset for us. There are several steps we need to take here:

- Remove observations with missing values
- Create variables containing the average number of bedrooms and rooms per district
- Create a Boolean (0/1) variable indicating whether a district is high value or not. We'll define this as being when the median house value for that district is over $250K

Let's start by by loading the data and using the `head()` function to remind us of the available variables/features

```{r eval=FALSE}
housing <- read.csv("../datafiles/housing.csv")
head(housing)
```

```{r echo=FALSE}
housing <- read.csv("data/housing.csv")
head(housing)
```

Next, load the **dplyr** package for data manipulation, and use the `filter()` function to remove observations with missing values:

```{r message=FALSE}
library(dplyr)
housing <- housing %>% 
  filter(!is.na(total_bedrooms))
```

Next, we'll create the features with the average number of rooms and bedroom ratio. Here, we use the `mutate()` function to create a new feature 

```{r}
housing <- housing %>%
  mutate(avg_rooms = total_rooms / households,
         bedroom_ratio = total_bedrooms / total_rooms)
```

Now, we'll create two more features; one Boolean for the house values and one Boolean for the ocean proximity. This uses the `mutate()` function again, and an `ifelse()` function to create the Boolean values. This can be read as `ifelse(condition, a, b)`, where we assign the value `a` if the condition is TRUE and `b` if FALSE.

```{r}
housing <- housing %>%
  mutate(ocean_new = ifelse(ocean_proximity == "INLAND","No", "Yes"),
  mhv_new = ifelse(median_house_value > 250000, 1, 0))
```

Let's check to see what the dataset now contains:
```{r}
names(housing)
```

Let's now remove all the features that we don't want to use with the `select()` function. Note that prepending a `-` before a feature name will remove it.

```{r}
housing <- housing %>%
  select(-longitude, -latitude, -total_rooms, -total_bedrooms, 
         -ocean_proximity, -households, -median_house_value)
names(housing)
```

As a last step, we will center all the numerical features. This is an optional step for most ML methods, but can help in the interpretation of the logistic regression coefficients. We'll use the `mutate()` function again, but this time allow it to overwrite the existing feature

```{r}
housing <- housing %>%
  mutate(housing_median_age = housing_median_age - mean(housing_median_age),
         population = population - mean(population),
         median_income = median_income - mean(median_income),
         avg_rooms = avg_rooms - mean(avg_rooms),
         bedroom_ratio = bedroom_ratio - mean(bedroom_ratio))
```

Let's do a quick check to see what we now have. The `summary()` function, when used with a data frame, will provide summary statistics on each column or feature. You should verify that the `ocean_new` and `mhv_new` variables are Boolean, and that the centered variables all have a mean of zero. If any of these values does not match what you expect, you will need to reprocess these data. 

```{r}
summary(housing)
```

At this point, you would now spend a little time exploring the dataset to look for any outliers or potential problems for the analysis. As a simple step here, we'll just make a couple of box and whisker plots showing the value of two of the numerical features against the outcome (`mhv_new`):

```{r}
boxplot(housing_median_age ~ mhv_new, housing)
```
```{r}
boxplot(median_income ~ mhv_new, housing)
```

Try to make similar plots for `population`, `avg_rooms` and `bedroom_ratio`.

## Logistic regression

We'll start by building a logistic regression model for our dataset. For this method, we'll start by building a model using the basic R functions, then switch to **mlr3**. Logistic regression is a well-established method for modeling binary (and proportional) outcomes. It is part of a family of statistical models called generalized linear models (GLMs) that are designed to work with non-normally distributed data, and the R function to build one is called `glm()`. As this can be used for several different approaches, we need to specify the `family` parameter as `binomial`, i.e. 0/1 data. 

We build this using the standard R formula syntax, in which the tilde `~` is used to separate the outcome variable (on the left hand side) to the independent variables or features on the right hand side. We use the `.` notation on the right hand side to tell R to use all other variable in the `housing` dataset as features.

```{r}
housing.glm <- glm(mhv_new ~ ., housing, family = "binomial")
```

If we now use the `summary()` function on the output, we will get a list of the model coefficients, as well as the results of a test to see if these coefficient are significantly different to zero:

```{r}
summary(housing.glm)
```

As a reminder, this model models the outcome data as *log-odds*, so the coefficients are also in log-odds. To back-transform these to odds, simply use the `exp()` function:

```{r}
exp(coef(housing.glm))
```

As we have centered the data, the intercept tells us the odds of an average house being situated in a high value area $5.62\times10^{-2} \approx 1/18$. The coefficient for the numerical values then give the *rate* of increase of these odds for every one unit increase in that feature. So if a house has one more room than average, the odds increase by 1.06 or about 6%.

### Logistic regression in **mlr3**

We will now switch to using the **mlr3** package that we introduced last week. As a quick reminder, this follows the ML workflow, and requires that we set up

- A *task* that defines the data to be used and the outcome variable
- A *learner* that defines the algorithm to be used
- A *resampling strategy* to validate the algorithm
- A *performance measure* to test how well the model fits to the training data and predicts the test data.

Start by loading the necessary packages

```{r}
library(mlr3)
library(mlr3learners)
```

Now let's define the task. As this is a classification task, we will use `TaskClassif()`. Note that we will be using this same task for all algorithms in this lab, so we only need to do this once:

```{r eval=FALSE}
task_mhv <- TaskClassif$new(id = "mhv", backend = housing, 
                               target = "mhv_new")
```

Well, that didn't work. The function failed and complained that the outcome variable needs to be a *factor*. Factors in R are a specific type of data that define group membership. However, we have just labelled the high/low house value as 0/1, which R recognizes as numeric:

```{r}
class(housing$mhv_new)
```

We can re-type this feature as a factor as follows (we'll change the `ocean_new` feature as well):

```{r}
housing <- housing %>% 
  mutate(mhv_new = as.factor(mhv_new),
         ocean_new = as.factor(ocean_new))
class(housing$mhv_new)
```

Now let's try again to make the classification task:

```{r}
task_mhv <- TaskClassif$new(id = "mhv", backend = housing, 
                           target = "mhv_new")
print(task_mhv)
```

The task object (`task_mhv`) contains a set of useful summary information about the task, telling us the outcome or *target* variable is `mhv_new`, which is binary (not multi-class). There are five numerical features and one factor, and 20433 total observations. 

We'll also define a training and test set. Later, we'll use a full resampling strategy, so we won't need these, but we can use them now to examine how **mlr3** trains a classification task. In the previous lab, we used a simply holdout method, where a proportion of the samples were randomly selected and allocated to the training and test datasets. With categorical outcomes, there is a chance that the dataset might be unbalanced (i.e. more of one class than another), and this can affect resampling, especially if the samples are not recorded in a random order. To check this, let's look at the proportion of low (`0`) and high (`1`) value districts. This line of code

- Extracts the binary labels (`task_mhv$truth()`)
- Creates a table of counts of each type (low/high value)
- Calculates the proportion of each

```{r}
prop.table(table(task_mhv$truth()))
```

Which shows that there are nearly three times as many low value districts as high value. 

To ensure that resampling keeps these proportions in training and testing datasets, we can add a *stratum* to the task to define a a group for stratification. This is set as one of the *roles* in the task. Currently this is not set:

```{r}
task_mhv$col_roles$stratum
```

And we can simply set this to the target variable `mhv_new`. 

```{r}
task_mhv$col_roles$stratum = "mhv_new"
```

Note that the variable used for stratification does not necessarily have to be the target class. (In fact, multiple categorical features can be used for stratification to maintain their frequency distribution in each fold.)

We'll now set up a hold out resampler using the `rsmp()` function and instantiate it for the housing task:

```{r}
set.seed(1234)
holdout_rsmp <- rsmp("holdout", ratio = 0.8)
holdout_rsmp$instantiate(task_mhv)
```

Once the resampler has been instantiated, the row IDs for the training and test set are held under `resampler$instance`. We can extract these, and then use them to calculate the proportion of 0's and 1's in the training and test sets to check that this has stratified correctly"

```{r}
train_set <- holdout_rsmp$instance$train
test_set <- holdout_rsmp$instance$test

prop.table(table(task_mhv$truth()[train_set]))
prop.table(table(task_mhv$truth()[test_set]))
```

Next, we will define the learner. Previously, we used `mlr_learners` to define this. Here we'll use a shortcut function `lrn()`, which has the advantage of a) having less characters to type; and b) allowing us to specify various parameters and hyperparameters directly (we'll see this later). 

```{r}
learner <- lrn("classif.log_reg")
learner
```

And finally, we will train the learner using the training dataset

```{r}
learner$train(task_mhv, row_ids = train_set)
```

Let's look at the model coefficients. These should be close to (but not exactly the same as the model we fit above):

```{r}
learner$model
```

Now predict for the training set to get an estimate of the calibration error:

```{r}
pred_train <- learner$predict(task_mhv, row_ids = train_set)
pred_train
```

You should see that the predicted output is i 0/1's, the same as the input. This means that we can derive a confusion matrix directly:

```{r}
pred_train$confusion
```

However, we would ideally like to use the AUC as a performance measure, and this requires the predictions to be in probabilities, rather than binary classes. If we change the learner setup so that the prediction type is set to `prob`, we can get predictions as probabilities for each class (0 and 1) (see the 4th and 5th column of the output). You should see that the response is just whichever class has the highest probability:

```{r}
learner$predict_type <- "prob"
pred_train <- learner$predict(task_mhv, row_ids = train_set)
pred_train
```

We can now calculate an AUC score for comparison with other methods. 

```{r}
measure <- msr("classif.auc")
pred_train$score(measure)
```

Note that the **mlr3viz** package has a function to plot the ROC curve which is used to derive this AUC (you will also need to install the **precrec** package for this to work):

```{r}
library(mlr3viz)
autoplot(pred_train, type = 'roc')
```
Note  that this is the AUC score for the training data, which gives us an idea of the calibration skill. We can also get the validation AUC to test the predictive skill of the model by making a prediction in the same way for the test set

```{r}
pred_test <- learner$predict(task_mhv, row_ids = test_set)
pred_test$score(measure)
autoplot(pred_test, type = 'roc')
```

```{r}
pred_test <- learner$predict(task_mhv, row_ids = test_set)
pred_test$score(measure)
```

#### Resampling

We'll now set up a full resampling strategy for this task. We'll use a 5-fold cross-validation and the AUC score. We start by setting up the resample with `cv` as the strategy and 5 folds and initialize it with `instantiate`. 

```{r}
cv_rsmp <- rsmp("cv", folds = 5)
cv_rsmp$instantiate(task_mhv)
```

We can get the row indices for the different folds directly from the resampler using `resampler$train_set(i)` and `resampler$test_set(i)`, where `i` is the fold number:

```{r results='hide'}
cv_rsmp$train_set(1)
cv_rsmp$test_set(1)
```

Again, we can check that this has stratified the samples correctly, by calculating the proportion of 0's and 1's:
```{r}
prop.table(table(task_mhv$truth(cv_rsmp$train_set(1))))
prop.table(table(task_mhv$truth(cv_rsmp$test_set(1)))) 
```

You can check the other folds, by replacing `1` with the fold number. We can now use the `resample()` function to run the sample, calculate the performance metrics and print them:

```{r}
rr <- resample(task_mhv, learner, cv_rsmp, store_models = TRUE)
rr$score(measure)
rr$aggregate(measure)
```

The overall AUC is `r round(as.numeric(rr$aggregate(measure)),3)`, which is a pretty good model. Note this down, and we'll now look at a couple of other methods to see if we can improve on this. 


## Naive Bayes

The Naive Bayes approach tries to establish the conditional probability of any class given the values of the input features. Whichever class has the highest overall probability will be used as a prediction for a new case. These conditional probabilities are built either:

- Categorical features: Looking for the relative number of times an output class co-occurs with a feature class 
- Continuous features: Building a probability function to relate values of the feature to the likelihood of a class (usually based on the mean and s.d. values of the feature for that class)

There are a few add-on packages in R that allow you to train a Naive Bayes model directly. One of these **e1071** is based on a C++ library (libsvm) and is very efficient. It is also the default Naive Bayes function called by **mlr3**, so we will use this here to run and train our Naive Bayes model. 

As we have already defined the task, we simply need to redefine our learner to `classif.naive_bayes`:

```{r}
learner <- lrn("classif.naive_bayes", predict_type = "prob")
learner
```

Let's first run it on the hold-out training set (`train_set`) to see what the results of the model look like:

```{r}
learner$train(task_mhv, row_ids = train_set)
```

Let's look at the model output::

```{r}
learner$model
```

The output here is quite different from the previous model. The first thing to note are the prior probabilities. These are the probability of a district being in a low or high value district, independent of the feature values. Only about 28% of the districts are classified as high value. 

The following section provides the conditional probabilities: the probability of a district being classified as high or low value, dependent on the value of the feature. Here, for example, you should be able to see that the mean value for `housing_median_age` in the high value areas (`1`) are much higher (1.84) than the lower value areas (-0.76). Remember that these are centered values, and simply suggest that higher value areas are substantially above the mean income value. The second column gives the standard deviation. Both the values for `housing_median_age` are high (12+) relative to the mean values, suggesting that there may be quite a lot of overlap between the probability distributions of the two classes. 

The only one that is different is the `ocean_new` parameter, which gives the direct conditional probability for each value class in each of the `ocean_new` classes. We can see here that there the conditional probability of being high value if the district is close to the ocean is approximately 94%. 

We'll now run this through the resampling strategy to get a cross-validated AUC score. As the resampler has already been set up and initialized, we can simply reuse this. Note this will mean that the folds will remain the same, in other words, each of the five divisions of the original dataset will contain the same observations as in the logistic regression example. 

```{r}
rr <- resample(task_mhv, learner, cv_rsmp, store_models = TRUE)
rr$score(measure)
rr$aggregate(measure)
```
The results are worse than the logistic regression. This is not too surprising; while the Naive Bayes is a very efficient classifier it is known to be relatively poor in making predictions. It's main use is in classifying large text documents.

## SVM

The last algorithm we will look at today is a support vector machine (SVM). For classification, these work by projecting the dataset in a parameter space defined by the features, then finding a plane or hyperplane that best discriminates between the classes. We will again use the **mlr3** interface, which will call the `svm()` function from the **e1071** package. 

We'll start by fitting a linear SVM, which is based only on the input features. Again, we can use the predefined task, but simply have to create a new learner. Note that we set the `kernel` to be linear, and set the `scale` argument to `TRUE` - this will rescale all features so that the classification isn't biased by some features having much larger units than others. 

```{r}
housing$ocean_new <- ifelse(housing$ocean_new == "Yes", 1, 0)

task_mhv <- TaskClassif$new(id = "mhv", backend = housing, 
                               target = "mhv_new")
```
```{r}
# learner = lrn("classif.svm", kernel = "linear", scale = c(1,1,1,1,1,0), 
#               predict_type = "prob")
learner <- lrn("classif.svm", kernel = "linear", scale = TRUE, 
              predict_type = "prob")
learner
```

Let's first run it on the hold-out training set (`train_set`) to see what the results of the model look like:

```{r}
learner$train(task_mhv, row_ids = train_set)
learner$model
```

The SVM output is a little obtuse. What we can see here though is that this model has required 2430 support vectors, suggesting that the dividing hyperplane is quite complex. 

Let's know run this through **mlr3**, first with a linear SVM kernel, then using a Gaussian radial basis function. Each time, we will run the resampler to obtain a cross-validated AUC score.

#### Linear kernel

```{r}
rr <- resample(task_mhv, learner, cv_rsmp, store_models = FALSE)
rr$aggregate(msr("classif.auc"))
```

Giving us an AUC score of `r round(rr$aggregate(msr("classif.auc")),3)`, which is better than the Naive Bayes model, but not as good as the logistic regression. 

#### Radial basis kernel

Now let's try running this with a radial basis kernel. This allows for non-linear transformations of the original feature set and can often find better ways to split the data than the linear kernel. As we are changing the learner, we need to update the parameters, then retrain it.

```{r}
learner <- lrn("classif.svm", kernel = "radial", scale = TRUE, 
              predict_type = "prob")
rr <- resample(task_mhv, learner, cv_rsmp, store_models = FALSE)
rr$aggregate(msr("classif.auc"))
```

Giving us an AUC score of `r round(rr$aggregate(msr("classif.auc")),3)`, which is surprisingly worse than the linear kernel. This may simply suggest that the dataset is not complex enough to need the different kernel, and the linear methods are sufficient. The SVM algorithm has a couple of key hyperparameters ($C$ for the cost, and $\gamma$ for the RBF function). We'll try re-running the algorithm with a lower value of $\gamma$:

```{r}
learner <- lrn("classif.svm", kernel = "radial", scale = TRUE, 
              predict_type = "prob", gamma = 0.01)
rr <- resample(task_mhv, learner, cv_rsmp, store_models = FALSE)
rr$aggregate(msr("classif.auc"))
```

Dropping $\gamma$ from the default value to 0.01 has a substantial impact on the model, improving the AUC to slightly higher than the logistic regression value. In future labs, we'll look at automatically tuning these parameters to optimize your models. 

## Exercise

The file *Sonar.csv* contains values of 208 sonar signals. The data have 60 features, each representing "the energy within a particular frequency band, integrated over a certain period of time", and an outcome variable `Class`, which is coded `M` or `R`. The goal of the experiment is to discriminate between the sonar signals bounced of a rock `R` or a metal object (a mine `M`). 

For the exercise, you need to carry out a comparison of the three methods presented in today's lab with this new dataset. You should use the **mlr3** framework to set up and test your models, and report the cross-validated AUC score for each one. At the end of this exercise, you should make a recommendation as to which method you think is best for this task. 

Your answer should include

- Your R code [2]
- A list of aggregate AUC scores, one per method [1]
- A note of any parameter settings you used (e.g. for the SVM method) [1]
- The recommendation [1]



