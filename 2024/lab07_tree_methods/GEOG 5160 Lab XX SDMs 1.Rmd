---
title: "Species Distribution Modeling Lab 1"
author: "Simon Brewer"
date: "3/10/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
```

## Species distribution models

Species distribution models refer to a range of methods that are used to identify the environmental variables that limit the existence of some organism in space, then use these to predict how the range may be altered by environmental change. These models have been widely applied to attempt understand the impact of future climate changes on species distribution, and the potential losses (and gains) that are likely to occur. Although most applications are based on ecological datasets, these methods can be used with any point-based spatial data where limiting factors can be identified including wildfire, landslides, disease risk and crime data.

In this lab, we will go through the steps to set up and build a simple species distribution model for the Colorado pine (*Pinus edulis*), a species of pine tree found in the southwest U.S. 

YOu will need to make sure that the following R packages are installed on your computer:

- **dismo**
- **raster**
- **rgdal**
- **RColorBrewer**

## Setup

Start by making a working directory on your hard disk. On the lab computers, it is easiest to make this on the N drive and work from there. Download the following files from Canvas, and place them in this directory

- *Pinus_edulis.csv*
- *borders.zip*
- *worldclim_elev_2.5.nc*

Now start RStudio and change the working directory to the folder you just created. This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder. 

Before doing anything else, run the following command and make sure that you can see the files you downloaded. 

```{r results='hide'}
list.files()
```

Now load the R packages needed to run the lab:

```{r message=FALSE, warning=FALSE}
library(rgdal)
library(dismo)
library(raster)
library(RColorBrewer)
```

## Getting data 1: location data

### Reading data from files

We'll start by reading in and plotting the known locations of *Pinus edulis* trees. The file *Pinus_edulis.csv* contains a set of locations downloaded from the [Global Biodiversity Information Facility (GBIF)][gbifID] (in the next section we will look at how to download your own data from here). We use the `read.csv()` function to read in the data, then check the first few rows using the `head()` function and the types of variables using `str()`:

```{r}
pe = read.csv("./Pinus_edulis.csv")
head(pe)
str(pe)
class(pe)
```

We now convert this to a Spatial* object in R which will facilitate working with other spatial data. This can be done in several ways, but a simple method is to use the `coordinates()` function and specify which columns contain the coordinates:

```{r}
coordinates(pe) <- ~longitude+latitude
class(pe)
```

This has converted the original data into a SpatialPointsDataFrame. We can now plot this:

```{r fig.keep='none'}
plot(pe)
```

Let's make a slightly better map by loading a shapefile of country borders. This is in the zipped file *borders.zip*, so download this to your working directory and unzip it. The shapefile can be read in using the function `readOGR()`:

```{r}
borders = readOGR("./ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp")
```

We can now remake the plot - note that we set the x and y limits to cover a larger area than just the points:
```{r}
plot(pe, xlim=c(-120,-80), ylim=c(25,55), pch=21, bg="darkorange", axes=TRUE)
plot(borders, add=TRUE)
```

### Getting online data

If you need to get your own species data, the **dismo** package includes a function `gbif()` that allows you to download records directly from the GBIF. To demonstrate this, we'll use it here to get occurrence records of the timber rattlesnake *Crotalus horridus*. This function allows selection by Latin name, so here we specify the genus and species names separately (this will take a few second to retrieve all the records:

```{r eval=FALSE}
rattler=gbif('crotalus','horridus')
```

```{r echo=FALSE}
rattler = read.csv("../datafiles/rattler.csv")
```

The returned object has much more information about the occurrence records, including metadata about the original study that supplied the data. Note that all records are returned irrespective of whether or not they have associated geolocations, so we subset only the records that have coordinates (the function does have an argument to exclude records with no longitude or latitude). 

```{r}
rattler = subset(rattler, !is.na(rattler$lon))
coordinates(rattler) <- ~lon+lat
```

Finally we plot the records using the same code as before

```{r}
plot(rattler, pch=21, bg="green", axes=TRUE)
plot(borders, add=TRUE)
```

Note that there are some odd locations, including an observation in Turkey. Before using these data in modeling, we would want to verify and potentially remove these records. 

## Getting data 2: environmental data

### Getting online data

There are a large number of available data sources for environmental data that can be used in species distribution models. We'll take data from the [Worldclim project][wcID] (Hijmans et al. 2005), a collection of standardized climate data at a variety of spatial resolutions. This data can be downloaded directly using the `getData()` function, which allows direct downloads from this and other datasets. This dataset has monthly averages of temperature and precipitation and a set of bioclimatic variables, which represent aggregate climate variables assumed to be linked to species distributions. We will get the set of these at 2.5 arc minute resolution:

```{r eval=FALSE}
current.env=getData("worldclim", var="bio", res=2.5)
```
```{r echo=FALSE}
load("current.env.RData")
```

Let's plot the first of these variables (BIO1: annual temperature):

```{r}
plot(raster(current.env,1))
```

The default colorscale for raster figures in R is an elevational palette. You can change this by using one of the RColorBrewer palettes. To do this, first create a palette using `brewer.pal()`, then add this to the `plot()` function. (To see all available palettes and their acronyms, type `display.brewer.all()`.)

```{r}
my.pal = brewer.pal(9,"YlOrRd")
plot(raster(current.env,1), col=my.pal)
```

As these data are quite large, we will crop the data to the approximate extent of the western US. 

```{r}
myext = extent(c(-130,-100,30,50))
current.env = crop(current.env, myext)
```

Now replot the annual temperature values, and overlay the *Pinus edulis* observations:

```{r}
plot(raster(current.env,1), col=my.pal)
plot(pe, add=TRUE, pch=21, bg="olivedrab")
```

As you have probably noticed the temperature values don't look right. In the original file, these were stored as degrees Celsius $\times 10$, so the last thing that we will do is to divide by 10. 

```{r}
summary(current.env)
```

As we only need to modify the temperature variables (in the first six layers), we do the following: We use the `subset()` function to extract temperature layers and divide by 10, and the precipitation values, then use the `stack()` function to recombine them:

```{r}
current.env = stack(subset(current.env, seq(1,11)) / 10, subset(current.env, seq(12,19)))
# summary(current.env)
```

### Reading data from files

Although the `getData()` function gives access to several datasets, if you are building your own models, you may need to use other datasets. The **raster** package provides functions to access a wide range of gridded data formats. We'll use it here to read in a dataset of elevation values for the study area in the file *worldclim_elev_2.5.nc*:

```{r}
elev = raster("worldclim_elev_2.5.nc")
plot(elev)
```

We can now add this to the other environmental variables:

```{r}
current.env = stack(current.env, elev)
```

## A simple SDM

While there a range of different methods that are commonly used to model species distributions, we will focus today on one of the simplest: generalized linear models (GLMs). GLMs are commonly used across many different disciplines with a variety of different data types, including binary (0/1) data (this is also known as logistic or binomial regression). Here, we will consider the observed presences of P. edulis as 1's, but we will need to add some 'pseudo-absences': locations that represent an absence of this species. 

### Selecting pseudo-absence points

Pseudo-absence points simply refer to a (usually random) set of locations taken from the same study area as the observations, which are assumed to be sites where the species of interest is absent. We will use a simple random selection of points using the `randomPoints()` function, but there are more sophisticated algorithms available (see Barbet-Massin et al. (2012) for more details). 

Note that we need to specify:

- a mask that describes the spatial resolution of the points (one of the environmental layers)
- how many pseudo-absence points we would like (we'll use the same number as we have of presences)
- the coordinates of the presence points (this avoid selecting pseudo-absences in the same location as these)

```{r}
# Randomly sample points (same number as our observed points)
absence <- randomPoints(mask = raster(current.env, 1),  
                   n = nrow(pe), p=pe)
```


```{r}
plot(elev)
points(absence, pch=16, cex=0.75)
plot(pe, add=TRUE, pch=21, bg="darkorange")
```

### Training and testing sets

While we now have all the data we need to build our model, we need to split the data into training and testing sets to allow model validation. The **dismo** package has a function `kfold()` that will partition our observations into a set number of groups. In this lab, we only do a single split of the data - in the next lab we will look at how to carry out a full $k$-fold cross-validation.

```{r}
test.grp <- 1
presence.grp <- kfold(x = pe, k = 5) 
```

This simply creates a vector of group indices from 1 to $k$, which we will now use to partition the observations. $k$ here is set to 5, meaning that each group will contain approximately 20% of the observations.

```{r}
head(presence.grp)
presence.train = subset(pe, presence.grp != 1)
presence.test = subset(pe, presence.grp == 1)
```

To make sure this has worked, examine the dimensions of the two datasets (the training group should have approximately 4 times the observations in it):

```{r results='hide'}
dim(presence.train)
dim(presence.test)
```

Now we do the same thing for the pseudo-absence points:
```{r}
absence.grp <- kfold(x = absence, k = 5)
absence.train = subset(absence, absence.grp != 1)
absence.test = subset(absence, absence.grp == 1)
```

### Combining all the data

The R function we are going to use to build the model requires a data.frame, rather than the spatial objects that we have been working with so far. Our final step in processing the data is then to combine everything we have done so far into two of these, one with the training data, one with the testing data. To do this, we:

- Exyract and combine the coordinates from the presence and absence data
- Extract the environmental variables at these locations
- Create a vector of 0's and 1's to represent presences and absences
- Combine everything into a data frame

```{r}
train.crds <- rbind(coordinates(presence.train), coordinates(absence.train))
train.env <- extract(current.env, train.crds)
train.bin <- c(rep(1, nrow(presence.train)), rep(0, nrow(absence.train)))
train.df <- data.frame( cbind(pa=train.bin, train.env) )
```

And repeat for the testing dataset:

```{r}
test.crds <- rbind(coordinates(presence.test), coordinates(absence.test))
test.env <- extract(current.env, test.crds)
test.bin <- c(rep(1, nrow(presence.test)), rep(0, nrow(absence.test)))
test.df <- data.frame( cbind(pa=test.bin, test.env) )
```

### Building model

Let's start by building a simple model with all variables incorporated. A generalized linear model can be fit in R using the `glm()`, and here we use the `family` argument to specify that we want a binomial model. The syntax `pa ~ .` means model variations in the variable `pa` (presence-absence) with all other variables. 
```{r}
gm1 <- glm(pa ~ .,
           family = binomial(link = "logit"), data=train.df)
```

Let's now use the `summary()` function to look at the model:

```{r}
summary(gm1)
```

The columns of this table list:

- The variable name
- The regression coefficient
- The coefficient standard error
- A test statistic
- A $p$-value indicating the significance of the coefficient

You should be able to see that there are some problems with the model, as one coefficient has no estimated coefficient. This is because it (`bio7`) is a linear combination of two other variables, and so cannot be estimated. 

We'll rebuild our model with a subset of variables related to the seasonality of climate. This will include:

- BIO10 = Mean Temperature of Warmest Quarter
- BIO11 = Mean Temperature of Coldest Quarter
- BIO18 = Precipitation of Warmest Quarter
- BIO19 = Precipitation of Coldest Quarter

Ecological theory suggests that the organisms don't respond in a linear fashion to their environment, but instead had 'optimum' climate ranges. To include this in our model, we will add a second-order polynomial (quadratic) term to the two temperature variables to try and capture this non-linear relationship. Polynomial terms can be included in a model using the `poly()` function, where the second argument defines the order of the polynomial.

```{r}
gm2 <- glm(pa ~ alt + poly(bio10,2) + poly(bio11,2) + bio18 + bio19, family = binomial(link = "logit"), data=train.df)
summary(gm2)
```

This has removed the previous problem (mainly by removing the variable in question!), and the model coefficients are all significant, except perhaps the last one (`bio8`).

### Diagnoses

We will go ahead with this second model, and start by evaluating how well it has worked. The **dismo** package has a function (`evaluate()`) designed specifically to test the predictive skill of presence/absence SDMs. The function can be used in a couple of different ways. Here, we supply three objects:

- The set of environmental variables for the presence points
- The set of environmental variables for the absence points
- The model object

The function will use the model to estimate probability of presence at each point, then create confusion matrices for receiver operating characteristic analysis and other evaluation metrics. This tests power of  a series of probability thresholds to discriminate between the observed presences and (pseudo-)absences. Start by running this for the training data:

```{r}
presence.env <- extract(current.env, presence.train)
absence.env <- extract(current.env, absence.train)
gm2.train.eval <- evaluate(presence.env, absence.env, gm2)
gm2.train.eval
```

The output from this function contains a lot of information, but of most interest now is the area under the ROC curve (training AUC), here equal to `r round(gm2.train.eval@auc, 4)`. This varies between 0 to 1, where anything higher than 0.5 is better than random chance. The value here is close to 1, suggesting that the model discriminates well with the training dataset. Note that we can also plot the ROC curve as follows:

```{r}
plot(gm2.train.eval, "ROC")
```

Now let's look at the AUC for the test data (remember these are observations that were not used to make the model):

```{r}
presence.env <- extract(current.env, presence.test)
absence.env <- extract(current.env, absence.test)
gm2.test.eval <- evaluate(presence.env, absence.env, gm2)
gm2.test.eval
```

Here the testing AUC is substantially lower at `r round(gm2.test.eval@auc, 4)`. This is a very simple evaluation, and in the next lab, we will explore further ways to verify these models. 

## Predicting

Having built and tested our model, the next step is to use it to make a map of predicted presence or absence of our species. Rather than the point observations, we now want to use the full gridded dataset. 

### Current conditions

We'll start by predicting the distributions under current environmental conditions. Predictions can be made using the `predict()` function, a generic R method. One thing to be aware of is that predictions from a logistic model will be on a log-odds scale, and are somewhat difficult to interpret. To avoid this, we use the argument `type='response'` to convert these to probabilities:

```{r}
pe.curr.pred <- predict(current.env, gm2, type='response')
plot(pe.curr.pred)
points(pe, pch=16, cex=0.5)
```

While this does a fairly good job overall, the model appears to be too reliant on elevation as a covariate, causing the relatively high predictions in the mountains close to the west coast. The predictions are likely somewhat elevated toward the south of the region as well. 

As a final step here, we can convert this map into a binary map of presence and absence. To do this, we need to find a threshold which gives the best discrimination. For this, we use the `threshold()` function. This offers a number of methods of determining the cutoff value - here we use `spec_sens`, which sets "the threshold at which the sum of the sensitivity (true positive rate) and specificity (true negative rate) is highest." 

```{r}
pe.thresh <- threshold(x = gm2.test.eval, stat = "spec_sens")
```

We now use this to make the map - note that we remake the predictions on the original log-odds scale:
```{r}
pe.curr.pred <- predict(current.env, gm2)
plot(pe.curr.pred > pe.thresh)
points(pe, pch=16, cex=0.5)
```

### Future conditions

The Worldclim database also contains a set of future climate projections taken from models used in the recent IPCC climate report. These have matching variables and resolutions to the current environmental variables, and can be plugged directly into the model that we have built. 

We will use projections from the Hadley Centre Earth System model (HADGEM2-ES), run under and extreme emission scenario (RCP8.5, i.e. rapid increases in atmospheric greenhouse gases). We use the same `getData()` function as before, but with a few changes. We specify 'CMIP5' as the data source (this is the IPCC modeling project), specify the scenario (`rcp=85`), the model (`HE`) and the year (`70` = 2070). For full details of what is available go to the [Worldclim future webpage][wcfID]. 

```{r}
future.env=getData('CMIP5', var='bio', res=2.5, rcp=85, model='HE', year=70)
```

Once downloaded, we need to go through the same steps as before: 

- crop the data to the study region
- rescale the temperature variables
- include the elevation data

We also need to make sure the variable names match, which we do simply by assigning the current environmental variable names to the future equivalents. 

```{r}
future.env = crop(future.env, myext)
future.env = stack(subset(future.env, seq(1,11)) / 10, subset(future.env, seq(12,19)))
future.env = stack(future.env, elev)
names(future.env) = names(current.env)
```

```{r}
pe.rcp85.pred <- predict(future.env, gm2, type='response')
plot(pe.rcp85.pred)
points(pe, pch=16, cex=0.5)
```

And finally, a map of predicted presences using the threshold generated earlier:
```{r}
pe.rcp85.pred <- predict(future.env, gm2)
plot(pe.rcp85.pred > pe.thresh)
points(pe, pch=16, cex=0.5)
```

The impact of the changed climate is pretty clear here, with a much larger range predicted toward the north. Finally, if we take the difference between the two range maps (future - current), we can identify areas that are at risk of losing the species (in purple here) and areas that will become suitable (in green).

```{r}
pe.curr = pe.curr.pred > pe.thresh
pe.rcp85 = pe.rcp85.pred > pe.thresh

my.pal = brewer.pal(3,'PRGn')
plot(pe.rcp85 - pe.curr, col=my.pal)
```

While logistic regression provides a simple method to model species distributional ranges, it is not usually the best method - here it appears to significantly overpredict the ranges. In the next lab, we will look at two more widely used methods (random forests and maximum entropy), and look to see if they can improve on this.

## Appendix 1: Bioclimate variables

- BIO1 = Annual Mean Temperature
- BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp))
- BIO3 = Isothermality (BIO2/BIO7) (* 100)
- BIO4 = Temperature Seasonality (standard deviation *100)
- BIO5 = Max Temperature of Warmest Month
- BIO6 = Min Temperature of Coldest Month
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO8 = Mean Temperature of Wettest Quarter
- BIO9 = Mean Temperature of Driest Quarter
- BIO10 = Mean Temperature of Warmest Quarter
- BIO11 = Mean Temperature of Coldest Quarter
- BIO12 = Annual Precipitation
- BIO13 = Precipitation of Wettest Month
- BIO14 = Precipitation of Driest Month
- BIO15 = Precipitation Seasonality (Coefficient of Variation)
- BIO16 = Precipitation of Wettest Quarter
- BIO17 = Precipitation of Driest Quarter
- BIO18 = Precipitation of Warmest Quarter
- BIO19 = Precipitation of Coldest Quarter


[gbifID]: https://www.gbif.org
[wcID]: http://worldclim.org
[wcfID]: http://www.worldclim.org/CMIP5v1