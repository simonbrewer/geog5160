---
title: "GEOG 5160/6160 Lab 11 Unsupervised methods"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "April 7, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
```

## Unsupervised methods

In this lab, we'll look at how to implement two unsupervised classification methods:

- $k$-means classification
- Self-organizing maps

The data we will use is from the [Gap Minder project][gapID]. While you can download the individual variables from the web site, we will use a subset of data in the file *gapminder07.csv*. This is a slightly different subset of variables than the example used in class, with values only for 2007. Download this to your `datafiles` folderand make a new folder for today's class called `lab11`. You will also need the shapefile of country borders: `ne_50m_admin_0_countries.shp` (you should have this from a previous lab).

### R users

Start RStudio and set the working directory to this directory (This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder). 

You will need the following packages for today's lab, so make sure to install anything that is missing before proceeding.

- **dplyr**
- **ggpubr**
- **RColorBrewer**
- **countrycode**
- **sf**
- **kohonen**

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notebook for this lab (*GEOG_5160_6160_lab08.ipynb*). Make a new folder for the lab (`lab08`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). 

You will need to make sure the following packages are installed on your computer (in addition to the packages we have used in previous labs). 

- **xarray**: functions for working with regular arrays (`conda install xarray`)
- **xgboost**: extreme gradient boosting (`conda install py-xgboost`)

Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 




Once you have done this, make sure this is set as your working directory in R/RStudio and load the data. 

```{r}
gap1.df = read.csv("../datafiles/gapminder07.csv")
head(gap1.df)
```

Now load the following directories to process and visualize the data:

```{r}
library(dplyr)
library(ggpubr)
library(RColorBrewer)
```

We can now make some plots to look at the data. First, histograms of the three variables in the data (gdp per capita, population and life expectancy):

```{r}
gap1.df %>%
  gghistogram(x = "pop")
gap1.df %>%
  gghistogram(x = "lifeExp")
gap1.df %>%
  gghistogram(x = "gdpPercap")
gap1.df %>%
  gghistogram(x = "infant_mortality")
gap1.df %>%
  gghistogram(x = "fertility")
```

The histograms clearly show the skew in most of the variables, and we will need to transform the data before analyzing it. We'll do this in two steps. First, we'll log transform all variables to remove the skew, then use the `scale()` function, which in R scales data to $z$-scores by subtracting the mean and dividing by the standard deviation. To simplfy things, we do all this in a single step by nesting the `log()` function in the `scale()` function:


```{r}
gap2.df = gap1.df %>%
  mutate(pop = scale(log(pop)),
         lifeExp = scale(log(lifeExp)),
         gdpPercap = scale(log(gdpPercap)),
         infant_mortality = scale(log(infant_mortality)), 
         fertility = scale(log(fertility)))
head(gap2.df)
```

## $k$-means cluster analysis

We'll next use these data with the $k$-means cluster algorithm. The default function for this in R is `kmeans()`, and we need to pass the data we are going to use, and the number of requested clusters to this function:

```{r}
gap.kmeans = kmeans(gap2.df[,5:9], centers = 6)
```

There output of this algorithm provides information about the cluster solution. We can see the size of each cluster (the number of observations assigned to each cluster)

```{r}
gap.kmeans$size
```

And we can see the cluster assignments for each country:

```{r}
gap.kmeans$cluster
```

And finally the cluster `centers`. These are the prototypes; the average value of each variable for all the observations assigned to a given cluster (note these are the log-transformed and scaled data)
```{r}
gap.kmeans$centers
```

Here, we can see that cluster 6, for example, has the lowest life expectancy and GDP, as well as high infant mortality and fertility rates. 

### Maps

As these are spatial data, we can now plot the distribution of the clusters. First, we'll load the world shapefile in the `ne_50m_admin_0_countries` folder (we used this in a previous lab:)

```{r}
library(sf)
borders = st_read("../datafiles/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp")
```

Next, we add the cluster assignments to the `gap2.df` data frame:
```{r}
gap2.df$kmeans = as.factor(gap.kmeans$cluster)
```

Now we need to merge the `gap2.df` data frame with the world borders spatial object. To do this, we add a new column to the data frame containing the ISO3 standard country codes, using the **countrycode** package (you'll need to install this).

```{r}
library(countrycode)
gap2.df <- gap2.df %>% 
  mutate(ISO3 = countrycode(country, "country.name", "iso3c"))
```

Finally, we use these ISO codes to merge the two datasets. We need to specify the column name for each dataset that contains the label to be used in merging, which we do with `by.x` and `by.y`. 
```{r}
cluster_sf = merge(borders, gap2.df, by.x = "ADM0_A3", by.y = "ISO3")
```

Finally, we can plot out the clusters using the **tmap** package, which highlights the position of this poorer cluster 6 in Saharan and sub-Saharan Africa:

```{r}
library(tmap)
tm_shape(cluster_sf) + tm_fill("kmeans", palette = "Set2")
```

## Self-organizing maps

We'll now repeat this analysis with a self-organizing map (SOM). For this we'll use the **kohonen** package, which allows the construction of unsupervised and supervised SOMs. 

```{r}
library(kohonen)
```

Building the SOM is relatively simple: first we construct a grid (8x7) using the `somgrid()` function. Note the `topo` argument that controls the neighborhood shape (rectangular or hexagonal):

```{r}
mygrid = somgrid(xdim = 8, ydim = 7, topo = "hexagonal")
```

You can plot the empty grid to see the layout with (`plot(mygrid)`). 

Now we'll train the SOM. The function we use is `som()`, and we need to specify the data to be used as well as the SOM grid. The function requires the data to be as a matrix rather than an R data frame, so we first convert this, then pass it to the function:

```{r}
gap2.mat = as.matrix(gap2.df[,5:9])
gap.som = som(gap2.mat, mygrid)
```

Once finished, we need to see if the algorithm has converged. We can do this by plotting the change in the loss function as a function of the iteratons. As a reminder, the loss function here is the mean distance between the node weights and the observations assigned to that node:

```{r}
plot(gap.som, type = "changes")
```

We can see here that the loss function has decreased, but not stabilized suggesting that the algorithm has converged. We'll re-run it for a larger number of iterations by setting `rlen` to 2000:

```{r}
gap.som = som(gap2.mat, mygrid, rlen = 2000)
plot(gap.som, type = "changes")
```

This plot shows a series of step like drops in the loss function, finally stablizing at approximately iteration 1600. 

### Plots

Next we'll go through some visualizations of the data. We've already see then loss function changes, so next we'll make a couple of plots to show how the SOM has trained. First a plot of distances between nodes, which can be useful to identify outliers:

```{r}
plot(gap.som, type = "dist")
```

Next, we plot the number of observations per cluster to look for empty nodes. 

```{r}
plot(gap.som, type = "counts")
```

THe map has a couple of empty nodes. Empty nodes can be common when using larger grid sizes, and can be taken to represent a part of the parameter space that is missing in the observations. 

Next, we'll plot the codebook vectors. These are the weights associated with each feature, and can be used to interpret the organization of the SOM. 

```{r}
plot(gap.som, type = "code")
```

The map shows a clear gradient from poorer regions (`gdpPercap`, orange) on the left to richer on the right, and a gradient from smaller populations at the top to smaller at the bottom. Note that the life expectancy follows the gradient of GDP fairly well, and the poorer regions tend to have higher infant mortality and fertility rates. 

It is also possible to plot out the individual feature weights as a heatmap to illustrate the gradient using `type = "property"`. The codebook vectors of weights are contained within the `gap.som` object. The syntax to extract them is a little complicated, and some examples are given below

```{r}
plot(gap.som, type = "property", property = gap.som$codes[[1]][,"pop"],
     main = "Pop")
```

```{r}
plot(gap.som, type = "property", property = gap.som$codes[[1]][,"lifeExp"],
     main = "Life Exp.")
```

```{r}
plot(gap.som, type = "property", property = gap.som$codes[[1]][,"fertility"],
     main = "Fertility")
```

### Cluster analysis

As a next step, we'll cluster the nodes of the map to try and identify homogenous regions to simplify the interpretation of the SOM:

```{r}
nclus = 6
som.cluster <- kmeans(gap.som$codes[[1]], nclus)$cluster
mypal = brewer.pal(nclus, "Set2")
# plot these results:
plot(gap.som, type="mapping", bgcol = mypal[som.cluster], 
     main = "Clusters", pch = '.') 
add.cluster.boundaries(gap.som, som.cluster)
```

Next, we obtain aggregate values for each cluster. While we can get these values from the `gap.som` object, we can also work back to get values from the original data. In the following code w

- Extract a vector with the node assignments for each country
- Use this to get the cluster number for the node that the country is assigned to
- Attach this to the original `gap2.df` dataframe

```{r}
nodeByCountry = gap.som$unit.classif
gap1.df$somclust  = som.cluster[nodeByCountry]
```

Now we can use this list of clusters by country to aggregate the variables:

```{r}
cluster.vals = aggregate(gap1.df[,5:9], 
                         by = list(gap1.df$somclust), mean)
```

For this some, we get the following table:

```{r echo=FALSE}
knitr::kable(cluster.vals)
```

This shows the approximate characteristics for the clusters:

1. High life expectancy and GDP, low mortality and fertility rates, medium population
2. Low life expectancy and GDP, very high mortality and fertility rates, medium population
3. Low life expectancy and GDP, high mortality and fertility rates, small population
4. Medium life expectancy, low GDP, medium mortality and fertility rates, large population
5. High life expectancy and GDP, lower mortality and fertility rates, large population
6. High life expectancy and medium GDP, medium mortality and fertility rates, small population

We can now add country labels to the plot. To do this, we use the `type = "mapping"`, and provide the list of country names as labels. We pass this through the `abbreviate()` function to keep the labels short:

```{r}
plot(gap.som, type="mapping", labels = abbreviate(gap1.df$country, 6),
     bgcol = mypal[som.cluster], 
     main = "Clusters", cex = 0.5, keepMargins=TRUE) 
```

Finally, we add the SOM cluster assignments back to the `cluster_sf` spatial object and map them out:

```{r}
gap2.df$somclust = as.factor(gap1.df$somclust)
cluster_sf = merge(borders, gap2.df, by.x = "ADM0_A3", by.y = "ISO3")
tm_shape(cluster_sf) + tm_fill("somclust", palette = "Set2")
```

## Exercise

For the exercise, you will need to build use an unsupervised classification method with the California housing dataset using a self-organizing map. Your answer should use the following variables (you can use code from a previous example to create the `avg_rooms` and `avg_bedrooms` variables):

- housing_median_age 
- population
- median_income
- median_house_value 
- avg_rooms
- avg_bedrooms

Your answer should consist of the following:

- Your R code
- A plot showing that the loss function has stabilized
- A plot of the counts per nodes
- A plot of the codebook vectors
- One or more heatmaps of the individual features on the SOM
- A $k$-means clustering of the SOM nodes (and a figure showing these on the SOM grid)
- A table given the average values of each feature per cluster

You do not need to provide a map of the SOM clusters, but if you do, the following code will be useful:

- Run this before selecting the subset of features for use in the classification. This makes a copy of the district coordinates
```{r eval=FALSE}
housing.crds = housing %>%
  select(longitude, latitude)
```

- Run this after the cluster analysis to add the clusters and coordinates back to the `housing` data frame, and convert to a Spatial* object. This assumes that the SOM output is stored in `housing.som` and the cluster assignment in `som.cluster`:

```{r eval=FALSE}
housing$cluster = as.factor(som.cluster[housing.som$unit.classif])
housing = cbind(housing, housing.crds)
coordinates(housing) = ~longitude+latitude
```


[gapID]: https://www.gapminder.org