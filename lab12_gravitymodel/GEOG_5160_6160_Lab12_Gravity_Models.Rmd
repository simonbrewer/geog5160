---
title: "GEOG 5160/6160 Lab 12 Spatial interaction models"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
```

## Spatial interaction models

In today's lab we will walk through the steps of building a spatial interaction model of commuting flows between London boroughs, using population size and median salary information. The code in this lab is modified from an example given by Dr. Adam Dennett of UCL's Center for Applied Spatial Analysis. You will need a set of R packages to carry out all parts of this lab, so make sure these are installed, then start loading them:

```{r message=FALSE, warning=FALSE}
library(sp)
library(MASS)
library(reshape2)
library(rgdal)
library(maptools)
library(dplyr)
library(stplanr)
library(ggplot2)
library(leaflet)
```

## Read in data

First, let's read in the commuting flow data. This is taken from the 2001 UK Census, downloaded from https://wicid.ukdataservice.ac.uk/. 

```{r}
cdata = read.csv("../datafiles/LondonCommuting2001.csv")
head(cdata)
```

If we take a look at the first few lines of the file you will see that each line gives the commuting flow between one origin (`Orig`) and one destination (`Dest`). Commuting flows are given as total flow (`Total`), and broken down by different modes of transportation. In the last few columns, population and income variables have been added for each origin-destination pair. There are a total of `r nrow(cdata)` pairs of commuting flows.

```{r}
nrow(cdata)
```

Next, read in and plot a shapefile of the London boroughs:

```{r}
LondonBNG = readOGR("../datafiles/LondonBNG/London.shp")
plot(LondonBNG)
```

Before we get to modeling the flow data, there are a couple of pre-processing steps that we need to undertake. 

First, calculate a matrix of distances between all pairs of boroughs (this will be the $d_{ij}$ term in our gravity model), and for this we use the function `spDist()`:

```{r}
dist <- spDists(LondonBNG)
dist[1:10,1:10]
```

Next, we need to transform this for use with the commuting flow data. Here, we use the function `melt()` from the **reshape2** package. This is a neat little function that takes a matrix or some other wide format data, and melts it down into a thin vector format, where the first two columns refer to a row-column pair from the distance matrix, and the third gives the distance between them. 
```{r}
distPair <- melt(dist, varnames = c("Dest","Orig"), value.name = "dist")
head(distPair)
```

Now add the distance column into the dataframe, and take a look the modified data frame:
```{r results='hide'}
cdata$dist <- distPair$dist
head(cdata)
```

Next we will swap the order of some of the columns. This is not important for modeling, but will allow us to visualize the flow data. For visualization, we will use the `od2line` function from the **stplanr** package. This takes a set of O/D pairs and generates a spatial network between them, by extracting the coordinates of each pair from a spatial object. It requires that the first two columns of the O/D data frame are the codes of the origin and destination that match the spatial location in the shapefile. There are a couple of ways of doing this, but to make our lives as easy as possible, we'll use the `select()` function from the **dplyr** package:

```{r}
cdata2 <- dplyr::select(cdata, OrigCodeNew, DestCodeNew, Total, everything())
cdata2$Orig <- as.factor(cdata2$Orig)
```

And finally, we remove the intra-borough flows. As the distances generated by `spDist()` for these are zero, these can cause in the models we generate. These can be included, by setting the internal borough travel distance to some positive value, but to make out lives easier, we will just exclude them here and focus on the inter-borough flows. 

```{r}
cdata2 <- cdata2[cdata2$OrigCode!=cdata2$DestCode,]
```

With that done, we can visualize the flow data using the `od2line` function from the **stplanr** package. This needs a set of pairs of locations (from `cdata`) and a shapefile with the spatial geometry of the individual locations (this is the shapefile we loaded earlier). With these, it will create a a network between locations:
```{r}
travel_network <- od2line(flow = cdata2, zones = LondonBNG)
```

To get some sense of the size of the flows, we can use this variable to set the line widths of the network links (as a weighting factor `w`), and then plot it:
```{r}
w <- cdata2$Total / max(cdata2$Total) * 10
plot(LondonBNG)
plot(travel_network, lwd = w, add=TRUE)
```

Now we can create pivot table to turn paired list into matrix (and compute the margins as well)
```{r}
cdata2mat <- dcast(cdata2, Orig ~ Dest, sum, value.var = "Total", margins=c("Orig", "Dest"))
cdata2mat[1:10,1:10]
```

## Modeling

Now let's produce a gravity model based on this flow data. The equation for the classic unconstrained gravity model is:

\[
T_{ij}=k V_i^\mu W_j^\alpha d_{ij}^\beta 
\]

Itâ€™s perfectly possible to produce some flow estimates by plugging some arbitrary or expected estimated values into our parameters. The parameters relate to the scaling effect / importance of the variables they are associated with.

A starting assumption may be that the effects of origin and destination attributes on flows scale in a linear fashion (i.e. for a 1 unit increase in the population at an origin results in a 1 unit increase in flows of people from that origin, or halving the average salary at a destination, will half the inflow of commuters). This would imply that both $\mu$ and $\alpha$ are set to 1. For $\beta$, we will use a value of -2, suggesting that commuting flow follows a power law with respect to distance.

With these coefficients, we can start to make our flow estimates. To try and keep this as clear as possible, we'll estimate each term in the gravity mode equation individually. 

- First the term $V_i^\mu$
```{r}
mu <- 1
vi1_mu <- cdata2$OrigPop^mu
```

- Next the term $W_j^\alpha$
```{r}
alpha <- 1
wj2_alpha <- cdata2$DestSal^alpha
```

- Then the term $d_{ij}^\mu$
```{r}
beta <- -2
dist_beta <- cdata2$dist^beta
```

Finally, we need an estimate of $k$, the proportionality constant. As the name suggests, this is a constant, and is used to avoid over- or under-estimating the total number of flows. When we have some information about the expected flow data, we can estimate $k$ based on the total flows in the region and the equation given above:

\[
k = \frac{T}{\sum_i \sum_j V_i^\mu W_j^\alpha d_{ij}^\beta}
\]

Where $T$ is the total number of *all* commuting flows or trips in the dataset. Effectively, this then scales the amount of flows based on population and salary alone to the observed number of trips, which has the effect of making the predicted value of the total number of trips ($\sum \hat{T}_{ij}$) correct. Note that if we don't know the total flow data, $k$ is usually taken from previous studies or by trial and error. (While we refer to this model as an unconstrained gravity model, the use of $\sum T_{ij}$ technically makes this a *total* constrained model.)

```{r}
k = sum(cdata2$Total) / sum(vi1_mu * wj2_alpha * dist_beta)
```

We can now use the gravity model to estimate the flows by simply multiplying all this together, and store this back in the original data frame:

```{r}
cdata2$unconstrainedEst1 <- round(k * vi1_mu * wj2_alpha * dist_beta, 0)
```

To make sure that $k$ is doing it's job correctly, estimate the observed total number of flows:
```{r}
sum(cdata2$Total)
```

And compare this to the predicted total:
```{r}
sum(cdata2$unconstrainedEst1)
```

If you want to see the results as an O/D matrix, we can use the `cast()` or `dcast()` function to turn the set of inter-borough flows into a matrix. 

```{r}
#turn it into a little matrix and have a look at your handy work
cdata2mat1 <- dcast(cdata2, Orig ~ Dest, sum, value.var = "unconstrainedEst1", margins=c("Orig", "Dest"))
```

And if we look at a part of this matrix, we can see the resulting predicted flows:

```{r}
cdata2mat1[1:10,1:10]
```

### Model goodness-of-fit

For a small set of locations, we can take a look at this matrix and compare it to known flows (the `cdata2mat` matrix shown above). For larger datasets, this quickly becomes impractical. Instead, let's calculate the root mean squared error (RMSE) between the observed and predicted flows. 

As a reminder, the RMSE is given by the following equation, and is basically an estimate of the average error in the model estimates. The closer to 0 the RMSE value, the better the model.

\[
RMSE = \sqrt{\frac{\sum_i(y_i-\hat{y}_i)^2}{n}}
\]

```{r}
sqrt(mean((cdata2$Total-cdata2$unconstrainedEst1)^2))
```

## Calibrating the model

The traditional way to improve the model fit has been by gradually changing the set of parameters, and rebuilding the model until the RMSE is as low as possible. However, we can also do this by regression. If we log-transform the gravity model given above, we get the following equation:

\[
\mbox{log} T_{ij}=\mbox{log}(k) + \mu \mbox{log}(V_i) + \alpha \mbox{log} (W_j) + \beta  \mbox{log} (d_{ij})
\]

Which you should recognize as a basic linear regression model, where the intercept gives us an estimate of $\mbox{log}(k)$, and the slope coefficients give us estimates of $\mu$, $\alpha$ and $\beta$, respectively. 

To get a sense of what this is trying to do, let's make a plot of the untransformed flow data against the individual OD distances. 

```{r}
plot(cdata$dist, cdata$Total,
     main="London commuting flow data", xlab="Dist (dij)", ylab="Flow (Tij)")
```

Not a very linear fit between the flow and distance. If we do the same, but log-transform both variables, we get the following:

```{r}
plot(log(cdata$dist), log(cdata$Total),
     main="London commuting flow data", xlab="Dist (dij)", ylab="Flow (Tij)")
```

And while there is a lot of scatter, we can see a linearly declining relationship: less people travel over longer distances. If we could estimate the slope of this decline, this would give us the estimate of $\beta$. 

While it is entirely feasible to fit this using a regular linear regression, based on ordinary least squares (OLS), this approach has been criticized as being unsuitable for modeling flow data. OLS models are designed for use with continuous, unbounded outcome variable (i.e. something that can potentially take a value from -ve infinity to +ve infinity). Flow data cannot be negative, at the minimum a flow is equal to or bounded by zero. Also, we are generally dealing with unit objects: a person, a car, etc. A better approach is based on the use of Poisson regression, another form of generalized linear model (GLM). This models the flows directly as untransformed counts, using the following equation:

\[
T_{ij}=\mbox{exp}( \mbox{log}(k) + \mu \mbox{log}(V_i) + \alpha \mbox{log} (W_j) + \beta  \mbox{log} (d_{ij}) )
\]

All this has done is to eliminate the log term on the left hand side and replace it with an exponent on the right hand side. This means that we model the untransformed counts, which by definition are non-negative (bounded at zero) and integer values. This has some further advantages in how the model treats the errors, which make this approach more robust. 

To fit this in R, we use the `glm()` function, which we encountered a couple of labs ago. The syntax to fit this for the London flow data is:

```{r}
uncosim <- glm(Total ~ log(OrigPop)+log(DestSal)+log(dist), na.action = na.exclude, family = poisson(link = "log"), data = cdata2)
```

Note that we still log transform the covariates (population, etc), and use the argument `family=poisson` to tell the function that we are using count data. Now let's look at the output:

```{r}
summary(uncosim)
```

So what does all this tell us? The most important is the table of coefficients. We get the following estimates:

- $\mbox{log}(k)$: `r coef(uncosim)[1]`
- $\mu$: `r coef(uncosim)[2]`
- $\alpha$: `r coef(uncosim)[3]`
- $\beta$: `r coef(uncosim)[4]`

The final column of this table gives an estimate of the significance of the coefficient (`Pr(>\z\)`). The lower this is, the higher the significance in explaining the flow data. Here all the coefficients we have used are highly significant. 

Comparing these to the original simple model above, we see that this first model underestimates the coefficients for emission ($\mu$) attraction ($\alpha$). As both of these are above 1, it suggest that as both population and salary increase in an area, this results in proportionally higher commuting flows. The distance coefficient ($\beta$) was overestimated, but is still above 1, indicating the per-unit travel cost increase as distance increase. 

As before, we can calculate the RMSE for this model to compare against the simple model. To do this we use the `fitted()` function to get the predicted flows for each pair of locations, then round up. 

```{r}
cdata2$unconstrainedEst2 = round(fitted(uncosim), 0)
```

Now reuse the code from above to calculate the RMSE.

```{r}
## Model 1
sqrt(mean((cdata2$Total-cdata2$unconstrainedEst1)^2))
## Model 2
sqrt(mean((cdata2$Total-cdata2$unconstrainedEst2)^2))
```

Which shows that, for our calibrated model, the RMSE has dropped by approximately 20%. 

And calculate the total estimated flow:
```{r}
sum(cdata2$unconstrainedEst2)
```

Which matches well to the total observed flow:
```{r}
sum(cdata2$Total)
```

This is unsurprising as we used the sum of all flows to calibrate the $k$ coefficient. Next convert your estimates back to an O/D matrix and look at the estimates:

```{r results='hide'}
cdata2mat2 <- dcast(cdata2, Orig ~ Dest, sum, value.var = "unconstrainedEst2", margins=c("Orig", "Dest"))
cdata2mat2[1:10,1:10]
```

While the total is well constrained, the margins of the matrix are not. For example, we can compare the estimates of total outflow from each origin by looking at the row totals of the observed O/D matrix, and the new modeled one. If we exclude the extreme values, the predicted totals appear somewhat biased.

```{r}
totoutflowObs = cdata2mat[,"(all)"]
totoutflowPred = cdata2mat2[,"(all)"]
plot(totoutflowObs, totoutflowPred, log="xy",
     xlab="Observed outflow", ylab="Predicted outflow")
abline(0,1)
```

And we can do the same for the inflows. To do this, we need to extract the relevant row, which is a little more complex.
```{r}
totinflowObs = as.numeric(cdata2mat[cdata2mat$Orig=="(all)", -1])
totinflowPred = as.numeric(cdata2mat2[cdata2mat2$Orig=="(all)", -1])
plot(totinflowObs, totinflowPred, log="xy",
     xlab="Observed inflow", ylab="Predicted inflow")
abline(0,1)
```

Now, let's recycle the code from above to plot the predicted flows:
```{r}
w <- cdata2$unconstrainedEst2 / max(cdata2$unconstrainedEst2) * 10
plot(LondonBNG)
plot(travel_network, lwd = w, add=TRUE)
```

We can also plot the model errors on the same network. To do this, we first calculate the residuals, then translate them to network weights, using a somewhat arbitrary weighting. Finally, we plot the absolute values, color coded by if they are positive (red) or negative (blue) errors). 

```{r}
r <- cdata2$unconstrainedEst2 - cdata2$Total
w <- abs(r) / sum(abs(r)) * 250
err <- ifelse(r > 0, 2, 4)
plot(LondonBNG)
plot(travel_network, lwd = w, add=TRUE, col=err)
```

## Constrained model

**THIS IS AN OPTIONAL PART OF THE LAB**

In the previous step, we estimated an unconstrained model. While here, we were able to calibrate this using the full set of flow (as these were available), this model can also be fit to partial, incomplete O/D flow information. As we have the full O/D matrix, we can improve this model by using extra information to constrain the model. 

- If we use the row (origin) totals, we get a *production* constrained model, e.g. if we have information on the amount of disposable income and shopping habits of the people living in different areas from loyalty card data

- If we use the column (destination) totals, we get an *attraction* constrained model, e.g. if we have information on new infrastructure developments such as a new airport, and wish to understand the impact on transportation flows

- If we have use both row and column totals, we get the *doubly* constrained model

### Production constrained model

This model is given by

\[
T_{ij} = A_i O_i W_j^\alpha d_{ij}^\beta 
\]

Where $O_i$ represents the total outflow from zone $i$
\[
O_i=\sum_j T_{ij}
\]

And $A_i$ is a balancing factor it account for different levels of productivity. To model this, we just rewrite the equation of the Poisson model to allow estimates of this factor:

\[
T_{ij}=\mbox{exp}( \mbox{log}(k) + \mu_i + \alpha \mbox{log} (W_j) + \beta  \mbox{log} (d_{ij}) )
\]

Here the coefficient $\mu_i$ is a vector of balancing factors for each zone. In a model, this is just a set of *dummy variables*, which we represent here by using the Borough name. Note that we also keep $k$ to represent the model intercept. In R, the syntax for this model is given as:

```{r results='hide'}
prodSim <- glm(Total ~ Orig+log(DestSal)+log(dist), na.action = na.exclude, family = poisson(link = "log"), data = cdata2)
summary(prodSim)
```

The main difference between this and the previous model is that we no longer have a single estimate of $\mu$, but instead have set of coefficients for each origin. You might see that the first location is missing (the borough of Barking and Dagenham). This is used as the the reference location, and the rest of the coefficients indicate how important the other locations are as origins for commuters. 

We might wish to have a less arbitrary choice of location as origin. For London, the City of London (shown on the map below) can be assumed to be the main destination for commuters. 

```{r}
plot(LondonBNG)
plot(LondonBNG[LondonBNG$lad15nm=="City of London",], add=TRUE, col=2)
```

To set this as the reference level in R, use the following code:

```{r}
cdata2$Orig2 = relevel(cdata2$Orig, ref = "City of London")
```

And let's use this to build a second model with the City of London as reference:
```{r results='hide'}
prodSim2 <- glm(Total ~ Orig2+log(DestSal)+log(dist), na.action = na.exclude, 
               family = poisson(link = "log"), data = cdata2)
summary(prodSim2)
```

Now all these coefficients ($\mu_i$) are relative to the City of London. Let's map them out to see what the spatial pattern looks like. Here, we use the `coef()` function to extract these, and add them to the shapefile. Note that we need to remove the 1st and last two coefficients (the intercept and $\alpha$ and $\beta$ slopes).

```{r}
modcoef = coef(prodSim2)[-c(1,34,35)]
LondonBNG$prodcoef = c(0, modcoef)
```

And then plot it, to show how commuting flows increase as we move further away from the center closer to the suburbs.

```{r}
spplot(LondonBNG, "prodcoef")
```

Now let's explore the predicted flows. First, get the flow for each O/D pair from the original model:

```{r results='hide'}
cdata2$prodsimFitted <- round(fitted(prodSim), 0)
```

Then create an O/D matrix, with the marginal sums of total outflow (rows) and inflows (columns):
```{r results='hide'}
cdata2mat3 <- dcast(cdata2, Orig ~ Dest, sum, value.var = "prodsimFitted", margins=c("Orig", "Dest"))
cdata2mat3
```

If we now redo the comparison of the outflow values, we can see that the constraint has worked and the outflows match.

```{r}
totoutflowObs = cdata2mat[,"(all)"]
totoutflowPred = cdata2mat3[,"(all)"]
plot(totoutflowObs, totoutflowPred, log="xy",
     xlab="Observed outflow", ylab="Predicted outflow")
abline(0,1)
```

Including the constraint has also improved the model RMSE, showing some benefit of incorporating this extra information in the model. 

```{r}
sqrt(mean((cdata2$Total-cdata2$prodsimFitted)^2))
```

### Attraction constrained model

The attraction constrained model uses information about the total inflow to each region to constrain the model:

\[
T_{ij} = D_j B_j V_i^\mu d_{ij}^\beta 
\]

Where $D_i$ represents the total outflow from zone $i$
\[
D_j=\sum_i T_{ij}
\]

And $B_j$ is a balancing factor it account for different levels of attraction. We make a similar modification to the Poisson model to accommodate this:

\[
T_{ij}=\mbox{exp}( \mbox{log}(k) + \mu \mbox{log} V_i + \alpha_i + \beta  \mbox{log} (d_{ij}) )
\]

Now we have a vector of dummy variables ($\alpha_i$), one per zone, representing the level of attractiveness for commuting. In R, we simply include the destinations rather than the origins:

```{r results='hide'}
attrSim <- glm(Total ~ Dest+log(OrigPop)+log(dist), na.action = na.exclude, family = poisson(link = "log"), data = cdata2)
summary(attrSim)
coef(attrSim)
```

The output here again contains a single coefficient per region (minus the reference region), but here indicates how much more (or less) likely it is to be a *destination* for commuters. 


Now let's get the flow for each O/D pair, and compare to the observations:

```{r}
cdata2$attrsimFitted <- round(fitted(attrSim),0)
```

Create an O/D matrix, with the marginal sums of total outflow (rows) and inflows (columns):
```{r results='hide'}
cdata2mat4 <- dcast(cdata2, Orig ~ Dest, sum, value.var = "attrsimFitted", margins=c("Orig", "Dest"))
cdata2mat4
```

No redo the comparison of the inflow values:

```{r}
totinflowObs = as.numeric(cdata2mat[cdata2mat$Orig=="(all)", -1])
totinflowPred = as.numeric(cdata2mat4[cdata2mat4$Orig=="(all)", -1])
plot(totinflowObs, totinflowPred, log="xy",
     xlab="Observed inflow", ylab="Predicted inflow")
abline(0,1)
```

And calculate the RMSE, which shows that including the destination constraint has a substantial improvement on the model: 

```{r}
sqrt(mean((cdata2$Total-cdata2$attrsimFitted)^2))
```

### Doubly constrained model

Of course, the next obvious step is to combine these two constraints in the doubly constrained model:

\[
T_{ij} = A_i O_i D_j B_j d_{ij}^\beta 
\]

Estimating this by hand is somewhat difficult, as the value of $A_i$ depends on $B_j$ and vice versa. It therefore requires a set of iterations to converge on the best estimates. In our case, however, there is no need to do this, and we can just run Poisson regression with a full set of the full set of factors (origins and destinations). 

```{r results='hide'}
doubSim <- glm(Total ~ Orig+Dest+log(dist), na.action = na.exclude, family = poisson(link = "log"), data = cdata2)
#let's have a look at it's summary...
summary(doubSim)
```

As before, get the estimated flow for each O/D pair:

```{r}
cdata2$doubsimFitted <- round(fitted(doubSim),0)
```

Then create an O/D matrix, with the marginal sums of total outflow (rows) and inflows (columns):
```{r results='hide'}
cdata2mat5 <- dcast(cdata2, Orig ~ Dest, sum, value.var = "doubsimFitted", margins=c("Orig", "Dest"))
cdata2mat5[1:10,1:10]
```

The double constraint results in a marked improvement in the model fit, and a drop in the RMSE.

```{r}
sqrt(mean((cdata2$Total-cdata2$doubsimFitted)^2))
```

## Outro

The work here is designed to show that it is relatively easy to build a range of spatial interaction models, from arbitrary hand-picked coefficients, to more complex, constrained models. Placing these in the framework of Poisson regression allows a fairly straightforward way to build the models, but also offers the possibility of extending these model by including more variables that may be relevant to attracting or emitting flows between locations. 

It should also be possible to see from this, that the same approach could be applied to more specific flows (the London commuting file contains information about different modes for example), or extended to include different 'cost' information to replace the distance variable. Other improvements include modeling intra-regional flows as well as flows between regions, and the use of more complex models (e.g. negative binomial) to deal with some of the limitations of the Poisson model. 

## Exercise