---
title: "GEOG 5160/6160 Lab 10 Recurrent neural networks"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "April 1, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
```

## Introduction

**THIS IS AN OPTIONAL LAB**

In this lab, we'll look at how to build a recurrent neural network using the Keras package. The example is modified from an example in Python from Jason Brownlee's [blog on machine learning][mlmID]. 

You will need the file called *pollution.csv*, which should be available from Canvas with this document. Download this to your `datafiles` folder (extract any zip files). Make a new folder for today's class called `lab10` (if you haven't already done so). 

### R users

Start RStudio and set the working directory to this directory (This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder). 

The **mlr3** developers are currently working on an interface to Keras models, which would allow you to use the same approach that we have used in previous labs to build models. Unfortunately, this is still in early development, so for this lab, we'll use the R **keras** package. 

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notebook for this lab (*GEOG_5160_6160_lab10.ipynb*). Make a new folder for the lab (`lab10`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). 

Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 

## Recurrent neural networks

### Data processing

Let's start by loading the libraries we'll need for this lab:

```{r message=FALSE}
library(dplyr)
library(tidyr)
library(keras)
library(ggplot2)
```

Next, we'll load the dataset and take a quick look at the first few lines:

```{r}
dat <- read.csv("../datafiles/pollution.csv")
head(dat)
```
The data are hourly pollution and weather measurements from the US embassy in Beijing for a period of 5 years. The goal will be to build a model that can do a step ahead prediction, i.e. can predict the PM2.5 concentration at any hour $t$ from the values of all the variables at previous hour $t-1$, *including the previous value of PM2.5*.

```{r}
names(dat)
```

Let's make a couple of plots to see what the variables look like for the first year:

```{r}
dat_2010 <- dat %>%filter(date < 2011)
plot(dat_2010$pollution, type = 'l', ylab = 'PM2.5')
plot(dat_2010$temp, type = 'l', ylab = 'PM2.5')
plot(dat_2010$rain, type = 'l', ylab = 'PM2.5')
plot(dat_2010$press, type = 'l', ylab = 'PM2.5')
```

Before we can build a model there are a few things we'll need to do to process these data:

- Encode the wind direction variable
- Normalize all the features
- Create lagged variables (i.e. the values of each variable one hour ahead of the target time)
- Create a training and testing set

```{r}
table(dat$wnd_dir)
```

We can encode these to integer values by transforming them to a factor (a categorical variable), then to a numeric value:

```{r}
dat$wnd_dir <- as.numeric(factor(dat$wnd_dir, levels = c("NE", "NW", "SE", "cv")))
```

We'll normalize the data to 0-1 range for training. First, we'll create a short R function to carry out the min-max normalization:

```{r}
normalize <- function(x) {
  x <- c(x)
  (x - min(x)) / (max(x) - min(x))
}
```

And now, we'll use **dplyr**'s `mutate` function to replace the original values with the transformed values. Normally, we'd only normalize the features, but as we are going to use the PM2.5 value from the previous time step as an additional feature, we need to normalize this as well. As this means we will be modeling the normalized PM2.5 values, we'll store the minimum and maximum. This will allow us to back convert any predictions we make to parts per billion. 

```{r}
scaled <- dat %>%
  mutate(pollution = normalize(pollution),
         dew = normalize(dew),
         temp = normalize(temp),
         press = normalize(press),
         wnd_dir = normalize(wnd_dir),
         wnd_spd = normalize(wnd_spd),
         snow = normalize(snow),
         rain = normalize(rain))

pollution_min <- min(dat$pollution)
pollution_max <- max(dat$pollution)
```

As we want to model pollution using the previous hour's features, we need to create a lagged version of each one. So for any given observation, we need the value of each feature from the previous hour. There's a couple of ways to do this, but we'll use a simple one, the `lag()` function from the **tidyr** package. In the following code, we create new `lag_1` features, remove any row with a missing value and finally keep only the lagged features and the pollution:

```{r}
lagged <- scaled %>%
  mutate(lag_1_pollution = lag(pollution, 1),
         lag_1_dew = lag(dew, 1),
         lag_1_temp = lag(temp, 1),
         lag_1_press = lag(press, 1),
         lag_1_wnd_dir = lag(wnd_dir, 1),
         lag_1_wnd_spd = lag(wnd_spd, 1),
         lag_1_snow = lag(snow, 1),
         lag_1_rain = lag(rain, 1)
  ) %>%
  drop_na() %>%
  select(pollution, contains("lag_1"))
head(lagged)
```

If you look at the first few rows, you should see that the target (pollution) is now linked to the previous hour's pollution value (and the other features).

The last thing we need to do is to create out training and testing sets. We'll keep the first two years for training, and the other three years for testing. Some things to note here:

- We generate an index for the samples that is the number of days in a year * the number of hours. Technically it should be one less than this as we will have lost the first row once we lagged the features
- The data are converted to a matrix form, then reshaped to a 3D array. As we are going to be fitting a recurrent neural network, we need to make sure the input tensor has the correct shape. RNNs use rank 3 tensors with a shape given by (`sample`, `timesteps`, `features`). Here, we only have a single time step, because we are only using the preceding hour as input. Ultimately we'll rely on the RNNs hidden state to 'remember' earlier information

```{r}
n_obs = 365 * 24 * 2## SHould be -3

X_train <- data.matrix(lagged[1:n_obs, -1])
X_train <- array(X_train, dim = c(n_obs, 1, 8))
y_train <- lagged$pollution[1:n_obs]
```

Now let's do the same for the test set:

```{r}
X_test <- data.matrix(lagged[n_obs:nrow(lagged), -1])
X_test <- array(X_test, dim = c(nrow(X_test), 1, 8))
y_test <- lagged$pollution[n_obs:nrow(lagged)]
```

### Model

Now let's put together a model. Before trying a recurrent NN, let's just build a standard dense network. This will allow us to test if the additional complexity of the RNN is worth it or not. 

As before, we build our model as a sequential series of layers. We have just a single hidden layer with 50 nodes and we'll use a default liner activation function. This feeds into a output layer with a single node to match the observed, non-lagged PM2.5 values.

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(input_shape = dim(X_train)[2:3], units = 50)
model %>%
  layer_dense(units = 1) # output

summary(model)
```

Now let's compile this by adding an optimizer, loss function and performance metric. We'll use a different optimizer called Adam, which works well with large, noisy datasets (and is particularly efficient for deep learning).

```{r}
model %>% compile(
  optimizer = optimizer_adam(), 
  loss = "mse", 
  metrics = c("mae")
)
```

And now we can train the network. We'll use a batch size of 64 as we have quite a lot of observations to work with, and run this for 50 epochs. We'll train on the first year of training data, and validate on the second.

```{r}
train_index <- 1:365*24
X_train_partial <- X_train[train_index, , ]
X_train_partial <- array(X_train_partial, dim = c(nrow(X_train_partial), 1, 8))
y_train_partial <- y_train[train_index]

X_val <- X_train[-train_index, , ]
X_val <- array(X_val, dim = c(nrow(X_val), 1, 8))
y_val <- y_train[-train_index]
```

```{r eval=FALSE}
history <- model %>% fit(
  x = X_train_partial, 
  y = y_train_partial, 
  batch_size = 64, 
  epochs = 50, 
  validation_data = list(X_val, y_val),
  shuffle = FALSE) # 
```

```{r eval=TRUE, echo=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train, 
  batch_size = 64, 
  epochs = 50, 
  validation_data = list(X_test, y_test),
  verbose = 0,
  shuffle = FALSE) # 
```

```{r}
plot(history)
```

The validation shows no sign of overfitting, but little improvement beyond about 35 epochs. Let's now build the full training model and evaluate:

```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(input_shape = dim(X_train)[2:3], units = 50)
model %>%
  layer_dense(units = 1) # output

model %>% compile(
  optimizer = optimizer_adam(), 
  loss = "mse", 
  metrics = c("mae")
)
```

```{r eval=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train, 
  batch_size = 64, 
  epochs = 35, 
  shuffle = FALSE) # 
```

```{r eval=TRUE, echo=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train, 
  batch_size = 64, 
  epochs = 35, 
  verbose = 0,
  shuffle = FALSE) # 
```

```{r}
plot(history)
```

We now evaluate on the test set

```{r}
results <- model %>% keras::evaluate(X_test, y_test, verbose = 1)
results
```

As we had to normalize the PM2.5 values to use them as a lagged variable, the MAE is on this normalized scale. To get a better assessment of the model, we can calculate an error value by back-transforming the prediction. A simple function to back transform from a minmax scaling is:

```{r}
back_transform <- function(x, xmin = 0, xmax = 1) {
  return((x * (xmax - xmin)) + xmin)
}
```

To get a back transformed MAE, we need to 

- Predict for the test set on the normalized scale
- Back transform these predictions
- Back transform the matching test labels (PM2.5 values)
- Calculate the mean absolute error

```{r}
y_test_pred_norm <- model %>% predict(X_test)
y_test_pred <- back_transform(y_test_pred_norm, pollution_min, pollution_max)
y_obs <- back_transform(y_test, pollution_min, pollution_max)

mean(abs(y_test_pred - y_obs))
```

## Long short term memory

Now let's build a recurrent neural network. We'll use a very similar structure to the previous model, but replace the hidden layer with a LSTM layer. This includes both the hidden state component and the carry track, allowing the model to propagate information over a large number of time steps. The input and output remain the same

```{r}
model <- keras_model_sequential()
model %>%
  layer_lstm(input_shape = dim(X_train)[2:3], units = 50)
model %>%
  layer_dense(units = 1) # output
summary(model)

model %>% compile(
  optimizer = optimizer_adam(), 
  loss = "mse", 
  metrics = c("mae")
)
```

Now let's train and validate. As this is a more complex algorithm, we'll train for 100 epochs. 

```{r eval=FALSE}
history <- model %>% fit(
  x = X_train_partial, 
  y = y_train_partial, 
  batch_size = 64, 
  epochs = 100, 
  validation_data = list(X_test, y_test),
  shuffle = FALSE) # 
```

```{r eval=TRUE, echo=FALSE}
history <- model %>% fit(
  x = X_train_partial, 
  y = y_train_partial, 
  batch_size = 64, 
  epochs = 100, 
  validation_data = list(X_test, y_test),
  shuffle = FALSE) # 
```

```{r}
plot(history)
```

The results suggest that the model converges at around 50 epochs. Let's train a final model and evaluate.

```{r}
model <- keras_model_sequential()
model %>%
  layer_lstm(input_shape = dim(X_train)[2:3], units = 50)
model %>%
  layer_dense(units = 1) # output

model %>% compile(
  optimizer = optimizer_adam(), 
  loss = "mse", 
  metrics = c("mae")
)
```

```{r eval=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train, 
  batch_size = 64, 
  epochs = 50, 
  shuffle = FALSE) # 
```

```{r eval=TRUE, echo=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train, 
  batch_size = 64, 
  epochs = 100, 
  verbose = 0,
  shuffle = FALSE) # 
```

We now evaluate on the test set

```{r}
results <- model %>% keras::evaluate(X_test, y_test, verbose = 1)
results
```

And let's calculate the error after back-transformation

```{r}
y_test_pred_norm <- model %>% predict(X_test)
y_test_pred <- back_transform(y_test_pred_norm, pollution_min, pollution_max)
y_obs <- back_transform(y_test, pollution_min, pollution_max)

mean(abs(y_test_pred - y_obs))
```

This gives a reduction in the MAE of about 2-3 ppb, a decrease of around 10% relative to the dense network model. 

## Appendix

### Beijing pollution dataset: *pollution.csv*

| Column header | Variable |
| --- | --- |
| date | POSIX date (y-m-d h:m:s) |
| pollution | PM2.5 concentration (ppb) |
| dew | Dew point |
| temp | Temperature (C) |
| press | Air pressure (hPa) |
| wnd_dir | Wind direction (categorical) |
| wnd_spd | m s-1 |
| rain | mm |


[mlmID]: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/