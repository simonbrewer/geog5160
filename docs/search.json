[
  {
    "objectID": "GEOG_5160_6160_lab01.html",
    "href": "GEOG_5160_6160_lab01.html",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "",
    "text": "In this lab, we will introduce the basic functionality of R, together with some simple plotting functions. We will be using the following files for these examples:\n\nA dataset of morphological measurements of three species of penguin from the file penguins.csv\n\nMore information about this dataset can be found here: https://allisonhorst.github.io/palmerpenguins/index.html"
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#a-quick-note-on-formatting",
    "href": "GEOG_5160_6160_lab01.html#a-quick-note-on-formatting",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "A quick note on formatting",
    "text": "A quick note on formatting\nIn this and subsequent labs, code that can be entered into R will be high-lit, e.g.:\n\nplot(x, y)\n\nAnd R output will be formatted with ## at the start of the line. File names will be given in italics and will be available in the ‘Datafiles’ directory on the course Canvas site or through links on the class webpage."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#getting-started",
    "href": "GEOG_5160_6160_lab01.html#getting-started",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Getting started",
    "text": "Getting started\n\nRStudio layout\nThe R Studio interface consists of several windows. Start R Studio from the ‘Start’ menu under Windows, and the following window should appear:\n\n\n\nRStudio Interface\n\n\n\nBottom left: console window (also called command window). Here you can type simple commands after the &gt; prompt and R will then execute your command. This is the most important window, because this is where R actually does stuff.\nTop left: editor window (also called script window). Collections of commands (scripts) can be edited and saved. When you don’t see this window, you can open it with [File \\(&gt;\\) New \\(&gt;\\) R script]. Just typing a command in the editor window is not enough, it has to get into the command window before R executes the command. If you want to run a line from the script window (or the whole script), copy and paste it to the console. Alternatively, you can click [Run] or press CTRL+ENTER to send it to the command window.\nTop right: workspace / history window. In the workspace window you can see which data and values R has in its memory. You can view and edit the values by clicking on them. The history window shows what has been typed before.\nBottom right: Files / plots / packages / help window. Here you can open files, view plots (also previous plots), install and load packages or use the help function. You can change the size of the windows by dragging the grey bars between the windows.\n\n\n\nWorking with R\nMuch of your time spent with R will involve typing commands in at the console, and R Studio has some help with this.\n\nThe up/down arrow keys allow you to cycle back through commands that have been previously entered. You can then modify or reuse these as necessary. The commands can also be accessed through the `History’ tab in the top-right panel\nThe console has ‘tab-completion’, which allows you to enter the first few characters of a string or function name, press ‘Tab’, and R Studio will bring up a list of possible options that match the string you entered. Try typing pri and pressing ‘Tab’ - you should see print as part of the list, and you can click on this, or scroll down to use it from the list.\n\n\n\nWorkspace\nR has a workspace where variables and data are stored as you use it. This is held in the memory of the computer, so if you are working from a file, you will need to read it in to the R workspace, and then work on the values held in memory. This means that you only access files to read in or write out data, the rest of the time you are working on a copy in the workspace.\n\n\nWorking directory\nR defines the working directory as the folder in which it is currently working. When you ask R to open a certain file, it will look in the working directory for this file, and when you tell R to save a data file or plot, it will save it in the working directory.\nFor this class, the labs will assume that you have your files organized according to the following structure:\n+-- geog6160\n|   +-- datafiles\n|   +-- lab01\n|   +-- lab02\n|   +-- lab03\n...\n|   +-- lab15\nTo do this, go to your Documents folder, and create a new folder called geog6160. In this now create two new folders, one called datafiles (where we will store all the data used across all labs and one called lab01, which we will use for today’s lab.\nOnce you have created these folders, we need to change R’s working directory so that it is pointing to lab01. The easiest way to do this is by going to the [Session] menu in RStudio, then [Change working directory]. This will open a file browser that you can use to browse through your computer and find the folder. (If you are using the base version of R, go to [File] \\(&gt;\\) [Change dir…] in Windows, or [Misc] \\(&gt;\\) [Change Working Directory] in Mac.)\nYou can also change the working directory manually using the setwd() function in the console. To do this, you may need to know the full path to the folder on your computer. If you followed the instructions given above, this should be:\n\nOn a Windows system: C:/Users/username/Documents/geog6160/lab01\nOn a Mac OSX system: /Users/username/Documents/geog6160/lab01\n\nWhere username is your name on the computer. You can also find this path by\n\nOn a Windows system: - Use the File Explorer to select the folderlab01`\nRight-click the folder and select ‘Properties’\nIn the pop-up window, the path will be listed under ‘Location’\nHighlight this, and copy the path\nOn a Mac OSX system:\nUse the Finder app to select the folder lab01\nGo to the “View” menu then “Show Path Bar”. THis will make the full path appear at the bottom of the Finder window\nRight-click the Path Bar file name\nSelect “Copy as Pathname” You can now run the setwd() command. Go to the console window in RStudio and enter the following code:\n\n\nsetwd(\"\")\n\nAnd paste your directory. The code should look something like this:\n\nsetwd(\"C:/Users/username/Documents/geog6160/lab01\")\n\nNote that the slashes are forward slashes and don’t forget the quotations. R is case sensitive, so make sure you write capitals where necessary. To check that you have correctly changed directory, enter the following command, which will show you the current working directory:\n\ngetwd()\n\nYou can also use relative paths. If your current working directory is geog6160 and you want to change to lab01, enter the following code (where the ./ changes the directory to a level higher than the current one).\n\nsetwd(\"./lab01\")\n\nIf your current working directory is lab01 and you want to change to geog6160, enter the following code (where the ../ changes the directory to a level below the current one).\n\nsetwd(\"../\")\n\nFinally, if your current working directory is lab01 and you want to change to lab02, which is at the same level as the current directory, enter the following code (where the ../ changes the directory to a level below the current one, and lab02 then moves to the level above that).\n\nsetwd(\"../lab02\")\n\nBefore proceeding with the rest of today’s lab, make sure to change your working directory back to lab01.\nIf this all seems a little foreign to you, don’t worry - there will be plenty of opportunities to practice this over the semester. Understanding the directory structure is very important in being able to manage your files both for this class and any analysis you do.\n\n\nUsing the console\nIn the console, the ‘&gt;’ is the prompt, and your commands will be entered here. Click on the console window, then enter the following:\n\n2+2\n\n[1] 4\n\n\nAnd press ‘Enter’, and R will tell you, not too surprisingly, that 2+2=4. The spacing is not relevant, you could equally enter 2 + 2 or 2+ 2 and get the same result. The [1] before the output is a vector index. It refers to the first value in the vector (here a vector of length 1). We’ll be using this later.\nWe can equally use standard math functions, for example, to take the natural log or square root of 2:\n\nlog(2)\n\n[1] 0.6931472\n\nsqrt(2)\n\n[1] 1.414214"
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#factors",
    "href": "GEOG_5160_6160_lab01.html#factors",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Factors",
    "text": "Factors\nThe vector of species names (penguin$species) is a set of character strings. R has another data type, a factor used to represent groups within your data. With a factor, R automatically assumes that every observation with the same name or level belongs to a single group, which can greatly facilitate comparing values between groups\nYou can convert a vector of character strings to a factor with the as.factor() function. The following code replaces the original species string with factor:\n\npenguin$species &lt;- as.factor(penguin$species)\n\nNow let’s recheck the class:\n\nclass(penguin$species)\n\n[1] \"factor\"\n\n\nIf we now check the first 10 values, you will see some additional information showing the levels of the factor, i.e. the individual groups.\n\npenguin$species[1:10]\n\n [1] Adelie Adelie Adelie Adelie Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\nR automatically sets the levels in alphabetical order, irrespective of the order in the vector. The first level is considered to be the reference level, which has some uses in regression model as we will see later. You can change the order of the levels using the factor() function. The following sets the species Gentoo as the reference:\n\nfactor(penguin$species, levels = c(\"Gentoo\", \"Chinstrap\", \"Adelie\"))\n\nWe will look further at how factors work in a later lab."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#univariate-statistics",
    "href": "GEOG_5160_6160_lab01.html#univariate-statistics",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Univariate statistics",
    "text": "Univariate statistics\nNow create two new vectors in R containing the list of bill lengths and the list of species names. Note the use of the assignment operator &lt;-. You can also use the equal sign (=) here and elsewhere in these examples.\n\nbl &lt;- penguin$bill_length_mm\nsp &lt;- penguin$species\n\nR has a large number of inbuilt functions. This section is designed to simply introduce you to the some basic functions for describing data. We’ll start by simply calculating the mean of the bill length values\n\nmean(bl)\n\n[1] NA\n\n\nThis returns the value NA, rather than a mean length. So what went wrong? In the original set of data, there are some missing values, also denoted by NA.\n\nbl[1:15]\n\n [1] 39.1 39.5 40.3   NA 36.7 39.3 38.9 39.2 34.1 42.0 37.8 37.8 41.1 38.6 34.6\n\n\nR’s default for most functions is to not calculate values when there are missing observations. This is really to alert you to the fact that the data are incomplete, and the value you would obtain might be biased. You can overrule this by adding the argument na.rm=TRUE to the following functions. This removes NAs and calculates the value with whatever is leftover.\nFunctions to describe the central tendency:\n\nmean(bl, na.rm = TRUE)\n\n[1] 43.92193\n\nmedian(bl, na.rm = TRUE)\n\n[1] 44.45\n\n\nFunctions to describe the dispersion (output not shown):\n\nsd(bl, na.rm = TRUE)\nvar(bl, na.rm = TRUE)\nmin(bl, na.rm = TRUE)\nmax(bl, na.rm = TRUE)\nquantile(bl, na.rm = TRUE)\n\nNote that quantile() takes a parameter that allows you to choose the quantile to be calculated, e.g. quantile(bl, c(0.1,0.9), na.rm = TRUE), will calculate the 10th and 90th percentile. Try adapting this to calculate the 25th and 75th percentile.\nSome other useful functions:\n\nsum(bl, na.rm = TRUE)\nsummary(bl)\n\nNote that we do not need to tell R to exclude NAs for the summary() function. This provides a set of summary statistics and lets you know how many values are missing.\nSome specific functions for categorical data\n\nlevels(sp)\ntable(sp)\n\nAs R is object oriented, functions will adapt to different data types\n\nsummary(bl) ## Summary of numeric vector\nsummary(sp) ## Summary of categorical vector\nsummary(penguin) ## Summary of data frame\n\n\nThe group_by() function\nThe group_by function allows you to apply a function to different subsets of data, without having to first split it into the subsets. On it’s own, this function simply creates the groups (here by species):\n\npenguin |&gt;\n  group_by(species)\n\n# A tibble: 344 × 8\n# Groups:   species [3]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nWe can use the pipe operator to chain this to the summarize function to calculate summary values:\n\npenguin |&gt;\n  group_by(species) |&gt;\n  summarize(mean_bill_length = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species   mean_bill_length\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie                38.8\n2 Chinstrap             48.8\n3 Gentoo                47.5\n\n\nNote that we can add more than one grouping, so to see the bill length by sex and species:\n\npenguin |&gt;\n  group_by(sex, species) |&gt;\n  summarize(mean_bill_length = mean(bill_length_mm, na.rm = TRUE))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 3\n# Groups:   sex [3]\n  sex    species   mean_bill_length\n  &lt;chr&gt;  &lt;fct&gt;                &lt;dbl&gt;\n1 female Adelie                37.3\n2 female Chinstrap             46.6\n3 female Gentoo                45.6\n4 male   Adelie                40.4\n5 male   Chinstrap             51.1\n6 male   Gentoo                49.5\n7 &lt;NA&gt;   Adelie                37.8\n8 &lt;NA&gt;   Gentoo                45.6\n\n\nRepeat this and calculate the standard deviation per species."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#index-plots",
    "href": "GEOG_5160_6160_lab01.html#index-plots",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Index plots",
    "text": "Index plots\nThe simplest type of plot is an index plot, which simply plots values in the order they are recorded in the input vector. These are useful for examining the basic data structure and identifying errors and outliers. plot is a generic plotting command and will adapt to different data types. The parameter type='p' gives the plot type, here using points. Other options are 'l' for lines, 'h' for histogram lines, 's' for a stepped plot and 'b' for both line and points. See help(plot) for more options and other parameters.\n\nplot(penguin$bill_length_mm, type = 'p')\n\n\n\n\n\n\n\n\nAs we have only asked to plot one variable, this is represented on the y-axis. The x-axis gives the index of the observation, in this case from 1 to 350."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#summary-plots",
    "href": "GEOG_5160_6160_lab01.html#summary-plots",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Summary plots",
    "text": "Summary plots\nSummary plots attempt to describe the distribution of the data, giving some ideas about which values are most common and which are most rare. Histograms are commonly used for this method, values are ‘binned’ into a set of classes, and the histogram represents the frequency of occurrences in that bin. The size of the bins can be set with binwidth:\n\nggplot(penguin, aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 1) + \n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nAs before, we can use colors to define different groups of data. We’ll use these to fill the histogram bars:\n\nggplot(penguin, aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(binwidth = 1) + \n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nBy default, these are show as stacked histogram bars (i.e. the height is the cumulative number of all species in a certain bin). We can change this to overlapping bins as follows (we’ll also move the legend to underneath the plot):\n\nggplot(penguin, aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(binwidth = 1, position = 'identity', alpha = 0.6) + \n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#bivariate-plots",
    "href": "GEOG_5160_6160_lab01.html#bivariate-plots",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Bivariate plots",
    "text": "Bivariate plots\nMore usefully, we can include a variable on both axis to show the relationship between them. We use the plot() function again, but now we give it two variables (x and y).\n\nplot(penguin$bill_length_mm, penguin$bill_depth_mm)\n\n\n\n\n\n\n\n\nThe tidyverse equivalent to this is ggplot, which we will mainly use during this class as it provides a user-friendly interface for plotting. ggplot uses a series of functions to build up a plot. In it’s simplest form, we can create the previous plot as follows:\n\nggplot(penguin, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSome things to note:\n\nWe define an aesthetic (aes()) which identifies the variables to be used on the x and y axes\nWe add a geometry to define the type of plot we want to make (a point or scatter plot)\n\nAs we know that these values come from three difference species, we can use this knowledge to add extra information to the plot, by adding a col argument to the aestehtic (you can also display different symbols using the shape argument):\n\nggplot(penguin, aes(x = bill_length_mm, \n                    y = bill_depth_mm,\n                    col = species)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLet’s clean up this plot a little by specifying the axis labels and a title. We’ll also add a theme to remove the grey background. Note that we do this by adding (+) different plot elements sequentially:\n\nggplot(penguin, aes(x = bill_length_mm, \n                    y = bill_depth_mm,\n                    col = species)) +\n  geom_point() +\n  scale_x_continuous(\"Bill length (mm)\") +\n  scale_y_continuous(\"Bill length (mm)\") +\n  ggtitle(\"Penguins, penguins, penguins\") +\n  theme_bw()\n\nAn alternative way to look at the association between factors and a variable is, again, to use boxplots. Note that this code uses a tilde (\\(\\sim\\)) between the variable and the set of factors. The tilde is often used to define dependency between two variables, and we will return to this again during the modeling part of this class.\n\nggplot(penguin, aes(x = species, y = bill_length_mm)) +\n  geom_boxplot() +\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nAs the boxplot does not automatically label the y-axis, we add this with the ylab parameter. See help(plot) and help(par) for a complete list of the plotting parameters."
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#graphic-output",
    "href": "GEOG_5160_6160_lab01.html#graphic-output",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Graphic output",
    "text": "Graphic output\nBy default, R plots graphics to the screen, but has the ability to save figures in most of the standard graphic formats. For a ggplot object, the easiest way is to save the output as an object, then use ggsave() to export:\n\np1 &lt;- ggplot(penguin, aes(x = bill_length_mm, \n                    y = bill_depth_mm,\n                    col = species)) +\n  facet_wrap(~island) +\n  geom_point() +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\nggsave(\"test.png\", p1)\n\nThis will also save into the majority of standard image formats simply by changing the file extension in the ggsave() command.\nAlternatively, you can copy-paste directly into another file by going to [Export] -&gt; [Copy to clipboard…] in R Studio’s plotting window."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html",
    "href": "GEOG_5160_6160_Installation.html",
    "title": "GEOG 5160 6160 Installation",
    "section": "",
    "text": "This document walks through installing and setting up R and Python. You should only need one of these for these class - I’d suggest using whichever you are most comfortable with. If you have any questions about which to choose, please ask me."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#getting-started",
    "href": "GEOG_5160_6160_Installation.html#getting-started",
    "title": "GEOG 5160 6160 Installation",
    "section": "Getting started",
    "text": "Getting started\nFor this class, the labs will assume that you have your files organized according to the following structure:\n+-- geog5160\n|   +-- datafiles\n|   +-- lab01\n|   +-- lab02\n|   +-- lab03\n...\n|   +-- lab11\nTo do this, go to your Documents folder, and create a new folder called geog6000. In this now create two new folders, one called datafiles (where we will store all the data used across all labs and one called lab01, which we will use for today’s lab.\nNow download the penguins.csv file from Canvas and move this to the datafiles subfolder. Download the notebook and move this to your lab01 folder."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#starting-python",
    "href": "GEOG_5160_6160_Installation.html#starting-python",
    "title": "GEOG 5160 6160 Installation",
    "section": "Starting Python",
    "text": "Starting Python\n\nWindows\nIn windows, you can run the conda version of Python by going to the [Start Menu] &gt; [Anaconda (64-bit)] &gt; [Anaconda prompt]. This will launch a new terminal window, and if conda has been correctly installed you should see the following prompt:\n(base) C:\\Users\\username&gt;\n\n\nMac OSX\nOpen any terminal. If conda has been correctly installed, you should see the following prompt\n(base) username:\n\n\nLinux\nOpen any terminal. If conda has been correctly installed, you should see the following prompt\n(base) username:"
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#changing-directories",
    "href": "GEOG_5160_6160_Installation.html#changing-directories",
    "title": "GEOG 5160 6160 Installation",
    "section": "Changing directories",
    "text": "Changing directories\nIn order to access the files, you’ll need to change directory in your terminal to the lab01 folder you made earlier using the cd command. If you know the full path to the folder, then simply enter:\ncd /full/path/to/folder\nOnce you’ve successfully changed directories, check that the notebook file is in your folder you have changed to by type ls to list the files.\n\nWindows\nFinding this path can be challenging on Windows, but the easiest was is to find the folder in Windows Explorer, right click on it and go to ‘Properties’. If you’ve created this in your Documents folder, the path will be something like C:/Users/username/Documents/geog5160/lab01.\n\n\nMax OSX\nOn a Mac, you can find the path to a folder in the Finder window, by clicking on [View] &gt; [Show Path Bar]. If you’ve created this in your Documents folder, the path will be something like /Users/username/Documents/geog5160/lab01.\n\n\nLinux\nOn a Linux system, the folder will be somewhere in your home directory. If you’ve created this in your Documents folder, the path will be something like /home/username/Documents/geog5160/lab01."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#conda-environments",
    "href": "GEOG_5160_6160_Installation.html#conda-environments",
    "title": "GEOG 5160 6160 Installation",
    "section": "conda environments",
    "text": "conda environments\nOne of the key features of conda is the use of environments. Each environment represents an isolated Python session, and so can be set up differently. This can help to avoid conflicts between add-on packages; you can create an environment for a particular type of analysis or data processing and only install the packages that you actually need. If conflicts arise, it is then easy to delete the environment and try to set it up again, all without impacting the base install of Python. The default environment is base (hence the (base) shown before the prompt). In general, you should only install packages here if you are likely to use them a lot. Packages that are specific to a given project should only be installed in that environment.\nTo create a new environment, simply type\nconda create -n new-env\nPython may ask you about the installation location for this environment, just accept the default. The environment will be set up then made available for use. Note that this only creates the environment, but you are still in the base environment. To change to new-env, type\nconda activate new-env\nAny you should see the prompt change from (base) to (new-env). If you want to see a list of all the available environments (the current one will be listed with a *):\nconda env list\nYou can stop your current environment return to the base environment:\nconda deactivate\nAnd environments can be deleted as follows:\nconda env remove -n new-env\nNow create an environment for class work (called geog5160)\nconda create -n geog5160\nconda activate geog5160\n\nInstalling packages\nThe conda command can also be used to install packages. To demonstrate this, we’ll start by installing the Jupyter Noteback package. This provides an interactive framework for testing and running Python code through your browser. Go to your terminal and enter the following code:\nconda install -c conda-forge notebook\nAnd press [y] to install all the supplementary packages. This will install notebook from the conda-forge repository or channel. There are several channels that host these packages, and conda uses anaconda.com by default. If your package is hosted elsewhere you need to specify this with the -c argument. The best way to find out where a particular package is held is by googling conda install packagename and following the links. Once you run this command, conda will check your local environment against the remote one, and download the necessary files to install the notebook (this includes any dependencies). Once you see the following three lines, the installation is complete:\n  Preparing transaction: done\n  Verifying transaction: done\n  Executing transaction: done\nNow we’ll install the pandas package to help with data manipulation and the matplot and seaborn packages for making simple plots:\nconda install pandas\nconda install matplotlib\nconda install seaborn\nYou will likely be prompted to install some supplementary packages; just press [y] to continue and install these.\n\n\njupyter notebooks\nFor the labs in the class, the Python code will be provided as jupyter notebooks. These are interactive html pages, that contain code and the output from that code, and are a convenient way to keep your Python work organized and test new ideas. To open one of these, you first need to start the notebook server:\njupyter notebook\nThis will launch a new page in your web browser with a list of the files that are available in your current directory.\n\n\n\nnotebook Interface\n\n\nClicking on any of these will then open the notebook on a new browser page. Or you can open a new notebook by clicking on the [New] button in the top right. A notebook consists of a set of cells, blocks of code or text. Each cell has a set type: generally either code or markdown. Markdown cells are used for descriptive text, and can be formatted using basic markdown tags. Code cells are where you enter your Python code. Once you have entered your code, you can then click the [Run] button to run it, and any output will appear below the cell:\n\n\n\nCode cells\n\n\nAt the top of the page there are a set of menus and actions buttons. The arrows allow you to move through the available cells, and the [+] button adds a new cell below your current position. The [Cell] menu has a lot of options for changing the cell type and for running different parts of the notebook (e.g. run all the cells above your current position).\n\n\n\nMenu bar\n\n\nIf you’ve got this far, we’ll switch to the jupyter notebook for the remainder of this lab. If you haven’t used these notebooks before, it’s probably worth spending a little time working with a new one at some point to get used to navigating through them."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#installing-python",
    "href": "GEOG_5160_6160_Installation.html#installing-python",
    "title": "GEOG 5160 6160 Installation",
    "section": "Installing Python",
    "text": "Installing Python\nThere are two commonly used versions of Python: 2.7 and 3. Version 3 was released in 2008 and there has been a gradual transition to only using 3 (support for 2.x was stopped in 2020). In this class, we will only use Python 3. There are several packaged distributions of Python that come with additional utilities. I would recommend using miniconda (these labs were written using this). Instructions for installing miniconda can be found here. Make sure you install version 3.8 (or higher) and that you are installing the 64bit version."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#using-conda",
    "href": "GEOG_5160_6160_Installation.html#using-conda",
    "title": "GEOG 5160 6160 Installation",
    "section": "Using conda",
    "text": "Using conda\n\nWindows\nIn windows, you can run the conda version of Python by going to the [Start Menu] &gt; [Anaconda (64-bit)] &gt; [Anaconda prompt]. This will launch a new terminal window, and if conda has been correctly installed you should see the following prompt:\n(base) C:\\Users\\username&gt;\n\n\nMac OSX\nOpen any terminal. If conda has been correctly installed, you should see the following prompt\n(base) username:\n\n\nLinux\nOpen any terminal. If conda has been correctly installed, you should see the following prompt\n(base) username:"
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#installing-r",
    "href": "GEOG_5160_6160_Installation.html#installing-r",
    "title": "GEOG 5160 6160 Installation",
    "section": "Installing R",
    "text": "Installing R\nR is available through the web page of The Comprehensive R Archive Network. At the top of this webpage, you will find three three links for downloading R depending on your operating system: Windows, Mac, or Linux.\n\nWindows\nTo install R on Windows, click the “Download R for Windows” link, then click “base”. On the next page, the first link should allow you to download the latest version of R. Run the program that this downloads and step through the installation wizard that appears. The wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine.\n\n\nMac\nTo install R on a Mac, click the “Download R for Mac” link, then download the package link (e.g. R-4.4.0.*.pkg). You’ll see a choice of two package. The first (arm64) is for the newer Macs with the M1/M2/M3 chips and the second (x86_64) is for the older intel-based Macs. If you’re not sure which one to choose go to the Apple menu (top left) and click on About This Mac to see your chip. Once this has downloaded, click on the file to run the installer. There are some options during the installation, but I’d recommend using the defaults for these. If your computer requires a password before installing new programs, you’ll need it here.\n\n\nLinux\nR comes pre-installed on many Linux systems, but is often an older version, and you’ll likely want to update this. Click on the “Download R for Linux.”, which will take you to a new page giving options for some of the more widely used Linux distros: Debian, Redhat, SUSE, and Ubuntu. Clicking on any of these will take you to a new page with detailed installation instructions for that system. Note that you can also download the source code from the first page and compile it yourself. This is relatively straightforward on most Unix-type operating systems. ## RStudio\nWhile R provides the computational engine for all the programming and analysis that you will be doing, its user interface is fairly basic. There have been several attempts to create a better interface; RStudio is probably the most widely used of these. It is designed to provide some extra tools and help in using R, and provides easier access to help documentation. Also, the RStudio interface looks the same for Windows, Mac OS, and Linux, which makes writing these labs easier.\nYou can download RStudio for free from the RStudio website. Click on the ‘Download’ link under RStudio Desktop, and then choose the right download for your operating system. Once you’ve installed RStudio, you can open it like any other program on your computer—usually by clicking an icon on your desktop."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#r-packages",
    "href": "GEOG_5160_6160_Installation.html#r-packages",
    "title": "GEOG 5160 6160 Installation",
    "section": "R packages",
    "text": "R packages\nPackages that are installed are kept in the R library directory, usually buried away in the computer system files. However, even when installed, a package and its functions are not available until it has been loaded into the R workspace. Once this is done you can use it’s functions, as you would those in the base version.\nIf you close R, the packages are cleared from the workspace. Upon restarting R, you will need to reload any packages you were using. You do not need to reinstall the package, only reload it.\n\nInstalling packages\nPackages can be installed in two ways:\n\nUse the function install.packages() from the console with the name of the package you wish to install\nGo to the ‘Packages’ menu and select ‘Install packages..’\nIn RStudio, go to the ‘Tools’ menu and select ‘Install packages..’\n\n\n\nUsing packages\nThere are several ways to use a package once it is installed:\n\nThe command library() will load a package into the R workspace from the console (or from a script)\nIn the basic version of R, using the menu ‘Packages &gt; Load package’ brings up a window that allows you to load installed packages by double clicking on the name\nIn RStudio, the ‘Packages’ tab in the bottom right panel does the same, bringing up a list of available packages.\n\nFor these last options, clicking in the box next to a package name will load the package, and clicking on the package name will bring up the associated help page."
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html",
    "href": "GEOG_5160_6160_lab02.html",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "",
    "text": "In this lab, we will introduce the basics of machine learning in R. We will repeat the example shown in class in which a linear regression model was built to predict housing prices for a set of locations in California. Before starting the lab, you will need to set up a new folder for your working directory. Go to your geog5160 folder now and create a new folder for today’s class called lab06. Now use setwd() to make this folder your working directory.\nR has a large number of packages for individual machine learning algorithms, but also has a couple of packages that are designed to manage a machine learning workflow. These packages take care of setting up training and testing data, as well as evaluating the models. We will see in later labs that these can also be used to optimize the set up of the model. The package we will use is called mlr3, which is a new version of an older package and under active development. You will need to install this, as well as a set of extensions:\n\nmlr3viz\nmlr3learners\nmlr3tuning\n\nAs a reminder, packages can be installed in RStudio by going to the ‘Packages’ tab and clicking on the [Install] button, or from the menu [Tools]-&gt; [Install packages…]. You can also install these from the console window by typing\n\ninstall.packages(\"mlr3\")\n\nMore details about the mlr3 package and the associated project can be found here.\nYou should also install the following packages to help in managing data and visualizing your results. We will not need all of these today, but will later on.\n\ndplyr\nsf\nraster\ntmap\n\n\n\n\nUnderstand how to set up a basic linear model in R\nUse the mlr3 package to design a ML task, a learner and a resampling strategy for validation\nRun a simple prediction with the model\n\nIt is highly recommended to use scripts to store your R code - this will allow you to easily change and modify it and submit the exercise.\n\n\n\nFor today’s lab, we will use a data set of California house prices from the file housing.csv, which is available through Canvas. The data are taken from a paper by Kelley and Barry (“Sparse spatial autoregressions.” Statistics & Probability Letters 33.3 (1997): 291-297), and is a classic dataset used in machine learning examples. You should also download the zipped file ca.zip. This contains a shapefile of California’s border. Move both of these to your datafiles directory and unzip and extract the files from ca.zip.\nThe csv file contains the following columns most of which should be self-explanatory), with values for each California district taken from the 1990 census:\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms: total number of rooms in the district\ntotal_bedrooms: total number of bedrooms in the district\npopulation\nhouseholds: total number of households in the district\nmedian_income\nmedian_house_value\nocean_proximity: a categorical value giving the position of the district relative to the ocean\n\nThe goal will be to build a model that can predict the median house value based on the other variables (or features).\n\n\n\nStart by creating a working directory for today’s lab (e.g. called ‘lab06’), and move the housing.csv and ca.zip files there. Unzip ca.zip. Next start RStudio (or R), and change your working directory to this directory. Check that you are in the correct place by typing the following in the console window:\n\nlist.files(\"./datafiles\")\n\nAnd make sure that you see housing.csv listed. Now load the file into R:\n\nhousing &lt;- read.csv(\"./datafiles/housing.csv\")"
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#objectives",
    "href": "GEOG_5160_6160_lab02.html#objectives",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "",
    "text": "Understand how to set up a basic linear model in R\nUse the mlr3 package to design a ML task, a learner and a resampling strategy for validation\nRun a simple prediction with the model\n\nIt is highly recommended to use scripts to store your R code - this will allow you to easily change and modify it and submit the exercise."
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#data",
    "href": "GEOG_5160_6160_lab02.html#data",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "",
    "text": "For today’s lab, we will use a data set of California house prices from the file housing.csv, which is available through Canvas. The data are taken from a paper by Kelley and Barry (“Sparse spatial autoregressions.” Statistics & Probability Letters 33.3 (1997): 291-297), and is a classic dataset used in machine learning examples. You should also download the zipped file ca.zip. This contains a shapefile of California’s border. Move both of these to your datafiles directory and unzip and extract the files from ca.zip.\nThe csv file contains the following columns most of which should be self-explanatory), with values for each California district taken from the 1990 census:\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms: total number of rooms in the district\ntotal_bedrooms: total number of bedrooms in the district\npopulation\nhouseholds: total number of households in the district\nmedian_income\nmedian_house_value\nocean_proximity: a categorical value giving the position of the district relative to the ocean\n\nThe goal will be to build a model that can predict the median house value based on the other variables (or features)."
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#setting-up-your-project",
    "href": "GEOG_5160_6160_lab02.html#setting-up-your-project",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "",
    "text": "Start by creating a working directory for today’s lab (e.g. called ‘lab06’), and move the housing.csv and ca.zip files there. Unzip ca.zip. Next start RStudio (or R), and change your working directory to this directory. Check that you are in the correct place by typing the following in the console window:\n\nlist.files(\"./datafiles\")\n\nAnd make sure that you see housing.csv listed. Now load the file into R:\n\nhousing &lt;- read.csv(\"./datafiles/housing.csv\")"
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#data-visualization",
    "href": "GEOG_5160_6160_lab02.html#data-visualization",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Data visualization",
    "text": "Data visualization\nWe’ll make a few figures to visualize the data before starting. First, a histogram of median_house_value:\n\nhist(housing$median_house_value, \n     xlab = \"Median House Value\", main = \"CA Housing Data\")\n\n\n\n\n\n\n\n\nNote that the house value data is capped at $500,000. Next, the same for median_income:\n\nhist(housing$median_income, \n     xlab = \"Median Income\", main = \"CA Housing Data\")\n\n\n\n\n\n\n\n\nNow try making additional histograms for the other variables in the housing2 data frame. The R function names() will give you a list of the column names if you have forgotten.\nA few things to note here.\n\nThe median_house_value and housing_median_age are both clearly capped at an upper limit (about 50,000 and 50 respectively). We may want to remove observations at these values (or ideally find the correct values)\nThe data are on a variety of different scales\nSeveral of the variables are right-skewed, and may need transforming to improve our models\n\nThe variables total_rooms and total_bedrooms represent the total number of rooms per district, but as the number of households vary among districts, we can’t use them directly. Instead, we’ll use these to create the average number of rooms per house, and the ratio of bedrooms to rooms.\n\nhousing$avg_rooms = housing$total_rooms / housing$households\nhousing$bedroom_ratio = housing$total_bedrooms / housing$total_rooms\n\nNext, we’ll visualize some of the data, starting with a quick map of the median house values. For this we first convert to an sf or simple features object. (The crs parameter defines the coordinate reference system or projection using an EPSG numeric code (4326 = WGS 84). For more detail on these codes, go here.)\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.4.1\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nca_sf = st_read(\"./datafiles/ca/ca.shp\")\n\nReading layer `ca' from data source \n  `/Users/u0784726/Dropbox/Data/devtools/geog5160/datafiles/ca/ca.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.482 ymin: 32.52883 xmax: -114.1312 ymax: 42.0095\nGeodetic CRS:  GRS 1980(IUGG, 1980)\n\nst_crs(ca_sf) &lt;- 4326\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\nhousing &lt;- st_as_sf(housing, \n                    coords = c(\"longitude\", \"latitude\"),\n                    crs = 4326)\n\nYou can map these values using R’s plot() function as follows. First we map the geometry of the California border, then add symbols for the median_house_value variable.\n\nplot(st_geometry(ca_sf))\nplot(housing[\"median_house_value\"], add = TRUE, pch = 16, alpha = 0.25)\n\n\n\n\n\n\n\n\nAnd you should be able to clearly see the areas with high values around Los Angeles and the Bay Area.\n\nThe tmap package\nThe tmap package is an add-on to R that makes thematic maps, which are generally a little nicer looking than the default spplot() maps. This works by using a series of layers (not unlike making a GIS map). First, we specify a spatial object that we want to plot with tm_shape(), then add a geometry to this (e.g. tm_fill(), tm_symbols(), etc. Here we make a ‘bubble’ plot, with the size of the symbols representing the median_house_value:\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\ntm_shape(ca_sf) + tm_borders() +\n  tm_shape(housing) + tm_symbols(\"median_house_value\")\n\nLegend labels were too wide. Therefore, legend.text.size has been set to 0.61. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nThe legend is too narrow to place all symbol sizes.\n\n\n\n\n\n\n\n\n\nHere, we do the same, but specify a color palette to use shading rather than size to represent value. We also specify alpha to make the points a little transparent.\n\ntm_shape(ca_sf) + tm_borders() +\n  tm_shape(housing) + tm_symbols(\"median_house_value\", palette = \"Reds\", \n                                    alpha = 0.75, size = 0.2, border.lwd = NA)\n\nHere we remake the same map, but add a title and some background colors.\n\ntm_shape(ca_sf) + tm_borders() + tm_fill(col = \"lightyellow\") +\n  tm_shape(housing) + tm_symbols(\"median_house_value\", palette = \"Reds\", \n                                    alpha = 0.75, size = 0.2, border.lwd = NA) +\n  tm_layout(bg = \"grey85\", legend.bg.color = \"white\", \n            main.title = \"CA Housing Data\")\n\n\n\n\n\n\n\n\nFor our first model, we’re only going to use a subset of the variables representing building characteristics (average number of rooms, bedroom ratio and age). There are several steps here:\n\nThe geometry column contains the coordinates for each district, and this can be removed by setting it to NULL\nWe use filter to remove all districts where the value is above $500K. As the data is capped here, using these districts would bias our model at high values\nWe use the select() function to choose the columns that we want to include\n\n\nnames(housing)\n\n [1] \"housing_median_age\" \"total_rooms\"        \"total_bedrooms\"    \n [4] \"population\"         \"households\"         \"median_income\"     \n [7] \"median_house_value\" \"ocean_proximity\"    \"avg_rooms\"         \n[10] \"bedroom_ratio\"      \"geometry\"          \n\nhousing2 = housing %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  filter(median_house_value &lt;= 500000) %&gt;%\n  select(median_house_value, avg_rooms, bedroom_ratio, housing_median_age)\nnames(housing2)\n\n[1] \"median_house_value\" \"avg_rooms\"          \"bedroom_ratio\"     \n[4] \"housing_median_age\""
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#tasks",
    "href": "GEOG_5160_6160_lab02.html#tasks",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Tasks",
    "text": "Tasks\nmlr3 defines two basic tasks: regression (for continuous variables) and classification (for categorical or binary variable). We’ll use the first of these with the housing data, as the values are continuous. A regression task can be created using the TaskRegr() function. In this function, we specify\n\nA label to describe the task\nA backend defining the data source (here the housing2 data frame). Note that is quite flexible and can also include SQL databases and cloud data APIs\nA target defining the response variable (i.e. the thing we want to model)\n\n\ntask_housing = TaskRegr$new(id = \"housing\", backend = housing2, \n                            target = \"median_house_value\")\nprint(task_housing)\n\nTasks have a series of attributes that allow you to investigate the characteristics of the data:\n\n## Number of observations\ntask_housing$nrow\n## Number of features\ntask_housing$ncol\n# See all data\ntask_housing$data()\n# retrieve data for rows with ids 1, 51, and 101\ntask_housing$data(rows = c(1, 51, 101))\n## Names of features \ntask_housing$feature_names\n## Name of target variable\ntask_housing$target_names"
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#learners",
    "href": "GEOG_5160_6160_lab02.html#learners",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Learners",
    "text": "Learners\nThe base mlr3 package only comes with a few machine learning algorithms or learners. Many more are available through the mlr3learners package, so let’s load this now.\n\nlibrary(mlr3learners)\n\nWarning: package 'mlr3learners' was built under R version 4.4.1\n\n\nTo see the list of available learners, type:\n\nmlr_learners\n\n&lt;DictionaryLearner&gt; with 27 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  regr.cv_glmnet, regr.debug, regr.featureless, regr.glmnet, regr.kknn,\n  regr.km, regr.lm, regr.nnet, regr.ranger, regr.rpart, regr.svm,\n  regr.xgboost\n\n\nNote that this package does not contain the algorithms, but acts to link a diverse range of R packages that include the machine learning methods. To get a better idea of how this works, let’s select a classification algorithm (a classification tree):\n\nlearner = mlr_learners$get(\"classif.rpart\")\nprint(learner)\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\nThe output of the print() function describes this particular algorithm. You should see that the functions used are from the rpart package, as well as information about the type of predictions that can be made and the type of features that can be included. This also lists the current set of parameters (or hyper-parameters) for that particular algorithm. When a learner is created, these are set to default values, and you can see the full list of these by typing:\n\nlearner$param_set\n\n&lt;ParamSet(10)&gt;\n                id    class lower upper nlevels        default  value\n            &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;         &lt;list&gt; &lt;list&gt;\n 1:             cp ParamDbl     0     1     Inf           0.01 [NULL]\n 2:     keep_model ParamLgl    NA    NA       2          FALSE [NULL]\n 3:     maxcompete ParamInt     0   Inf     Inf              4 [NULL]\n 4:       maxdepth ParamInt     1    30      30             30 [NULL]\n 5:   maxsurrogate ParamInt     0   Inf     Inf              5 [NULL]\n 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[0]&gt; [NULL]\n 7:       minsplit ParamInt     1   Inf     Inf             20 [NULL]\n 8: surrogatestyle ParamInt     0     1       2              0 [NULL]\n 9:   usesurrogate ParamInt     0     2       3              2 [NULL]\n10:           xval ParamInt     0   Inf     Inf             10      0\n\n\nThe linear models that we are using today do not have any parameters that we want to adjust, but we will look at optimizing or tuning these for more complex methods in a later lab. Now create a learner for OLS regression (regr.lm) as follows:\n\nlearner = mlr_learners$get(\"regr.lm\")\nprint(learner)\n\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\nlearner$param_set\n\n&lt;ParamSet(14)&gt;\n               id    class lower upper nlevels        default  value\n           &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;         &lt;list&gt; &lt;list&gt;\n 1:            df ParamDbl  -Inf   Inf     Inf            Inf [NULL]\n 2:      interval ParamFct    NA    NA       3 &lt;NoDefault[0]&gt; [NULL]\n 3:         level ParamDbl  -Inf   Inf     Inf           0.95 [NULL]\n 4:         model ParamLgl    NA    NA       2           TRUE [NULL]\n 5:        offset ParamLgl    NA    NA       2 &lt;NoDefault[0]&gt; [NULL]\n 6:      pred.var ParamUty    NA    NA     Inf &lt;NoDefault[0]&gt; [NULL]\n 7:            qr ParamLgl    NA    NA       2           TRUE [NULL]\n 8:         scale ParamDbl  -Inf   Inf     Inf         [NULL] [NULL]\n 9:   singular.ok ParamLgl    NA    NA       2           TRUE [NULL]\n10:             x ParamLgl    NA    NA       2          FALSE [NULL]\n11:             y ParamLgl    NA    NA       2          FALSE [NULL]\n12: rankdeficient ParamFct    NA    NA       5 &lt;NoDefault[0]&gt; [NULL]\n13:           tol ParamDbl  -Inf   Inf     Inf          1e-07 [NULL]\n14:       verbose ParamLgl    NA    NA       2          FALSE [NULL]"
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#training-and-testing-data",
    "href": "GEOG_5160_6160_lab02.html#training-and-testing-data",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Training and testing data",
    "text": "Training and testing data\nNext we’ll create a training and testing dataset. For this first iteration of our model, we’ll split the data manually into two sections, with 80% of the observations in the training set and 20% in the testing. In the following code, we:\n\nSet the random seed to allow for reproducibility (this is optional)\nCreate a set of indices for the training data using the sample() function. The first argument task_housing$nrow gives the range of numbers to randomly sample (between 1 and the number of observations). The second argument 0.8 * task_housing$nrow gives the number of random samples to take\nCreate a set of indices for the testing data as the indices not used in the training set\n\nThe set.seed() function just re-initializes R’s random number generator to the same value. This should ensure that you always get the same random split. You can skip this line if you’d like to see how much the results might vary if you have a different split into training and testing datasets.\n\nset.seed(1234)\ntrain_set = sample(task_housing$nrow, 0.8 * task_housing$nrow)\ntest_set = setdiff(seq_len(task_housing$nrow), train_set)\n\nTo see how many observations are in each set, use the length() function:\n\nlength(train_set)\n\n[1] 15580\n\nlength(test_set)\n\n[1] 3895\n\n\nNow let’s train our first model. The learner that we just created has a variable (model) that contains the model results. At the moment, this is empty:\n\nlearner$model\n\nNow train the model by calling the $train() method of our learner. Note that we supply the task created earlier, and an argument giving the indices of the observations to be used in training:\n\nlearner$train(task_housing, row_ids = train_set)\n\nNow model contains the model output (the coefficients from the linear model).\n\nlearner$model\n\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n       (Intercept)           avg_rooms       bedroom_ratio  housing_median_age  \n          240864.3              1358.9           -371992.4               864.1  \n\n\nCompare these to the results you got using the lm() function above. Are they the same? If not, why not?"
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#prediction",
    "href": "GEOG_5160_6160_lab02.html#prediction",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Prediction",
    "text": "Prediction\nPrediction in mlr3 is fairly straightforward. We use our now-trained learner and the $predict() method. We specify new data by using the testing set indices created above.\n\npredict_val = learner$predict(task_housing, row_ids = test_set)\nprint(predict_val)\n\n&lt;PredictionRegr&gt; for 3895 observations:\n row_ids  truth response\n       2 358500 209532.6\n       3 352100 248882.5\n       4 341300 225086.9\n     ---    ---      ---\n   19466 112000 187273.6\n   19470 116800 211262.7\n   19472  77100 184671.0\n\n\nThe print() function displays the first few rows of these data. Note that one column holds the truth - the observed value, and one holds the predicted value (response). The mlr3viz package contains functions for visualizing various aspects of your model. Here, we use the autoplot() function to display the predicted values of the test set against the truth:\n\nlibrary(mlr3viz)\nautoplot(predict_val)\n\n\n\n\n\n\n\n\nEach point represents one observation from the test set. The \\(x\\)-axis is the predicted value, and the \\(y\\)-axis the observed value. The spread of the cloud gives us some indication about the predictive skill of the learner; wider suggests a poorer performance.\nThe thin black line is the 1:1 line. If the model provides an unbiased prediction, then the points should lie along this line. The blue line is a linear model fit to the points - if this matches the thin line, this is also evidence for low bias. The difference here suggests that the model may slightly over-estimate at higher house values. One thing to note here is that there are several districts where the house prices are predicted to be negative. This is obviously incorrect (unless they are paying people to take a house). Transforming the outcome variable (e.g. log transformation) would help avoid this problem."
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#performance-measures",
    "href": "GEOG_5160_6160_lab02.html#performance-measures",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Performance measures",
    "text": "Performance measures\nThis plot allows us to visualize how well the model has predicted for the test data, but we also need to quantify this using a performance measure. This will eventually allow us to compare different learning algorithms or different setups of the same algorithm to see which is best. Not too surprisingly then, mlr3 comes with a whole suite of different measures that we can use. To see the full list, type:\n\nmlr_measures\n\n&lt;DictionaryMeasure&gt; with 65 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mauc_au1p, classif.mauc_au1u,\n  classif.mauc_aunp, classif.mauc_aunu, classif.mauc_mu,\n  classif.mbrier, classif.mcc, classif.npv, classif.ppv, classif.prauc,\n  classif.precision, classif.recall, classif.sensitivity,\n  classif.specificity, classif.tn, classif.tnr, classif.tp,\n  classif.tpr, debug_classif, internal_valid_score, oob_error,\n  regr.bias, regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae,\n  regr.medse, regr.mse, regr.msle, regr.pbias, regr.pinball, regr.rae,\n  regr.rmse, regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae,\n  regr.smape, regr.srho, regr.sse, selected_features, sim.jaccard,\n  sim.phi, time_both, time_predict, time_train\n\n\nNote that most methods either begin with classif. or regr., indicating what type of task they are suitable for. We create a selected measure with msr(), then use the $score() method to calculate this based on our prediction. One standard measure for regression methods is the root mean squared error (RMSE), so let’s calculate this now:\n\nmeasure = msr(\"regr.rmse\")\npredict_val$score(measure)\n\nregr.rmse \n 92976.82 \n\n\nAnother common measure is the mean absolute error (MAE):\n\nmeasure = msr(\"regr.mae\")\npredict_val$score(measure)\n\nregr.mae \n74937.39 \n\n\nWe can also investigate the model bias:\n\nmeasure = msr(\"regr.bias\")\npredict_val$score(measure)\n\nregr.bias \n-1301.908 \n\n\nNote that we can also use these measure to assess the calibration (the goodness-of-fit). For this, we run a second prediction for the training dataset, and calculate the RMSE.\n\npredict_cal = learner$predict(task_housing, row_ids = train_set)\nmeasure = msr(\"regr.rmse\")\npredict_cal$score(measure)\n\nregr.rmse \n  95260.2 \n\n\nThe RMSE is a good measure of the average error. It’s worth noting here that this suggests our error is pretty large, over $90,000 dollars relative the actual price."
  },
  {
    "objectID": "GEOG_5160_6160_lab02.html#resampling",
    "href": "GEOG_5160_6160_lab02.html#resampling",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Resampling",
    "text": "Resampling\nSo far, we have built and tested our model on a single split of the data (the hold-out method). However, if the training and testing datasets are not well set up, the estimates of model performance can be biased. There are several more exhaustive resampling strategies that can be used instead, and we will implement one of these now. To see the list of available strategies, type:\n\nmlr_resamplings\n\n&lt;DictionaryResampling&gt; with 9 stored values\nKeys: bootstrap, custom, custom_cv, cv, holdout, insample, loo,\n  repeated_cv, subsampling\n\n\nWe will use a \\(k\\)-fold cross-validation strategy (cv) to test our learning algorithm. The resampler is created using the rsmp() function. By default, the cv resampler uses 10 folds, but we will adjust this to use 5, by specifying the value of folds:\n\nresampling = rsmp(\"cv\", folds = 5)\nprint(resampling)\n\n&lt;ResamplingCV&gt;: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n\n\nNote that the Instantiated field is set to FALSE. This simply shows that the resampler has not yet been run.\nNote that we could have created the hold-out method used above, by setting the resampler to:\n\nrsmp(\"holdout\", ratio = 0.8)\n\nWe now run the resampling strategy. To do this, we need to provide a task, so that the dataset can be divided up appropriately. This is carried out by calling the $instantiate() method, and the resulting indices for training and testing for the different folds are stored in the resampling object:\n\nresampling$instantiate(task_housing)\nresampling$iters\n\n[1] 5\n\n\nTo examine any one of the training/test splits, we can obtain the list of indices or row numbers as follows:\n\nresampling$train_set(1)\nresampling$test_set(1)\n\nNow with a task, a learner and a resampling object, we can call resample(), which calibrates the model using the training sets from the resampling strategy, and predicts for the test sets. The argument store_models = TRUE tells the function to save each individual model as it is built.\n\nrr = resample(task_housing, learner, resampling, store_models = TRUE)\n\nINFO  [18:06:54.048] [mlr3] Applying learner 'regr.lm' on task 'housing' (iter 1/5)\nINFO  [18:06:54.077] [mlr3] Applying learner 'regr.lm' on task 'housing' (iter 2/5)\nINFO  [18:06:54.086] [mlr3] Applying learner 'regr.lm' on task 'housing' (iter 3/5)\nINFO  [18:06:54.093] [mlr3] Applying learner 'regr.lm' on task 'housing' (iter 4/5)\nINFO  [18:06:54.099] [mlr3] Applying learner 'regr.lm' on task 'housing' (iter 5/5)\n\nprint(rr)\n\n&lt;ResampleResult&gt; with 5 resampling iterations\n task_id learner_id resampling_id iteration  prediction_test warnings errors\n housing    regr.lm            cv         1 &lt;PredictionRegr&gt;        0      0\n housing    regr.lm            cv         2 &lt;PredictionRegr&gt;        0      0\n housing    regr.lm            cv         3 &lt;PredictionRegr&gt;        0      0\n housing    regr.lm            cv         4 &lt;PredictionRegr&gt;        0      0\n housing    regr.lm            cv         5 &lt;PredictionRegr&gt;        0      0\n\n\nThe output tells us that the resampler ran well, with no errors or warnings. If errors or warnings occur, you can examine them using the appropriate method:\n\nrr$errors\nrr$warnings\n\nWe can now calculate the performance measures. Set the measure to the RMSE as before, then use the $score method (as before) to see the results for each individual fold (in the last column of output:\n\nmeasure = msr(\"regr.rmse\")\nrr$score(msr(\"regr.rmse\"))\n\n   task_id learner_id resampling_id iteration regr.rmse\n    &lt;char&gt;     &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;\n1: housing    regr.lm            cv         1  94384.57\n2: housing    regr.lm            cv         2  94087.38\n3: housing    regr.lm            cv         3  95467.94\n4: housing    regr.lm            cv         4  95715.43\n5: housing    regr.lm            cv         5  94557.95\nHidden columns: task, learner, resampling, prediction_test\n\n\nWe can also get the aggregate RMSE value:\n\nrr$aggregate(measure)\n\nregr.rmse \n 94842.65 \n\n\nAs we saved all the individual models, we can explore these now. These are held in an object $learners:\n\nrr$learners\n\n[[1]]\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\n[[2]]\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\n[[3]]\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\n[[4]]\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\n[[5]]\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\nrr$learners[[1]]\n\n&lt;LearnerRegrLM:regr.lm&gt;: Linear Model\n* Model: lm\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor\n* Properties: loglik, weights\n\n\nAnd we can loop through all models quite simply and print out the coefficients for each one to see how much these vary across the different folds of data:\n\nfor (i in 1:5) {\n  lrn = rr$learners[[i]]\n  print(coef(lrn$model))\n}\n\n       (Intercept)          avg_rooms      bedroom_ratio housing_median_age \n       239272.7248          1570.4689       -373715.1568           877.5073 \n       (Intercept)          avg_rooms      bedroom_ratio housing_median_age \n       238806.2460          1340.7049       -358616.1952           833.3493 \n       (Intercept)          avg_rooms      bedroom_ratio housing_median_age \n        240981.461           1448.431        -381115.528            910.725 \n       (Intercept)          avg_rooms      bedroom_ratio housing_median_age \n        244793.780           1388.121        -388508.827            831.796 \n       (Intercept)          avg_rooms      bedroom_ratio housing_median_age \n       237341.0082          1961.2872       -363392.9427           799.4141"
  },
  {
    "objectID": "99_html_index.html",
    "href": "99_html_index.html",
    "title": "GEOG 5160 6160 Spatial Data Science in Practice",
    "section": "",
    "text": "Spatial data science is a fast-growing discipline, with wide-ranging applications, including human health, economics, water resources, energy and food security, infrastructure, natural hazards, and biodiversity. This class is designed to provide an introduction to the methods used to work with these data, including data acquisition and manipulation, building predictive model pipelines and spatial simulation approaches. Students taking this class will learn both the theory and practice of using these methods through a combination of lecture and hand-on computer work. Over the course of the semester, students will develop their own spatial data analytical projects, which will include the design and implementation of the project as well as the communication of results."
  },
  {
    "objectID": "99_html_index.html#footnotes",
    "href": "99_html_index.html#footnotes",
    "title": "GEOG 5160 6160 Spatial Data Science in Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎"
  },
  {
    "objectID": "GEOG_5160_6160_lab01.html#faceting",
    "href": "GEOG_5160_6160_lab01.html#faceting",
    "title": "GEOG 5160 6160 Lab 01",
    "section": "Faceting",
    "text": "Faceting\nYou can add further information through the use of facets. These break the original plot into a series of small multiples according to some categorical variable. Here, we’ll demonstrate this by plotting the bill measurements, colored by species and faceted by island\n\nggplot(penguin, aes(x = bill_length_mm, \n                    y = bill_depth_mm,\n                    col = species)) +\n  facet_wrap(~island) +\n  geom_point() +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "GEOG_5160_6160_Installation.html#visual-studio-code",
    "href": "GEOG_5160_6160_Installation.html#visual-studio-code",
    "title": "GEOG 5160 6160 Installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nAn alternative (but recommended) interface for Python programming is Microsoft’s Visual Studio Code (VSCode). This can be downloaded for your operating system here for free. Once installed, you’ll need to install the Python language support extension from the Extensions tab (click on the icon with four squares on the far left panel and search for Python).\nOnce this is installed, you should be able to open and create notebooks as you would with jupyter. These need to have a *.ipynb extension to be recognized as notebooks. Once opened, you can use the button in the top-left (labelled ’Select kernel) to use a given Conda environment. Make sure you set this to the environment you created for the class. If you have any trouble finding your Conda environments, there is a good guide to troubleshoot this here."
  }
]