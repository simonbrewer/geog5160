---
title: "GEOG 5160/6160 Lab 09 Pipelines"
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
```

## Introduction

In this lab, we'll go an example of building a pipeline to feed data into a machine learning model. You will need the *credit_data.csv* dataset, which should be available from Canvas with this document. Download this to your `datafiles` folder (extract any zip files). Make a new folder for today's class called `lab09`. 

### R users

Start RStudio and set the working directory to this directory (This can be changed by going to the [Session] menu and selecting [Set working directory] -> [Choose directory...], then browsing to the folder). 

You will need the following packages for today's lab, so make sure to install anything that is missing before proceeding.

- **dplyr**
- **mlr3verse**

### Python users

If you are using Python for today's lab, you'll need to download the Jupyter notebook for this lab (*GEOG_5160_6160_lab09.ipynb*). Make a new folder for the lab (`lab09`) and move the notebook to this. Now open a new terminal (in Windows go to the [Start Menu] > [Anaconda (64-bit)] > [Anaconda prompt]). 

You will need to make sure the following packages are installed on your computer (in addition to the packages we have used in previous labs). 

- **xarray**: functions for working with regular arrays (`conda install xarray`)
- **xgboost**: extreme gradient boosting (`conda install py-xgboost`)

Once opened, change directory to the folder you just made, activate your conda environment

```
conda activate geog5160
```

Start the Jupyter Notebook server:

```
jupyter notebook
```

And open the notebook for today's class. 

## Pipelines

Machine learning pipelines are commonly used to produce reproducible and consistent results from a machine learning model. The general goal is to link together a series of functions that undertake all (or most) of the data pre-processing steps, and link these directly into one or more algorithms. Pipelines have several advantages:

- They will process all data in the same way, making it easy to work with multiple datasets (including datasets used for predictions)
- They provide a more reproducible approach to data processing than a series of ad hoc steps and code
- They can easily be used in training, evaluating and tuning models
- They provide a description of the steps involved for later reference

We'll work again with the `credit_data.csv` file: a dataset of credit rankings for over 4000 people (see appendix for a description of the fields). The goal will be to predict `Status`, a binary outcome with two levels: `good` and `bad`. We'll start by loading the libraries we need:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(mlr3verse)
```

And then reading the data:

```{r}
credit_data <- read.csv("../datafiles/credit_data.csv")
str(credit_data)
```

As we have several categorical variables, we need to make sure that R recognizes these as factors. The following line of code checks each column in the `credit_data` data frame, and if it contains character data, it then converts it to a factor. Note this is similar to the approach in previous labs, where we convert individual variables to factors:

```{r}
credit_data <- credit_data %>% 
  mutate_if(is.character,as.factor)
```

If you run the `summary()` function, you should see that there are several missing values:

```{r}
summary(credit_data)
```

As machine learning algorithms can't use missing data to train, we need to decide what to do with these. Fortunately, **mlr3** does have functions for training classification neural networks, so we can take advantage of some of the pre-processing tools in the **mlr3pipelines** library to help with these missing values. Let's first set up a classification task with the credit dataset:

```{r}
credit_task <- TaskClassif$new("credit", backend = credit_data,
                               target = "Status")
```

There are several steps that we might want to do to process these data:

- Impute any missing values
- Convert categorical/factor variables to numeric by one-hot encoding
- Scale the numerical variables to prevent biases while training our neural networks

While it is possible to do this in an ad-hoc way (as we did in previous labs), we will set up a processing *pipeline* that contains all of these steps. This takes more time to set up, but has a number of advantages: we can use the pipeline directly in cross-validation or tuning, and we can use it to process any new data that we might to make predictions for, without having to remember the individual steps. The main function is `PipeOps` or `po()` which creates a pipeline operator that will carry our a single data processing step. To see the set of options, simply type:

```{r results='hide'}
library(mlr3pipelines)
mlr_pipeops
```

More information on the individual operations (and some examples) can be found [here][poID]. 

It is possible to set up the full pipeline in one step, but we'll work through this gradually so you can get a sense of what each operator is doing. First, we'll design two operators to process the categorical data. The first (`impute_cat`) will carry out a mode-based imputation of any missing values (i.e. fill in with the most common value). Note that we include an argument `affect_columns` to define which features should be processed (this will ignore any numerical features). The second operator will one-hot encode the same set of features. The argument `method = "treatment"` will drop the first encode column to prevent redundancy and multicollinearity (and alternative is `method = "onehot"` which doesn't drop the first column). 

```{r}
impute_cat <- po("imputemode",
             affect_columns = selector_type("factor"))
encode <- po("encode", method = "treatment",
             affect_columns = selector_type("factor"))
```

With these set up, we can now use a pipe function (`%>>%`) to combine these into a pipeline for the categorical features:

```{r}
cat <- impute_cat %>>%
  encode 
```

The pipeline we have just created has a `train()` method. This is does not train a model (as we haven't linked one into the pipeline yet), but will run the set of operators on a task. We can use this to show what the resulting transformation is:

```{r}
cat$train(credit_task)[[1]]$data()
```

You should see here that each of the original factor variables (e.g. `Home`) has been one-hot encoded, and there are now $m-1$ new features, each representing one level in the original factor (e.g. `Home.owner`). Note that this automatically drops the original feature and has not affected the numerical variables. In fact, if you re-run the `summary()` function on this new dataset, you should see that there are still missing variables in the numerical features

```{r eval=FALSE}
summary(cat$train(credit_task)[[1]]$data())
```

Now we'll set operators for the numerical variables: we'll use median imputation (`imputemedian`) for missing values, and we'll use a min-max scaling to a 0-1 range (`scalerange`):

```{r}
impute_num <- po("imputemedian", 
            affect_columns = selector_type("integer"))
scale <- po("scalerange", param_vals = list(lower = 0, upper = 1), 
            affect_columns = selector_type("integer"))
num <- impute_num %>>%
  scale 
```

As before, we can see the resulting transformation (note that this has not affected any of the factor variables):

```{r}
num$train(credit_task)[[1]]$data()
```

We can now combine these two sets of operators into the full pipeline. 

```{r}
graph <- cat %>>%
  num
```

If you now run this on the credit task, the resulting dataset has the full set of encoded variables and has now imputed values for anything that was missing. 

```{r}
graph$train(credit_task)[[1]]$data()
summary(graph$train(credit_task)[[1]]$data())
```

### Linking learners

So far, we have a pipeline that transforms the dataset into a format ready for modeling. The next step is to link it to a learner, so that we can pass the transformed data directly to it. We'll set up a classification based random forest, with `mtry = 3` and `num.trees = 500`:

```{r eval=TRUE}
lrn_rf <- lrn("classif.ranger", 
              mtry = 3, 
              num.trees = 500,
              predict_type = "prob")
```

And we can just add this to our existing pipeline

```{r}
graph <- cat %>>%
  num %>>%
  lrn_rf
```

**mlr3** uses a graph framework to connect all the pieces of a framework, which means that it can use R-based tools to visualize the pipeline:

```{r}
plot(graph)
```

Now if we call the `train()` method on the credit task, the data gets passed throught the pipeline, transformed, encoded and imputed and then used to train the neural network:

```{r message=FALSE}
graph$train(credit_task)
```

## GraphLearners

In the previous section, we linked the processing steps through to the final algorithm and trained it. For more complex work, e.g. evaluation or tuning, it it is necessary to convert this into a `GraphLearner`. **mlr3** recognizes this as a new learner (rather than a pipeline), with the same attributes as the standard `learner` objects we have used previously.

```{r}
glrn = GraphLearner$new(graph)
```

### Cross-validation

Now we can set up a 5-fold cross-validation and performance metric:

```{r}
resampling = rsmp("cv", folds = 5)
resampling$instantiate(credit_task)
msr_auc = msr("classif.auc")
```

```{r echo=FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

And run the resampler:

```{r}
rr = resample(credit_task, glrn, resampling, store_models = TRUE)
```

```{r}
rr$score(msr_auc)
rr$aggregate(msr_auc)
```

### Tuning

We can also use our combined pipeline and learner to tune hyperparameters of the learner. We'll try tuning the number of trees used to between 100 and 500. To see the full set of available hyperparameters, just type `glrn$param_set`.

```{r}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamInt$new("classif.ranger.num.trees", lower = 100, upper = 500)
))
tune_ps
```

If you are wondering where the name of the hyperparameter we are tuning (`classif.ranger.num.trees`) comes from, it's a combination of the learner `classif.ranger` and the parameter in the learner (`num.trees`). To check the names of all the available hyperparameters, just type: `glrn$param_set`.

Now we set up a terminating condition (`none` = run all possible values), and a search strategy (simple grid choice with 10 steps):

```{r}
evals = trm("none")
tuner = tnr("grid_search", resolution = 10)
```

And finally, we build an `AutoTuner` to carry out the tuning using a simple holdout strategy for the inner cross validation. Note that we use the GraphLearner, rather than the base random forest learner as the first argument:

```{r}
at_rf = AutoTuner$new(learner = glrn, 
                      resampling = rsmp("holdout"),
                      measure = msr_auc, 
                      search_space = tune_ps,
                      terminator = evals,
                      tuner = tuner)
```

And with that done, it's time to tune the model (this will take a few seconds):

```{r message=FALSE}
at_rf$train(credit_task)
```

```{r}
at_rf$learner
at_rf$tuning_result
```

The final selected model as a hidden layer size of `r at_rf$tuning_result$classif.ranger.mtry` and an AUC of `r round(at_rf$tuning_result$classif.auc, 2)`. 

## Pipeline predictions

We can use the tuned learner to make predictions for a new data set. As the learner is built on the data transformation pipeline, we would need to carry out any transformations prior to prediction. Instead we can simply pass the new data to the learner, and leave it to do all that for us. We'll create a single example and use the `predict_new()` method to get a prediction of credit risk (feel free to use different values to see the impact here):

```{r}
new_credit <- data.frame(Seniority = 8, 
                         Home = "rent",
                         Time = 36,
                         Age = 26,
                         Marital = "single",
                         Records = "no",
                         Job = "fixed",
                         Expenses = 50,
                         Income = 100,
                         Assets = 0,
                         Debt = 10,
                         Amount = 100,
                         Price = 125)

at_rf$learner$predict_newdata(newdata = new_credit)
```

## Feature selection

When working with datasets with large numbers of features we often want to reduce the number of features used to build the model. We can further modify the pipeline to allow for feature selection using the **mlr3filters** package. The full set of filters can be found on the package [website][filID] or by typing:

```{r}
library(mlr3filters)
mlr_filters
```

We'll use the `mim` filter. For a given classification task this calculates an information based correlation between the outcome and each feature and then selects the subset with the highest values. We'll use this to pick the top 3 features. As we want to use this in the pipeline, we again use the `po()` function to create a new operator. We specify the filter type (`mim`) and the number of features we want to retain:

```{r}
filter_mim <- po("filter", flt("mim"), filter.nfeat = 3)
```

We then just need to add this to our pipeline

```{r}
graph <- cat %>>%
  num %>>%
  filter_mim 
```

And you can see which features are selected:

```{r}
graph$train(credit_task)[[1]]$data()
```

Note that one impact of running this following the one-hot encoding is that the filter might select the encoding of *individual* levels of the original factor. 

We don't, however, know if 3 is the best subset of variables to include in the model. This is where the link between the pipeline, learner and tuning becomes very useful as we can tune this parameter (`filter.nfeat`) just as we would tune any hyperparameter. To do this, first create a new `GraphLearner` with the combination of data transformations, filter and learner:

```{r}
graph <- cat %>>%
  num %>>%
  filter_mim %>>%
  lrn_rf

glrn = GraphLearner$new(graph)
```

Now set up a new tuning grid that includes both the number of features and the number of trees in the random forest (as a reminder, type `glrn$param_set` to see the parameter names)

```{r}
tune_ps = ParamSet$new(list(
  ParamInt$new("mim.filter.nfeat", lower = 5, upper = 20),
  ParamInt$new("classif.ranger.num.trees", lower = 100, upper = 500)
))
tune_ps
```

Now we set up the tuning strategy (terminator, search). As there are 19 possible values for the number of features and 401 for `num.trees`, there are 7619 possible combinations to try. To save time in the lab, we'll run a random search on 20 possible combinations:

```{r}
evals = trm("evals", n_evals = 20)
tuner = tnr("random_search")
at_rf = AutoTuner$new(learner = glrn, 
                      resampling = rsmp("holdout"),
                      measure = msr_auc, 
                      search_space = tune_ps,
                      terminator = evals,
                      tuner = tuner)
```

And finally run. Note that this is not a very exhaustive search and you might want to increase `n_evals` if you have the time.

```{r}
at_rf$train(credit_task)
```

Let's take a look at the output

```{r}
at_rf$learner
at_rf$tuning_result
```

### PCA transformation

For a final example, we'll look at a different feature selection strategy. Rather than selecting out original features, we'll use a PCA transformation to create new features. These are based on the original features, but a) are uncorrelated and b) try to maximize the amount of information contained in each one. 

To do this, we'll recreate our pipeline with a new operator that will carry out the PCA transformation (`pca`). We also add a new filter that selects the set of new features based on how much of the original variation in the dataset that they explain (we'll start by choosing the set that explain >50%).


```{r}
pca <- po("pca")
filter <- po("filter", filter = flt("variance"), filter.frac = 0.5)
graph <- cat %>>%
  num %>>%
  pca %>>% 
  filter
```
Let's take a look at the transformed data:

```{r}
graph$train(credit_task)[[1]]$data()
```

Which gives us `r ncol(graph$train(credit_task)[[1]]$data()) - 1` new features, called PC1, etc. Now we'll connect this pipeline to our learner:

```{r}
graph <- cat %>>%
  num %>>%
  pca %>>% 
  filter %>>%
  lrn_rf

plot(graph)
```

And create a new GraphLearner

```{r}
glrn = GraphLearner$new(graph)
```

With all this in place, we can now tune our model. We'll again tune the size of the hidden layer, but we'll also use tune the number of new PC features that we use in the model by tuning the `filter.frac` parameter.

- Set up parameter space
```{r}
tune_ps = ParamSet$new(list(
  ParamDbl$new("variance.filter.frac", lower = 0.25, upper = 0.95),
  ParamInt$new("classif.ranger.num.trees", lower = 100, upper = 500)
))
tune_ps
```

- Set up terminator and search strategy (these are the same as before):

```{r}
evals = trm("evals", n_evals = 20)
tuner = tnr("random_search")
```

- Set up the `AutoTuner`

```{r}
at_rf = AutoTuner$new(learner = glrn, 
                      resampling = rsmp("holdout"),
                      measure = msr_auc, 
                      search_space = tune_ps,
                      terminator = evals,
                      tuner = tuner)
```

- And tune

```{r}
at_rf$train(credit_task)
```

When this is eventually done, inspect the output to see the final choices for the two hyperparameters, as well as the final AUC for the tuned model:

```{r}
at_rf$learner
at_rf$tuning_result
```

## Evaluating tuned pipelines

While the rpevious code allows us to select the value of the hyperparameters, the performance score shown above is only based on the training set. It is calculated using only part of that training set, but is still considered to not be an independent test of predictive skill. To evaluate the model, we need to run the same nested cross-validation that we looked at in a previous lab. Fortunately, we have nearly all the elements we need to this (the pipeline / graph learner and the autotuner). We just need to do the following:

- Define the inner and outer cross-validation strategies

```{r}
rsmp_inner <- rsmp("holdout", ratio = 0.8)
rsmp_outer <- rsmp("cv", folds = 3)
```

- Update the autotuner

```{r}
at_rf = AutoTuner$new(learner = glrn, 
                      resampling = rsmp_inner,
                      measure = msr_auc, 
                      search_space = tune_ps,
                      terminator = evals,
                      tuner = tuner)
```

- Run

```{r}
rr_rf <- resample(credit_task, at_rf, rsmp_outer,
                  store_models = TRUE)
```

And print the final set of results:

```{r}
extract_inner_tuning_results(rr_rf)[, 
                                    c("variance.filter.frac",
                                      "classif.ranger.num.trees",
                                      "classif.auc")]
```

## Exercise

For the exercise we will once again use the data from the *Sonar.csv* file to model types of object (rocks 'R' or mines 'M') using the values of a set of frequency bands. The goal of the exercise is to build the best predictive model for predicting these data, and you are free to choose any of the algorithms/learners we have previously looked at. You should use the **mlr3** framework to setup, train and test your model. You will need to choose a cross-validation strategy and calculate the AUC to assess the model. 

As the data has a large number of features, you should build a pipeline to reduce the number of features using one of the two filter examples (mutual information or PCA) from the lab. Note that there are no categorical features so you can skip those steps. You should then tune both the filter and at least one hyperparameter of your model (if you are not sure about this, please ask!)

Your answer should consist of the following

- A description of your pipeline (this can include a figure showing the steps)
- The values you obtained for the number of features and the selected hyperparameter
- The cross-validated AUC

You should also provide your full R code. 

## Appendix

### Cereal data set

From https://www.kaggle.com/crawford/80-cereals

|    | Column name | Feature                | 
|----|-------------|------------------------|
| 1  | `Name`      | Name of cereal          |
| 2  | `mfr`       | Manufacturer of cereal  |
|    |             | A = American Home Food Products |
|    |             | G = General Mills |
|    |             | K = Kelloggs |
|    |             | N = Nabisco |
|    |             | P = Post |
|    |             | Q = Quaker Oats |
|    |             | R = Ralston Purina |
| 3  | `type`      | cold or hot |
| 4  | `calories`  | calories per serving |
| 5  | `protein`   | grams of protein          |
| 6  | `fat`       | grams of fat         |
| 7  | `sodium`    | milligrams of sodium   |
| 8  | `fiber`     | grams of dietary fiber           |
| 9  | `carbo`     | grams of complex carbohydrates   |
| 10 | `sugars`    | grams of sugars       |
| 11 | `potass`    | milligrams of potassium       |
| 12 | `vitamins`  | vitamins and minerals - 0, 25, or 100,          |
|    |             | indicating the typical percentage of FDA recommended         |
| 13 | `shelf`     | display shelf (1, 2, or 3, counting from the floor) |
| 14 | `weight`    | weight in ounces of one serving         |
| 15 | `cups`      | number of cups in one serving          |
| 16 | `rating`    | a rating of the cereals          |

### Credit data set 

From https://github.com/gastonstat/CreditScoring

|    | Column name | Feature                | 
|----|-------------|------------------------|
| 1  | `Status`    | credit status          |
| 2  | `Seniority` | job seniority (years)  |
| 3  | `Home`      | type of home ownership |
| 4  | `Time`      | time of requested loan |
| 5  | `Age`       | client's age           |
| 6  | `Marital`   | marital status         |
| 7  | `Records`   | existence of records   |
| 8  | `Job`       | type of job            |
| 9  | `Expenses`  | amount of expenses     |
| 10 | `Income`    | amount of income       |
| 11 | `Assets`    | amount of assets       |
| 12 | `Debt`      | amount of debt         |
| 13 | `Amount`    | loan amount requested  |
| 14 | `Price`     | price of good          |

[poID]: https://mlr3pipelines.mlr-org.com
[filID]: https://mlr3filters.mlr-org.com/index.html
