---
title: "GEOG 5160 6160 Lab 07"
author: 
  - name: "Simon Brewer"
    email: simon.brewer@ess.utah.edu
    affiliations:
      - name: University of Utah
        address: 260 S Central Campus Drive
        city: Salt Lake City
        state: UT
        postal-code: 84112
date: last-modified
format:
  html:
    toc: true
editor: visual
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(png)
library(grid)
set.seed(42)
```

# Introduction

For the next few labs, we'll be looking at deep learning models. As these are computationally costly models, they are generally built using specialized software to make them as efficient as possible. The full software stack is a little complicated, but in short, it consists of:

-   The backend deep learning software. This includes **tensorflow**, **torch** and **jax**.
-   The **keras** API. This is written in Python and tries to act as a unified interface to the different types of software
-   The R **keras** package. This uses the **reticulate** package to call **keras** functions from R

If at this point, this seems excessively complicated, well, it is. But there is a reason for this, which is to allow a great amount of flexibility. If you want to develop very complex models, you can work directly with the deep learning package. If you need only to use standard functions, then you can use **keras**. And if you want to use R as the front-end to all of this, you can do that too.

We'll mainly be using a combination of **keras** and **tensorflow** in this class, so the goals of this lab are to introduce these are get them running with a couple of simple examples.

## Objectives

-   Set up **keras** and **tensorflow** on your computer
-   Understand the basic operations in a neural network
-   Build a simple classification model using standard network layers
-   Train and evaluate this model

# Installation

Installing deep learning software can be quite complicated. The backend software is in continual and rapid development, so that the libraries change often. In addition, these often are set up to use any available GPU power rather than (or in addition to) CPU power, and this requires an extra step of installing drivers for this. Here, we'll try to use the simplest approach in R, which is to install the **keras** package, and then let it take care of the backend. So start by installing this package:

```{r eval=FALSE}
install.packages("keras")
```

If everything went ok, then load the library and run the following command to install the backend

```{r eval=FALSE}
library(keras)
install_keras()
```

This should create a specific virtual environment called `r-tensorflow`, and install some additional packages (this may take a few minutes). If you get any errors with this, please let me know.

## Installing into a virtual environment

You can also change the name of the virtual environment as follows (this allows you to install different backends or versions in different environments). Don't worry about this step if the first installation went well

``` {eval="FALSE"}
library(reticulate)
virtualenv_create("r-keras", python=install_python())
library(keras)
install_keras(envname = "r-keras")
use_virtualenv("r-keras")
```

# **Tensorflow**

## Tensors

Let's start by looking at the building blocks of these networks, *tensors*. This is the standard data object, and describes an array of numbers with a certain number of dimensions. If that sounds a lot like matrices and arrays, it is because this is just a highly efficient way of storing these. Start by loading the libraries:

```{r}
library(keras)
library(tensorflow)
```

Now we can make a simple 2D tensor with 2 rows and 3 columns as follows. This uses R's `array` function to create the data, then converts it to a tensor:

```{r}
r_array <- array(1:6, c(2, 3))
tf_tensor_2D <- as_tensor(r_array)
tf_tensor_2D
```

And here's a 1D tensor

```{r}
r_array <- array(1:4, c(4))
tf_tensor_1D <- as_tensor(r_array)
tf_tensor_1D
```

And a 3D tensor

```{r}
r_array <- array(1:12, c(2,3,2))
tf_tensor_3D <- as_tensor(r_array)
tf_tensor_3D
```

You can use a lot of the basic R functions with these. For example, to see the size of the tensor

```{r}
dim(tf_tensor_2D)
```

or

```{r}
tf_tensor_2D$ndim
```

You can also add two tensors (as long as they have the same shape)

```{r}
tf_tensor_2D + tf_tensor_2D
```

There are also functions to create tensors with specific values

```{r}
## Tensor of 1s
tf$ones(shape(1, 3))
## Tensor of 0s
tf$zeros(shape(1, 3))
## Tensor of random numbers (normally distributed)
tf$random$normal(shape(1, 3), mean = 0, stddev = 1)
## Tensor of random numbers (uniformly distributed)
tf$random$uniform(shape(1, 3))
```
**tensorflow** also defines a `variable` data type:

```{r}
W <- tf$Variable(array(0, dim = c(2,2)))
W
```

While this will look a lot like a tensor, this is used to store values that change during training including model weights and parameters

## Basic operations

**tensorflow** comes with a lot of basic math functions. Let's start by creating a 2D random tensor:

```{r}
tensor_rnd = tf$random$normal(shape(2, 3), mean = 10, stddev = 1)
```

Then we can calcualte the square or square root:

```{r}
tf$square(tensor_rnd)
```

```{r}
tf$sqrt(tensor_rnd)
```

There are also specific functions that are key to train a neural netwok. You might recall from the lecture that there are three main calculate steps. 

1) In the forward pass, the output of every neuron in the network is calculated as the weighted sum of the inputs, plus a bias term $y = W \cdot x + b$. The first part of this equation is the dot product of two matrices, the array of weights ($W$) and the input values ($x$). In tensorflow, this calculated using the `matmul` function.

For example, here we create tensors of weights and random input values and multiply them together:

```{r}
W <- tf$Variable(array(runif(4), c(2,2)))
print(W)

x <- as_tensor(array(rnorm(4), c(2, 2)))
print(x)

tf$matmul(x, W)
```

If we also create a *bias* (a constant additive term), we can create the basic weighted sum

```{r}
b <- tf$Variable(array(1, dim = c(2)))
print(b)

tf$matmul(x, W) + b
```

2. Activation function

The next step in the forward pass is to send the output of node (above) through an activation function. There are several of these, and the code below simply illustrates how they convert a vector of randomly generated values (`x`)

- Linear activation

```{r}
x <- as_tensor(array(rnorm(100), c(100, 1)))
x_act <- activation_linear(x)
plot(x, x_act)
```

- Sigmoid activation. Note that this requires a second column of zeros added to it

```{r}
xs <- k_concatenate(list(x, 
                         as_tensor(rep(0, length(x)), shape = c(length(x),1))))
x_act <- activation_sigmoid(xs)
plot(x[,1], x_act[,1])

```

- ReLu activation. This is a widely used activation function in deep learning, which sets all output below zero to zero, and uses a linear transform for the values above 0

```{r}
x_act <- activation_relu(x)
plot(x, x_act)
```

3. Backpropagation

Following the forward pass, neural networks are trained using backpropagation. In this step, a gradient is calculated for each weight relatively to the loss or error. This gradient tells the network how much to update each weight (steep gradients mean that the error is large and a larger update is needed), as well as the direction of change (positive or negative). This becomes very complicated in larger networks, where this has to be (back)propagated through multiple layers and across mulitple nodes. Tensorflow uses the concept of a gradient *tape* to help in this. This records all the necessary steps to estimate these gradients by tracking all the connections in the network. Once set up, it can then estimate the gradient of any output in the network relative to any variable or set of variables. To illustrate this, the following code uses a simple model of $y = 2 \times x + 3$. This is used to create a `GradientTape` object, and then we can estimate the gradient. As this is just a simple linear model, the gradient is, of course, the slope of 2:

```{r}
x <- tf$Variable(0)
with(tf$GradientTape() %as% tape, {
  y <- 2 * x + 3
})
grad_of_y_wrt_x <- tape$gradient(y, x)
grad_of_y_wrt_x
```

# Putting it together: a linear classifier

We'll now illustrate how all of this comes together with a really simple example. This is a dataset with two features (`x1` and `x2`) and a binary target (`class`):

```{r}
df = read.csv("./datafiles/slc.csv")
head(df)
```

As tensorflow really wants values as an array, we'll create two of these, one for the features (`inputs`) and one for the targets:

```{r}
inputs <- cbind(df$x1, df$x2)

targets <- array(ifelse(df$class == "neg", 0, 1), dim = c(nrow(df),1))
```

Let's plot these out to see the distribution:

```{r}
plot(x = inputs[, 1], y = inputs[, 2],
    col = ifelse(targets[, 1] == 0, "purple", "green"))
```

From the plot, you should be able to already visualize where you would place a line to separate the two groups. We'll now create a linear classifier that can find this line as the following equation (if you look closely, you'll see that this is just a linear model with slopes (`W`) and intercept (`b`)). 

$$
\mbox{prediction} = W \times input + b
$$




This is trained to minimize the square of the difference between predictions and the targets. As you’ll see, it’s actually a much simpler example than the end-to-end example of a toy two-layer neural network you saw at the end of chapter 2. However, this time you should be able to understand everything about the code, line by line.

Next, we'll initialize values for `W` and `b`, using random values and with zeros, respectively.

```{r}
input_dim <- 2
output_dim <- 1
W <- tf$Variable(initial_value =
                   tf$random$uniform(shape(input_dim, output_dim)))
print(W)
b <- tf$Variable(initial_value = tf$zeros(shape(output_dim)))
print(b)
```


As we have two inputs, `W` is just two scalar coefficients, `w1` and `w2`. `b` is a single scalar coefficient representing the intercept. So the full, expanded model is:

$$
\mbox{prediction} = [w_1,w_2] \times [x_1, x_2] + b = w_1 \times x_1 + w_2 \times x_2 + b
$$


Let's create a function that performs a forward pass by calculating the above equation (note this uses the `matmul` function to get the weighted output). Normally, this would use a sigmoid activation function as the outcome is binary, but to keep things simple, we won't transform the output. Note that we use a function to calculate this so that we can reuse this easily in the training loop below

```{r}
model <- function(inputs)
 tf$matmul(inputs, W) + b
```

To illustrate what this does, let's just run this with the first row of inputs:

```{r}
input1 = as_tensor(array(inputs[1, ], c(1,2)), dtype = 'float32')
model(input1)
```

Giving the predicted value for the first observation (which is a `0`). This is obviously with random weights, so there's really no expectation that it will be close to the actual value. Next, we need to calculate how close it is through a loss function. For this example, we'll use a simple mean squared error, and we'll create a function to calculate that:

```{r}
square_loss <- function(targets, predictions) { 
  per_sample_losses <- tf$square(tf$subtract(targets, predictions))
  tf$reduce_mean(per_sample_losses)
}
```

Now let's put these together with the gradient tape function to calculate the update to the weights through backpropagation. In this function, it uses the `GradientTape` to store the results of the forward pass and loss calculation. Then the tape is used to get the gradients or derivatives of the weights relative to the loss. Finally, the update is calculated by multiplying the gradient by a learning rate. This is a value less than one, which limits the update at each step. 

```{r}
learning_rate <- 0.1
training_step <- function(inputs, targets) {
  with(tf$GradientTape() %as% tape, {
    predictions <- model(inputs)
    loss <- square_loss(predictions, targets)
  })
  grad_loss_wrt <- tape$gradient(loss, list(W = W, b = b))
  W$assign_sub(grad_loss_wrt$W * learning_rate)
  b$assign_sub(grad_loss_wrt$b * learning_rate)
  loss
}
```

After all that preparation, we're ready to train the model. We convert the inputs to a tensor, then run a loop 40 times, each time calling the `training_step` function to update the weights

```{r}
inputs <- as_tensor(inputs, dtype = "float32")
 for (step in seq(40)) {
   loss <- training_step(inputs, targets)
   cat(sprintf("Loss at step %s: %.4f\n", step, loss))
 }
```

Once trained, we can predict for each of the original observations and plot the predicted values. We'll use a simple threshold of 0.5 (i.e. predictions above 0.5 are classed as `1`, below 0.5 as `0`). We can also plot the classifier by using the trained values of `W` and `b` to get the slope and intercept:

```{r}
predictions <- model(inputs)
inputs <- as.array(inputs)
predictions <- as.array(predictions)

plot(x = inputs[, 1], y = inputs[, 2],
    col = ifelse(predictions[, 1] <= 0.5, "purple", "green"))

slope <- -W[1, ] / W[2, ]
intercept <- (0.5 - b) / W[2, ]
abline(as.array(intercept), as.array(slope), col = "red")
```

# The **keras** API

The examples above show how to work with the low-level **tensorflow** interface. In practice, you can skip a lot of the detail by using the **keras** API. In the code below, we'll build a multi-layer neural network 


```{r}
library(tidyverse)
df = read.csv("./datafiles/credit_data.csv")
head(df)
```

```{r}
df = df |>
  # mutate(Status = as.factor(Status)) |>
  mutate(Status = ifelse(Status == 'bad', 1, 0)) |>
  drop_na()
```


```{r}
library(tidymodels)

df_split = initial_split(df, prop = 0.8)

df_train = training(df_split)
df_test  = testing(df_split)
```

```{r}
table(df_train$Status)
table(df_test$Status)
```


```{r}
rec = recipe(Status ~ ., df_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

df_train = rec |>
  prep() |>
  bake(df_train)

df_test = rec |>
  prep() |>
  bake(df_test)
```


```{r}
feature_names = colnames(df_train) |> 
  setdiff("Status")
feature_names
```





```{r}
train_features = as.matrix(df_train[,feature_names])
train_targets <- as.matrix(df_train$Status)

test_features = as.matrix(df_test[,feature_names])
test_targets <- as.matrix(df_test$Status)

```

## Keras

```{r}
model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(256, activation = "relu") |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

model
```


```{r}
metrics <- list(
  metric_precision(name = "accuracy")
  # metric_precision(name = "precision"),
  # metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-4),
  loss = "binary_crossentropy",
  metrics = metrics
)
```



```{r}
model |> fit(
  train_features, train_targets,
  validation_data = list(test_features, test_targets),
  batch_size = 256,
  epochs = 30,
  # callbacks = callbacks,
  verbose = 2
)
```

```{r}
plot(model$history$epoch, model$history$history$loss, type = 'l')
lines(model$history$epoch, model$history$history$val_loss, col = 2)
```

```{r}
plot(model$history$epoch, model$history$history$accuracy, type = 'l')
lines(model$history$epoch, model$history$history$val_accuracy, col = 2)
```

```{r}
y_pred_test <- model |>
  predict(test_features)

y_pred_test <- ifelse(y_pred_test > 0.5, 1, 0)
```

```{r}
table(test_targets, y_pred_test)
```

```{r}
pred_correct <- df_test$Status == y_pred_test
cat(sprintf("Validation accuracy: %.2f", mean(pred_correct)))
```


# Exercise

For the exercise, you will need to build use an unsupervised classification method with the Cancer dataset from the lab 4 (*data_atlantic_1998_2012.csv*). As a reminder, this file contains information on cancer rates from around 660 counties in the eastern part of the US. There is also a shapefile with the polygons for each county (*COUNTY_ATLANTIC.shp*).

For the exercise, you will need to cluster these data and map out the resulting clusters. You will need to carry out the following steps:

-   Read in the data and select the columns: `Cancer`, `Smoking`, `Poverty`, `PM25`, `SO2` and `NO2`
-   Scale the data using a $z$-score transform
-   Cluster the data using *either* $k$-means *or* a self-organizing map
-   Extract the cluster number for each county and add it to the shapefile
-   Make a map of the clusters
-   

Use a Quarto document to record your answers and output. Assignments, to include both the Quarto document and (ideally) the compiled HTML file, should be submitted to Canvas by Mar 17th. Please use the following naming convention: `Lab06_lastname`.

# Appendix

## Gap Minder Dataset

GapMinder dataset on global inequality (1800 to 2018): *gapminder_1800_2018.csv*

| Column header     | Variable                           |
|-------------------|------------------------------------|
| `country`         | Country name                       |
| `year`            | Year                               |
| `pop`             | Population                         |
| `child_mortality` | Child mortality rate (1000 births) |
| `fertility`       | Birth rate                         |
| `per_cap_co2`     | Per capita CO2 emissions           |
| `income`          | Mean income (\$)                   |
| `life_expectancy` | Mean life expectancy               |
| `population`      | Population size                    |

## Atlantic county cancer rates

*data_atlantic_1998_2012.csv*

| Column header | Variable                             |
|---------------|--------------------------------------|
| `FIPS`        | Country name                         |
| `x`           | Year                                 |
| `y`           | Population                           |
| `Cancer`      | Cancer mortality rate / 100,000      |
| `Poverty`     | Poverty rate (% below poverty level) |
| `Smoking`     | Smoking rate (%)                     |
| `PM25`        | Annual mean PM2.5 concentration      |
| `NO2`         | Annual mean NO2 concentration        |
| `SO2`         | Annual mean SO2 concentration        |
