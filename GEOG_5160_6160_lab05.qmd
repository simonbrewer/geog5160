---
title: "GEOG 5160 6160 Lab 04"
author: 
  - name: "Simon Brewer"
    email: simon.brewer@ess.utah.edu
    affiliations:
      - name: University of Utah
        address: 260 S Central Campus Drive
        city: Salt Lake City
        state: UT
        postal-code: 84112
date: last-modified
format:
  html:
    toc: true
editor: visual
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(png)
library(grid)
set.seed(42)
```

# Introduction

While machine learning methods have been frequently used with spatial data, there is a growing awareness of how the characteristics of these data may cause some issues. In this lab, we'll look at how more robust methods of evaluating machine learning models with spatial data, and at some approaches that can incorporate location and improve predictions.

We'll use a couple of datasets to illustrate these methods:

-   *lsl.csv*: Location of landslide events in southern Ecuador
-   *ta.tif*: A raster file with environmental predictors
-   *data_atlantic_1998_2012.csv*: A dataset of cancer rates for counties in Atlantic states
-   *COUNTY_ATLANTIC.zip*: a shapefile for the Atlantic States

You will need to make sure the following packages are installed on your computer (in addition to the packages we have used in previous labs).

-   **terra**: working with raster data
-   **sf**: working with spatial data
-   **tmap**: making thematic maps
-   **spatialsample**: spatial sampling
-   **SpatialML**: geographic random forests
-   **FRK**: spatial basis functions

As a reminder, packages can be installed in RStudio by going to the 'Packages' tab and clicking on the \[Install\] button, or from the menu \[Tools\]-\> \[Install packages...\]. You can also install these from the console window by typing

```{r eval=FALSE}
install.packages("terra")
```

## Objectives

-   Understand how to use different cross-validation strategies for spatial data
-   Use a geographic random forest to explore spatial variations in model results
-   Incorporate location in machine learning models

**It is highly recommended to use scripts or Quarto documents to store your R code - this will allow you to easily change and modify it and submit the exercise.**

Next load the libraries you will need for the lab. You should at this stage have most of these already installed. Add anything that is not installed using the `install.packages()` function.

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
library(sf)
library(terra)
library(tmap)
```

## Data processing

Let's load the landslide data first and take a look at the content:

```{r}
lsl = read.csv("./datafiles/lsl.csv")
head(lsl)
```

The first two columns show the easting and northing. The third indicates whether or not a landslide had occurred at that location (the target) and the remaining columns are features to be used in the model. See the appendix for more detail on these.

```{r}
lsl <- lsl |>
  mutate(lslpts = as.factor(as.numeric(lslpts)))
```

We can use the `st_as_sf` function from the **sf** package to convert these to a simple features spatial object and plot. The data are from UTM 17S, so we can use the EPSG code of 32717 to set the correct reference system:

```{r}
lsl_sf <- st_as_sf(lsl, coords = c("x", "y"), crs = 32717)
plot(st_geometry(lsl_sf))
```

And we can use the thematic mapping package to show the distribution of the landslide points:

```{r}
tm_shape(lsl_sf) +
  tm_symbols(col = "lslpts") +
  tm_scale_bar(position = c("left", "top")) +
  tm_compass(position = c("right", "top"))
```

Next let's load some environmental data to make predictions. This is in a multi-layer GeoTIFF file, and we can load it using the `rast` function from **terra**:

```{r}
env <- rast("./datafiles/ta.tif") 

env
```

Each of the individual layers can be accessed using R's list notation, with two brackets. So to map the `log10_carea` values:

```{r}
tm_shape(env[["log10_carea"]]) +
  tm_raster(style = "sd") +
  tm_shape(lsl_sf) +
  tm_dots()
```

# Spatial prediction

First, let's build a simple model using a classic train/test split (80/20) and evaluate it. First create the training/test split. Note that the data is well balanced (equal numbers of 1's and 0's) so we don't need to worry about stratifying the sample:

```{r}
library(tidymodels)
dat_split <- initial_split(lsl, prop = 0.80)

dat_train <- training(dat_split)
dat_test  <- testing(dat_split)
```

Next, we'll build a model. We're going use a random forest without tuning, so we can simply:

-   Set up the formula to define the target (`lslpts`) and the features
-   Instantiate a random forest object
-   Fit the model using the training set

```{r}
lsl_f <- lslpts ~ slope + cplan + cprof + elev + log10_carea

rf <- rand_forest(mode = "classification")

rf_fit <- rf |> 
  fit(lsl_f, dat_train)
```

We'll get a first evaluation here using the AUC:

```{r}
y_test_pred <- predict(rf_fit, dat_test, type = 'prob') |>
  bind_cols(dat_test |> dplyr::select(lslpts))

roc_auc(y_test_pred, lslpts, .pred_1, event_level = 'second')
```

We get an AUC of around 0.88, which indicates a pretty good model. We'll dig into this more below.

Next, let's predict landslide susceptibility for the study region using the raster images we loaded above. In the previous lab, we did this by extracting all the values to a data frame, and then predicting. An easier way is to use the `predict` function that comes with the **terra** package, which allows direct prediction on raster layers. We'll do this below, but a few things to note:

-   We use the `::` notation to force R to use the `predict` function from the **terra** package (there are other packages with the same function name)
-   We use the fitted model from the tidymodels object (`rf_fit$fit`)
-   We set the `type` to `response` to get predictions on a 0-1 scale
-   Last, and possibly most important, the input raster (`env`) must have the same names for the layers as the columns in the data frame used to fit the model

Let's now predict:

```{r}
lsl_pred = terra::predict(env, model = rf_fit$fit, 
                          type = "response", na.rm = TRUE)
```

And plot. Note that this gives two rasters as output: a) the probability of the *absence* of a landslide and b) the probability of *presence*, so we'll plot the second one here (`X1`)

```{r}
tm_shape(lsl_pred[["X1"]]) + 
  tm_raster() + 
  tm_shape(lsl_sf) + 
  tm_dots()
```

# Spatial cross-validation

Now let's dig a little further into the cross-validation of this model. The AUC value we got above is from a single hold-out, ignoring the spatial distribution of the observations.

First, let's re-run this as a 5-fold cross-validation to get a more robust estimate. For this we:

-   Define a recipe using the formula from above
-   Instantiate a random forest (we could simply reuse the code above, but adding this here helps clarify what is going on)
-   Define and create the folds
-   Link all of this together into a workflow
-   Run the cross-validation and check the results

```{r}
rec <- recipe(lslpts ~ slope + cplan + cprof + elev + log10_carea, lsl)

rf <- rand_forest(mode = "classification")

nfolds = 5
folds <- vfold_cv(lsl, v = nfolds)

workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(rf)

results <- workflow |>
  fit_resamples(resamples = folds, 
                metrics = metric_set(accuracy, roc_auc))

collect_metrics(results)
```

And we get a slightly lower, but more robust AUC estimate. However, we still haven't accounted for the spatial dependency in the data. This means that the value we have is somewhat reliant on the spatial correlation between the training and test set, as the observations are mixed. This means that our AUC value is really telling us about how well the model predicts *within* existing observations, in other words, how well it can interpolate.

A different test is how well our model would work in an area where we know the features, but we don't have any observations of landslides (or non-landslides). This will tell us how well the model works at predicting *outside* existing observations, or how well it can extrapolate. To test this, we need to use a spatial sampling method that will divide the data into contiguous training and testing regions. In a spatial $k$-fold, we divide the data into $k$ regions, set one as testing and the others as training, fit and evaluate a model and repeat.

We'll use the **spatialsample** library to create the sampling scheme. This has a number of options - we'll use a clustering approach, where clusters of locations are made based on the distance between them.

Let's go ahead and build this now. Note that we have to pass the `sf` object we made earlier (so that the function can find the coordinates):

```{r}
library(spatialsample)
spfolds <- spatial_clustering_cv(data = lsl_sf, 
                                 v = nfolds)
```

This library comes with an `autoplot` function to quickly visualize the results:

```{r}
autoplot(spfolds)
```

An alternative is spatial block sampling, where the study region is divided into regular grid. All observations in a grid box are considered as part of a fold, with several boxes making the entire fold:

```{r}
spblock <- spatial_block_cv(data = lsl_sf, 
                            v = nfolds)
autoplot(spblock)
```

We'll now use the first set of folds (the cluster sampling) to cross-validate the model. This integrates well with the **tidymodels** approach we have used so far, with the only difference that

-   We need to specify the `sf` object in the recipe, not the data frame
-   We use `spfolds` in the resampling.

I've copied the full code again here, so that it is easier to see what is going on.

```{r}
rec <- recipe(lslpts ~ slope + cplan + cprof + elev + log10_carea, lsl_sf)

rf <- rand_forest(mode = "classification")

workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(rf)

results <- workflow |>
  fit_resamples(resamples = spfolds, 
                metrics = metric_set(accuracy, roc_auc))

collect_metrics(results)
```

You should see here that the AUC decreases substantially, indicating less predictive power when extrapolating.

# Using location in machine learning

We'll now move on to exploring the use of location in machine learning. We'll look at this in three ways

-   Geographical random forest (GRF)
-   Using coordinates as features
-   Spatial basis functions

We'll also use a different data set here, with a continuous outcome, as the current GRF implementation does not work (well) with with binary outcomes.

The file *data_atlantic_1998_2012.csv* contains information on cancer rates from around 660 counties in the eastern part of the US. Let's load that here as well as a shapefile containing the county polygons:

```{r}
atl <- st_read("./datafiles/COUNTY_ATLANTIC.shp")
atl
```

```{r}
cancer <- read.csv("./datafiles/data_atlantic_1998_2012.csv")
head(cancer)
```

Both the shapefile and the cancer data have the county FIPS code, so we can use this to merge the two datasets together, which allows us to map out the cancer rates, showing high rates in the south and western part of the region:

```{r}
atl_cancer <- merge(atl, cancer, by = "FIPS")
```

```{r}
tm_shape(atl_cancer) +
  tm_fill("Cancer", palette = "viridis") +
  tm_scale_bar() + tm_compass()
```

## Geographical random forest

Let's start this section by building a GRF with these data. To keep things simple here and below, we'll just use the pollutant variables as features (`PM25`, `NO2` and `SO2`). The GRF function does not use the `sf` object that we made earlier, but instead uses the original `cancer` dataframe and a second dataframe with the coordinates of each spatial location. We'll extract this from the `sf` object by finding the centroids, and then the coordinates of these

```{r}
atl_crds <- st_coordinates(st_centroid(atl_cancer))
```

Now we can fit the model. We won't try and tune this here, but instead run it with an adaptive kernel that uses the 50 closest locations to build each model in a moving window (you might want to try playing with the `bw` argument to see how this affects the results). Note the other arguments:

-   The formula defining the relationship between variables
-   The dataframes holding the data (`dframe`) and coordinates (`coords`)

```{r warning=FALSE, results='hide', message=FALSE}
library(SpatialML)
cancer_grf <- grf(Cancer ~ PM25 + NO2 + SO2,
    dframe = cancer, kernel = 'adaptive', bw = 50,
    coords = atl_crds)
```

The function will give you quite a lot of output (you can turn this off with `print.results=FALSE`). The routine starts by fitting a global model (i.e. with all the observations) and then fits local models to each observation using the kernel we defined in the function. The output contains a summary of both of these, and we can use this contrast the global and local approaches. Two useful numbers are a) the global and local R2 and b) the global and local MSE.

More usefully, we can start to visualize the results. First we'll explore the importance of the three features we used in the model. As a remidner, this is a measure of the loss of predictive power when one of the variables is randomly shuffled. Let's start by plotting the importance scores for the global model:

```{r}
library(vip)
cancer_grf$Global.Model
vip(cancer_grf$Global.Model)
```

Next, let's get the local importance scores. In the model output, there is a dataframe called `Local.Variable.Importance` which (unsurprisingly) holds the importance scores for each model (for each location). A second dataframe (`LGofFit`) holds goodness of fit scores for each location and we'll use this to visualize variations in the R2:

```{r}
head(cancer_grf$Local.Variable.Importance)
head(cancer_grf$LGofFit)
```

Let's extract these and add them to our `sf` object:

```{r}
atl_cancer$localR2 <- cancer_grf$LGofFit$LM_Rsq100 
atl_cancer$vip_PM25 <- cancer_grf$Local.Variable.Importance$PM25
atl_cancer$vip_NO2 <- cancer_grf$Local.Variable.Importance$NO2
atl_cancer$vip_SO2 <- cancer_grf$Local.Variable.Importance$SO2
```

And finally map everything:

```{r message=FALSE, warning=FALSE}
tm_shape(atl_cancer) + 
  tm_fill(c("localR2", 
            "vip_PM25", "vip_NO2", "vip_SO2"))
```

A few things to note from these maps:

-   The local R2 values vary a lot (with some regions that have negative values). This can be interpreted as a map of where the model is doing a reasonable job (green) and where it fails (red). The low values indicate that other variables may be important in producing this model
-   The importance scores for the three features largely follow the same pattern (high importance around metropolitan centers, low elsewhere). This is not too surprising as pollutant values tend to correlate pretty strongly.

## Using coordinates as features

Next, we'll explore the use of coordinates as features. Conceptually, we are trying to improve the model outcome by incorporating some measure of spatial dependency or autocorrelation between locations. This helps to address models that tend to over- or under-estimate systematically in space. To illustrate this point, we'll first make a random forest model using all the data:

```{r}
rf <- rand_forest(mode = "regression")

rf_fit <- rf |> 
  fit(Cancer ~ PM25 + NO2 + SO2, cancer)
```

Now predict and merge into the `sf` object:

```{r}
atl_cancer$y_hat = predict(rf_fit, cancer)$.pred
```

Calculate the error as prediction - observation and plot:

```{r}
atl_cancer$error = atl_cancer$y_hat - atl_cancer$Cancer
tm_shape(atl_cancer) +
  tm_fill("error") +
  tm_scale_bar() + tm_compass()
```

And these results show that systematic error in the model (note the region of negative values in the west and south, and positive values in the north-west). Before including the coordinates in a model, let's run a 5-fold cross-validation on this model to get a reference RMSE and $R^2$. I've listed all the steps in the next block of code for clarity:

-   Set up the recipe (no data processing)
-   Instantiate the model
-   Define the cross-validation strategy
-   Combine this into a workflow
-   Run the cross-validation and print results

```{r}
rec <- recipe(Cancer ~ PM25 + NO2 + SO2, cancer)

rf <- rand_forest(mode = "regression")

nfolds = 5
folds <- vfold_cv(cancer, v = nfolds)

workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(rf)

results <- workflow |>
  fit_resamples(resamples = folds, 
                metrics = metric_set(rmse, rsq))

collect_metrics(results)
```

Giving us an RMSE of around `r round(collect_metrics(results)$mean[1], 2)`, and an $R^2$ of `r round(collect_metrics(results)$mean[2], 2)`.

Now let's re-run this with coordinates included (these are in the `x` and `y` columns or we could use the centroid coordinates we extracted earlier):

```{r}
rec <- recipe(Cancer ~ PM25 + NO2 + SO2 + x + y, cancer)

rf <- rand_forest(mode = "regression")

nfolds = 5
folds <- vfold_cv(cancer, v = nfolds)

workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(rf)

results <- workflow |>
  fit_resamples(resamples = folds, 
                metrics = metric_set(rmse, rsq))

collect_metrics(results)
```

And we get a pretty substantial improvement in model performance, with around a 2 point drop in RMSE and a doubling of $R^2$.

## Spatial basis functions

As a final step, we'll re-fit this model using spatial basis functions to represent location. As a reminder, these functions are designed to encode relative location (i.e. the proximity of each observation to the others) in a way that maximizes the signal-to-noise ratio in a memory efficient manner. These are best suited for large spatial and spatio-temporal datasets; we'll use them here more as a demonstration of how to create and use these.

We'll use these here to try and further improve our cancer model. R has an add-on library (**FRK**) that has a function to easily create these for spatial data. It currently only works with an oldr version of R's spatial objects, so first, let's convert our existing `sf` object to that format:

```{r}
atl_cancer_sp <- as_Spatial(atl_cancer)
```

Next, load the **FRK** library and use the `auto_basis` function to generate the basis functions for the cancer dataset:

```{r}
library(FRK)
atl_basis <- auto_basis(data = atl_cancer_sp)
```

And we can quickly visualize the results:

```{r}
show_basis(atl_basis) +
  coord_fixed() +
  geom_sf(data = atl_cancer, fill = NA, color = "red")
```

By default, this places three levels of basis functions from low-resolution to capture large scale spatial patterns (large circles) to high-resolution to capture fine-scale patterns (smallest circles). The red polygons show the distribution of the counties from the cancer dataset.

If you type the name of the basis object (`atl_basis`) it will give you an overview. This shows a total of 1428 functions that we could relate our observations too. We'll make some adjustments to this:

-   We'll only use two resolutions (`nres=2`)
-   We'll prune the basis functions. This removes all basis functions that have less than `prune` observations

Note that you can also adjust the resolution of the functions. For example, adding the argument `regular=2` will double the resolution. Alternatively, setting `regular=0` will add irregular basis functions based on the data density.

```{r}
atl_basis <- auto_basis(data = atl_cancer_sp, 
                        nres = 2, prune = 2)

show_basis(atl_basis) +
  coord_fixed() +
  geom_sf(data = atl_cancer, fill = NA, color = "red")
```

Now, we need to extract the value of each basis function for each observation. The closer an observation is, then the higher the value will be, and this is then used to spatially correlate observations (i.e. observations who have similar values on all basis functions must be close together).

We do this in two steps. First, we use the function `eval_basis` to get the values for each observation as a matrix. We add column names starting with `b` to this to identify each function

```{r}
basis_mat <- as.matrix(eval_basis(atl_basis, atl_cancer_sp))
colnames(basis_mat) <- paste0("b", 1:ncol(basis_mat))
dim(basis_mat)
```

The resulting matrix has the same nunber of rows as observations and same number of columns as basis functions. Next, we use R's `cbind` function to merge this with the original cancer dataset to make a new dataframe (`atl2`. Then we drop all unused predictors.

```{r}
atl2 <- cbind(cancer, basis_mat)

atl2 <- atl2 |>
  select(-FIPS, -x, -y, Smoking, Poverty)
```

Finally, we repeat out cross-validation exercise from above to estimate the predictive power of the model. Note that we use a much larger number of trees in the random forest, and a high number for the features used at each split, given the size of the new dataset

```{r}
rec <- recipe(Cancer ~ ., atl2)

rf <- rand_forest(mode = "regression", trees = 2000, mtry = 20)

nfolds = 5
folds <- vfold_cv(atl2, v = nfolds)

workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(rf)

results <- workflow |>
  fit_resamples(resamples = folds, 
                metrics = metric_set(rmse, rsq))

collect_metrics(results)
```

And we get another small improvement in the model. Note that we have not tuned any of these models (or the basis functions), and it's is possible that better performance could be achieved in practice.

# Exercise

In the cancer dataset, there are two possible predictor variables that we did not use above (poverty rates `Poverty` and smoking rates `Smoking`). For the exercise, you will need run a new analysis that includes these. There are three options given below; students in GEOG 5160 will need to do one of these, students in GEOG 6160 will need to do two.

1.  Use the cancer dataset to carry out a non-spatial and spatial cross-validation, following the example above. This will be a regression task, and you should include all the possible predictors (poverty, smoking and air pollutants). You can use a random forest, or any of the other algorithms we have looked at so far. Report the RMSE and $R^2$ for both cross-validation methods, and write a short statement that explains in simple terms what these mean (1-2 sentences)
2.  Make a new geographical random forest model that includes the variables mentioned above. Your answer should include a) a variable importance plot based on the global model; b) maps of the importance scores for all variables and the local $R^2$; c) a short description (2-4 sentences) of the spatial patterns you observe
3.  Make a new model with spatial basis functions that includes the variables mentioned above. You should try at least two different basis functions setups (I'd recommend that at least one uses the irregular basis functions described in the section above). Your answer should include a) a figure showing the distribution of the basis functions you used; b) cross-validated RMSE and $R^2$ values; c) a short statement as to whether the basis functions have improved the model (1-2 sentences)

Use a Quarto document to record your answers and output. Assignments, to include both the Quarto document and (ideally) the compiled HTML file, should be submitted to Canvas by Feb 19th. Please use the following naming convention: `Lab04_lastname`.

# Appendix 1: Datafiles

## *lsl.csv*

-   `x`: easting (m)
-   `y`: northing (m)
-   `lslpts`: presence or absence of landslide (`[0,1]`)
-   `slope`: slope angle (degrees)
-   `cplan`: plan curvature (rad m−1) expressing the convergence or divergence of a slope and thus water flow
-   `cprof`: profile curvature (rad m-1) as a measure of flow acceleration, also known as downslope change in slope angle
-   `elev`: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area
-   `log10_carea`: the decadic logarithm of the catchment area (log10 m2) representing the amount of water flowing toward a location

## *ta.tif*

Raster dataset with same predictor variables as *lsl.csv*

## *data_atlantic_1998_2012.csv*

- `FIPS`: County FIPS code
- `x`: Easting (m)
- `y`: Northing (m)
- `Cancer`: Cancer mortality rate / 100,000
- `Poverty`: Poverty rate (% below poverty level)
- `Smoking`: Smoking rate (%)
- `PM25`: Annual mean PM2.5 concentration
- `NO2`: Annual mean NO2 concentration
- `SO2`: Annual mean SO2 concentration

