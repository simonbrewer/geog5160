{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOG 5160 6160 Lab 07\n",
    "\n",
    "## Data processing\n",
    "\n",
    "Let's start by by importing the modules we'll need for the class, then we'll go get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "## Set random seed for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinus edulis data\n",
    "\n",
    "Now, let's read in the known locations of *Pinus edulis* trees from the file *Pinus_edulis.csv*, and a set of pseudo-absences from the file *absence.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence = pd.read_csv(\"../datafiles/Pinus_edulis.csv\")\n",
    "presence.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "absence = pd.read_csv(\"../datafiles/absence.csv\")\n",
    "absence.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to append these two datasets to have both presence and absence in the same DataFrame. \n",
    "\n",
    "- Append the coordinates\n",
    "- Create a binary Series where 0 = absence and 1 = presence\n",
    "- Convert to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = presence.longitude.append(absence.longitude).values\n",
    "lat = presence.latitude.append(absence.latitude).values\n",
    "\n",
    "pa = pd.Series([1, 0]).repeat(299).values\n",
    "\n",
    "frame = { 'longitude': lon, \n",
    "         'latitude': lat,\n",
    "         'pa': pa\n",
    "        } \n",
    "pe = pd.DataFrame(frame)\n",
    "pe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environmental data\n",
    "\n",
    "There are a large number of available data sources for environmental data that can be used in species distribution models. We'll take data from the [Worldclim project][wcID] (Hijmans et al. 2005), a collection of standardized climate data at a variety of spatial resolutions. The data contains monthly averages of temperature and precipitation and a set of bioclimatic variables, which represent aggregate climate variables assumed to be linked to species distributions. I've already downloaded the bioclimatic variables for you and clipped them to the region you're going to work in. These are available in two NetCDF files containing modern (*current_env.nc*) and future (*future_env.nc*) climates for the study area. \n",
    "\n",
    "NetCDF is a standard format for large, multidimensional gridded dataset, and is commonly used to store climate data. Python has several libraries that will read these files. We'll use `xarray` here, as it comes with some simple functions to extract values and plot the data that work well with these data. The output of the `info` method gives you an overview of the dataset, including the dimensions, coordinates and the variables. Each z-level in this file stores one of the 19 bioclimate variables (the description of each of these is given in the appendix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "curr_env = xr.open_dataset('../datafiles/current_env.nc')\n",
    "curr_env.info\n",
    "#curr_env['variable'].sel(lon=-107, lat=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xarray` introduces two new data objects to Python, a DataArray (which contain the variables we are interested in) and a DataSet, which is a collection of DataArrays. The `current_env` DataSet contains two DataArrays (`crs` and `BIO`). The second of these contains the bioclimate variables that we want to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_env.BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot any individual layer using the `plot()` method. Note the indexing used to identify the layer to be shown (`BIO[0]` = the first layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_env.BIO[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the region we are using for our study area includes part of the Pacific Ocean, we'll create a land/sea mask. This will be used later to mask out any predictions over the ocean.  This is done quite simple by creating an array with 2's over the ocean (the NaN values in the data) and a second with 1's over the land. Then these are combined into the final mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ocean = 2 * np.ones((curr_env.dims['latitude'], \n",
    "                          curr_env.dims['longitude'])) * np.isnan(curr_env.BIO.isel(z=0))  \n",
    "mask_land = 1 * np.ones((curr_env.dims['latitude'], \n",
    "                         curr_env.dims['longitude'])) * np.isfinite(curr_env.BIO.isel(z=0))  \n",
    "mask_array = mask_ocean + mask_land\n",
    "mask_array.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the same data for the future (2080 under a high emissions climate scenario), so let's load that as well. Once we have built our model, we can then predict the distribution of Pinus edulis under this changed climate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_env = xr.open_dataset('../datafiles/future_env.nc')\n",
    "future_env.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the presence and absence points, we need to extract the environmental values for these points (these will be a features for machine learning). To do this we use the following steps:\n",
    "\n",
    "- Create sets of coordinates for the presence/absence data in `xarray` format\n",
    "- Use the `sel()` method to extract the associated climate values from the environmental grids for these coordinates\n",
    "- Convert the extracted data to a Pandas DataFrame\n",
    "- Concatenate the presence/absence data frame with the environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = xr.DataArray(pe.longitude, dims='x')\n",
    "lats = xr.DataArray(pe.latitude, dims='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = curr_env.BIO.sel(latitude = lats, longitude = lons, method = 'nearest')\n",
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"bio\"+str(i+1) for i in range(19)]\n",
    "x = pd.DataFrame(data.values.transpose(),\n",
    "                columns = var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pd.concat([pe, x], axis=1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we'll convert this new DataFrame to a GeoPandas dataframe, so we can make some quick maps of the presence/absence values and one of the associated environmental variables (`bio7`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "pe_gpd = gpd.GeoDataFrame(pe, \n",
    "                          geometry=gpd.points_from_xy(pe.longitude, pe.latitude), \n",
    "                               crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_gpd.plot(column=\"pa\", figsize = (6.5, 6.5), categorical=True, \n",
    "            markersize = 100, legend=True, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_gpd.plot(column=\"bio7\", figsize = (6.5, 6.5), \n",
    "            markersize = 100, legend=True, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and regression trees\n",
    "\n",
    "Classification and Regression Trees (CART) is a non-linear, non-parametric modeling approach that can be used with a wide variety of data. Regression trees are used with continuous outcome data, and classification trees with binary or categorical data, but the interface for these is the same in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a classification model for the *Pinus edulis* data set. First, let's set up the training and testing set using all 19 of the bioclimatic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"bio\"+str(i+1) for i in range(19)]\n",
    "\n",
    "X = pe[var_names]\n",
    "y = pe['pa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No prizes for guessing that the scikit-learn function for a classification decision tree is called `DecisionTreeClassifier()`. Let's initialize one of these, and train it on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "pe_tree = tree.DecisionTreeClassifier()\n",
    "pe_tree = pe_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the resulting tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(40,20)) \n",
    "tree.plot_tree(pe_tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the default settings results in a very overfit tree. To illustrate this, let's calculate the AUC on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_test_pred = pe_tree.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "\n",
    "Let's try to improve on this model by tuning it to find the best set of hyperparameters to limit overfitting. You can get the list of available hyperparameters for any scikit-learn model using `get_params()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several here that we could try to tune, but we'll focus on two that are important in limiting tree size:\n",
    "\n",
    "- `max_depth`: the maximum number of splits along any branch of the tree\n",
    "- `min_samples_leaf`: the minimum number of samples in a child node for it to be considered in the splitting procedure\n",
    "\n",
    "We'll use a scikit-learn function to help with the tuning called `GridSearchCV()`. As the name might imply, this will carry out a cross-validated search among a set of hyperparameter values to find the best value. Practically, this takes the dataset, splits it into training and testing, builds several models with different parameter values and predicts for the test set. The best values are then saved and returned. \n",
    "\n",
    "After importing the function, we need to define the parameter space that will be searched. This is set up as a Python dictionary, with the name of the hyperparameter as the key, and the value or set of values to be tested. Any parameters that are not specified will be held at their default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tree_param = {'max_depth':[4,5,6,7,8,9,10],\n",
    "             'min_samples_leaf':[5,6,7,8,9,10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we set up the cross-validation strategy using `GridSearchCV()`. We need to specify:\n",
    "\n",
    "- The algorithm to be tuned\n",
    "- The parameter space\n",
    "- The performance metric to be used to select the parameter values\n",
    "- The cross-validation strategy. Here we are using a simple 5-fold cross-validation, but it is possible to replace this with more complex strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_tree_tuned = GridSearchCV(tree.DecisionTreeClassifier(), tree_param, \n",
    "                             scoring='roc_auc', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the standard scikit-learn `fit()` method to run the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_tree_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full set of results are held in `pe_tree_tuned.cv_results_`, but we'll just show here the range of AUC scores found during tuning. These vary a little (from just under 0.8 to about 0.83). Tuning has a relatively small impact on decision trees, so this small range is not too surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_tree_tuned.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the `best_estimator`, i.e. the best tree that was built to see the parameters that were used, and the score it obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_tree_tuned.best_estimator_)\n",
    "print(pe_tree_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best max_depth:', pe_tree_tuned.best_estimator_.get_params()['max_depth'])\n",
    "print('Best min_samples_leaf:', pe_tree_tuned.best_estimator_.get_params()['min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's re-predict for our test set and see if we've improved on the original default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pe_tree_tuned.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build a random forest for the Pinus data. scikit-learn's random forest functions are found in the `ensemble` sub-module. Let's import this and then set up, train and test a random forest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_rf = ensemble.RandomForestClassifier()\n",
    "pe_rf.fit(X_train, y_train)\n",
    "y_test_pred = pe_rf.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with its default settings, this model shows a decent improvement over the decision tree. We'll now try to tune it. First, look at the available parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are several hyperparameters that we could tune. We'll focus again on two: `n_estimators` (the number of trees in the forest) and `max_features` (the number of randomly selected features used for each split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param = {'n_estimators':[100, 200, 300, 400, 500],\n",
    "             'max_features':[2,4,6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run our tuning grid search using these parameters and a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_rf_tuned = GridSearchCV(ensemble.RandomForestClassifier(), rf_param, \n",
    "                             scoring='roc_auc', cv=5)\n",
    "pe_rf_tuned.fit(X_train, y_train)\n",
    "print(pe_rf_tuned.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at the best fitting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_rf_tuned.best_estimator_)\n",
    "print(pe_rf_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if tuning the model has improved over the default (I got a very slight improvement, but your mileage may vary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pe_rf_tuned.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance\n",
    "\n",
    "Next we'll plot the permutation-based variable importance for this model. As a reminder, variable importance is a measure of how much worse a model becomes when we scramble the values of one of the features. The model is used to predict the outcome for some test data (here the out-of-bag samples) twice: once with the original values of the feature and once with randomly shuffled values. If there is a large difference in the skill of the model, this feature is important in controlling the outcome. \n",
    "\n",
    "As the result of the grid-based tuning includes the best estimator, we'll use this to get the variable importance values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_rf_tuned.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a just an array, it is a little difficult to parse out any differences. Instead, we'll sort them and print them together with the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pe_rf_tuned.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], var_names[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also plot the values as a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the form of the relationship between the occurrence of the pine and this feature (and any other one) using a partial dependency plot. This shows changes in the outcome across the range of some feature (with all other features held constant). Here, we'll use the `PartialDependenceDisplay()` function from the the `inspection` submodule to produce the plot. As arguments, this requires the model, the DataFrame or array used to build the model, and the feature that you want to show. This will take an array of feature indices, allowing you to plot several dependency plots together. \n",
    "\n",
    "If you are using an earlier version of scikit-learn (pre 1.2), then the function is called `plot_partial_dependence()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "PartialDependenceDisplay.from_estimator(pe_rf_tuned.best_estimator_, X, [16,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features here are `bio17` (precipitation of driest quarter) and `bio1` (mean annual temperature. The first of these shows a clear threshold effect, with an abrupt drop in suitability as this drops below about 30. The second show more of an optimum between values of about 6 and 10 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a boosted regression tree model for the Pinus data. In contrast to random forests that build a set of individual weak trees, boosted regression trees (BRTs) start with a single weak tree and iteratively improve on this. This is done by targeting the residuals from the previous set of models and trying to model that in the next tree. While this can make these methods very powerful, it is easy for them to overfit the data, and hyperparameter tuning becomes very important here. \n",
    "\n",
    "While scikit-learn has its own implementation of boosted regression, an alternative is through the xgboost library. This has a number of advantages with memory management and parallelization which can greatly speed up fitting these models, and we'll use this. It has a very similar interface to scikit-learn, so we can just reuse the data and tuning approaches from previous models. As before, we'll start by simply running it with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pe_xgb = XGBClassifier()\n",
    "pe_xgb.fit(X_train, y_train)\n",
    "y_test_pred = pe_xgb.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to tune it. First, get a list of available parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's a lot. A thorough search strategy would probably include some of the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our search space for grid search\n",
    "xgb_param = {\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.1, 0.2, 0.3],\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'gamma': [0, 0.1, 0.2],\n",
    "            'min_child_weight': [0, 0.5, 1],\n",
    "            'max_delta_step': [0],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1],\n",
    "            'colsample_bytree': [0.6, 0.8, 1],\n",
    "            'colsample_bylevel': [1],\n",
    "            'reg_alpha': [0, 1e-2, 1, 1e1],\n",
    "            'reg_lambda': [0, 1e-2, 1, 1e1],\n",
    "            'base_score': [0.5]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of keeping this relatively fast, we'll just test the following parameters, each with a fairly coarse grid. In practice, you'd want to do this more exhaustively. \n",
    "\n",
    "- `max_depth`: the number of splits in each tree\n",
    "- `learning_rate`: the contribution of each tree to the overall model\n",
    "- `n_estimators`: the total number of trees built\n",
    "\n",
    "We also set the parameter `subsample` to 0.5 to only use a random selection of observations in building each tree. As this is a constant, it won't be varied during the grid search. Once this is setup, we'll run the usual cross-validated grid search. You might see a few warnings appear, you can safely ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'subsample': [0.5], \n",
    "    'eval_metric': ['logloss']\n",
    "}\n",
    "\n",
    "pe_xgb_tuned = GridSearchCV(XGBClassifier(), xgb_param, \n",
    "                             scoring='roc_auc', cv=5, verbose = 0)\n",
    "pe_xgb_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_xgb_tuned.best_estimator_)\n",
    "print(pe_xgb_tuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model to predict for our test samples. This does give a small but notable increase over the un-tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pe_xgb_tuned.predict_proba(X_test)\n",
    "metrics.roc_auc_score(y_test, y_test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again extract the variable importance scores from the tuned model, which again shows `bio17` as being the most importance feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pe_xgb_tuned.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], var_names[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Now we tested and tuned our models, we can use them for prediction. For species distribution models, we generally want to predict the suitability for our species using a gridded dataset of the environmental variables used to build the model. We loaded this earlier in the lab as `curr_env`. As a reminder, this is an xarray `DataSet`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_env.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we can't use this directly for predictions. We'll need to convert it to a Pandas DataFrame. This requires a couple of steps. First, we use the `stack()` method to collapse the multiple levels into a 2D array, where the rows represent the variables and the columns represent individual grid locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_stack = curr_env.BIO.stack(dim = ['latitude', 'longitude'])\n",
    "print(bio_stack.values)\n",
    "print(bio_stack.shape)\n",
    "print(bio_stack.dim.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a DataFrame with the environmental variables. This needs to be transposed to have the same format as the DataFrame used in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_env_df = pd.DataFrame(bio_stack.values.transpose(),\n",
    "                columns = var_names)\n",
    "curr_env_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this to predict suitability. The only problem here is that there are number of rows that have missing values (NaNs) representing ocean grid cells. If we try to use these to predict, the method will fail, so we need a way to ignore them. We could simply drop them from the DataFrame, but this makes it difficult to map out the predictions, as these no longer line up with the coordinates. Instead, we'll set all missing values to 0 with the `fillna()` method. This means that the model will predict for these grid cells, but we can use the mask we created when visualizing the results. \n",
    "\n",
    "We'll go ahead and predict suitability using the tuned random forest model (feel free to swap this out for the decision tree of xgboost model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe_rf_tuned.predict_proba(curr_env_df.fillna(0))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we can convert it back to an xarray using the values (`x`) and the grid coordinates (which are extracted from the original `curr_env` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_pred = xr.DataArray(x[:,1].reshape(480,720), \n",
    "                         coords=[curr_env.coords['latitude'], \n",
    "                                 curr_env.coords['longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_pred.where(mask_array == 1).plot()\n",
    "plt.title(\"Pinus edulis\\npredicted current distribution\")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that model captures the current distribution well, but also predicts a large area of suitability in the north-west. We can also plot the predicted presence/absence as a binary outcome. To do this, we first need to get the optimal threshold for discriminating between absence (0) and presence (1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pe_rf_tuned.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred[:,1])\n",
    "threshold_rf = thresholds[np.argmax(tpr - fpr)]\n",
    "print(threshold_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_pa = curr_pred > threshold_rf\n",
    "\n",
    "curr_pa.where(mask_array == 1).plot()\n",
    "plt.title(\"Pinus edulis\\npredicted current distribution\")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future climate\n",
    "\n",
    "The previous maps are based on current (or at least end of last century) estimates of climate. We can equally predict for other time periods when we have data available. Earlier in the lab we read in climate data for the end of the century under a high emissions scenario (RCP8.5). We can go through the same steps to produce a map showing projected suitability for this species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_stack = future_env.BIO.stack(dim = ['latitude', 'longitude'])\n",
    "future_env_df = pd.DataFrame(bio_stack.values.transpose(),\n",
    "                columns = var_names)\n",
    "future_crds = pd.DataFrame(bio_stack.dim.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe_rf_tuned.predict_proba(future_env_df.fillna(0))\n",
    "future_pred = xr.DataArray(x[:,1].reshape(480,720), \n",
    "                         coords=[future_env.coords['latitude'], \n",
    "                                 future_env.coords['longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pred.where(mask_array == 1).plot()\n",
    "plt.title(\"Pinus edulis\\npredicted future distribution\")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we can use the threshold to identify areas of suitability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pa = future_pred > threshold_rf\n",
    "\n",
    "future_pa.where(mask_array == 1).plot()\n",
    "plt.title(\"Pinus edulis\\npredicted future distribution\")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final result, we can use the two binary maps to more easily visualize how the distribution of suitable areas is projected to change. By subtracting the current binary distribution from the future binary distribution, we end up with a map with three values:\n",
    "\n",
    "- 1: new areas of suitability\n",
    "- 0: No change\n",
    "- -1: loss of suitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_pa = future_pa.astype('int') - curr_pa.astype('int')\n",
    "change_pa.where(mask_array == 1).plot()\n",
    "plt.title(\"Pinus edulis\\npredicted change in distribution\")\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In a previous lab, you used data from the *Sonar.csv* file to model types of object (rocks 'R' or mines 'M') using the values of a set of frequency bands. The exercise for this lab is to use one of the ensemble methods (random forest *or* boosted regression trees) to produce a new model of these data. You should use the **scikit-learn** framework to set up and test your model, and you need to do the following:\n",
    "\n",
    "- Run the model with the default hyperparameter settings, and report the AUC for the test set [1]\n",
    "- Use `GridSearchCV()` to tune the model [2]\n",
    "- Report the AUC score for the tuned model, as well as the tuned parameters [1]\n",
    "- Produce a variable importance plot [1]\n",
    "\n",
    "In addition, your answer should include your code, either as a word document or jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Bioclimate variables\n",
    "\n",
    "- BIO1 = Annual Mean Temperature\n",
    "- BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp))\n",
    "- BIO3 = Isothermality (BIO2/BIO7) (* 100)\n",
    "- BIO4 = Temperature Seasonality (standard deviation *100)\n",
    "- BIO5 = Max Temperature of Warmest Month\n",
    "- BIO6 = Min Temperature of Coldest Month\n",
    "- BIO7 = Temperature Annual Range (BIO5-BIO6)\n",
    "- BIO8 = Mean Temperature of Wettest Quarter\n",
    "- BIO9 = Mean Temperature of Driest Quarter\n",
    "- BIO10 = Mean Temperature of Warmest Quarter\n",
    "- BIO11 = Mean Temperature of Coldest Quarter\n",
    "- BIO12 = Annual Precipitation\n",
    "- BIO13 = Precipitation of Wettest Month\n",
    "- BIO14 = Precipitation of Driest Month\n",
    "- BIO15 = Precipitation Seasonality (Coefficient of Variation)\n",
    "- BIO16 = Precipitation of Wettest Quarter\n",
    "- BIO17 = Precipitation of Driest Quarter\n",
    "- BIO18 = Precipitation of Warmest Quarter\n",
    "- BIO19 = Precipitation of Coldest Quarter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
